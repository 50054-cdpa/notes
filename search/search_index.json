{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to 50.054 Compiler Design and Program Analysis","text":"<ul> <li>Course Handout.</li> <li>Course Schedule.</li> </ul>"},{"location":"advanced_static_analysis/","title":"50.054 - Advanced Topics in Static Analysis","text":""},{"location":"advanced_static_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Apply Path Sensitive Analysis to Sign Analysis.</li> <li>Apply Static Analysis to detect software security loopholes.</li> </ol>"},{"location":"advanced_static_analysis/#recall-that-sign-analysis","title":"Recall that sign analysis","text":"<p>The Sign Analysis that we developed in the previous class has some limitation.</p> <p><pre><code>// PA1               // s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\n1: x &lt;- input        // s1 = s0[ x -&gt; s0(input) ]\n2: t &lt;- x &gt;= 0       // s2 = lub(s1,s5) [ t -&gt; lub(s1,s5)(x) &gt;== 0 ]\n3: ifn t goto 6      // s3 = s2   \n4: x &lt;- x - 1        // s4 = s3[ x -&gt; s3(x) -- + ]\n5: goto 2            // s5 = s4\n6: y &lt;- Math.sqrt(x) // s6 = s3 \n7: r_ret &lt;- y\n8: ret\n</code></pre> The monotonic equations in the comments are defined based on the following lattice.</p> <p><pre><code>graph\n    A[\"\u22a4\"]---B[-]\n    A---C[0]\n    A---D[+]\n    B---E\n    C---E\n    D---E[\u22a5]</code></pre> And the abstract value operators are defined as </p> -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) &gt;== \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) 0 \\(\\top\\) 0 \\(\\bot\\) 0 \\(\\top\\) 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>By converting the equation system into monotonic function </p> <pre><code>f1((s0, s1, s2, s3, s4, s5, s6)) = ( \n    [ x -&gt; top, t -&gt; top, input -&gt; top ], \n    s0[ x -&gt; s0(input) ], \n    lub(s1,s5) [ t -&gt; lub(s1,s5)(x) &gt;== 0 ],\n    s2, \n    s3[ x -&gt; s3(x) -- + ],\n    s4,\n    s3\n)\n</code></pre> <p>when we apply the fixed point algorithm to the <code>f1</code> and the VarSign lattice, we have the following solution</p> <pre><code>s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ], \ns1 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns2 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns3 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns4 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns5 = [ x -&gt; top, t -&gt; top, input -&gt; top ],\ns6 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\n</code></pre> <p>At label 6, <code>x</code>'s sign is \\(\\top\\). Such a problem exists in general as static analyses are approximation. Some point in the above analysis causes the result being overly approximated. </p> <ul> <li>Could it be due to the problem of how the abstract operators <code>--</code> and <code>&gt;==</code> are defined? <ul> <li>No, they are as best as we could infer given the variables (operands) are not assigned with concrete values. </li> </ul> </li> <li> <p>Could it be due to the lattice having too few elements (abstract values)? No, it remains as top, even if we introduce new elements such as <code>+0</code> and <code>-0</code> </p> <ul> <li>Let's say we adjust the lattice</li> </ul> <p><pre><code>graph\n    A[\"\u22a4\"]---A1[+0]\n    A[\"\u22a4\"]---A2[-0]\n    A2---B[-]\n    A2---C[0]\n    A1---C[0]\n    A1---D[+]\n    B---E\n    C---E\n    D---E[\u22a5]</code></pre> * and the abstract operators</p> -- \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) +0 \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + +0 \\(\\bot\\) -0 \\(\\top\\) -0 \\(\\top\\) \\(\\top\\) + -0 \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) -0 +0 - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) &gt;== \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) +0 +0 +0 +0 +0 +0 \\(\\bot\\) +0 +0 +0 + +0 + +0 \\(\\bot\\) -0 +0 +0 +0 0 +0 +0 \\(\\bot\\) + +0 +0 + +0 + + \\(\\bot\\) - +0 0 +0 0 +0 0 \\(\\bot\\) 0 +0 +0 + 0 + + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <ul> <li>It does not help, as it might give <code>t</code> a more precise abtract value but it does not help to improve the result of <code>x</code></li> </ul> </li> </ul> <p>The real cause of the loss of precision is path insensitivity of the sign analysis, i.e. it does not exploit the fact that the path of going in the while loop body is only valid under the pre-condition <code>x&gt;=0</code> and the path of going out of the while loop is only valid under the condition <code>x &lt; 0</code>. </p>"},{"location":"advanced_static_analysis/#path-sensitive-analysis-via-assertion","title":"Path sensitive analysis via assertion","text":"<p>Supposed in the source language level, i.e SIMP, we include the assertion statement. For example, we consider the source program of <code>PA1</code> in SIMP with assertion statements inserted in the body of the while loop and at the following statement of the while loop.</p> <pre><code>// SIMP2\nx = input;\nwhile x &gt;= 0 {\n    assert x &gt;= 0;\n    x = x - 1;\n}\nassert x &lt; 0;\ny = Math.sqrt(x);\nreturn y;\n</code></pre> <p>As we translate the above SIMP program in to Pseudo Assembly, we retain the assertions as instructions</p> <pre><code>// PA2               // s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\n1: x &lt;- input        // s1 = s0[ x -&gt; s0(input) ]\n2: t &lt;- x &gt;= 0       // s2 = lub(s1,s6) [ t -&gt; lub(s1,s6)(x) &gt;== 0 ]\n3: ifn t goto 7      // s3 = s2   \n4: assert x &gt;= 0     // s4 = s3[ x -&gt; gte(s3(x), 0) ]\n5: x &lt;- x - 1        // s5 = s4[ x -&gt; s4(x) -- + ]\n6: goto 2            // s6 = s5\n7: assert x &lt; 0      // s7 = s3[ x -&gt; lt(s3(x), 0) ] \n8: y &lt;- Math.sqrt(x) // s8 = s7 \n9: r_ret &lt;- y\n10: ret\n</code></pre> <p>We could add the following monotonic function synthesis case </p> <ul> <li>case \\(l: assert\\ t\\ &gt;=\\ src\\), \\(s_l = join(s_l)[ t \\mapsto gte(join(s_l)(t), join(s_l)(src))]\\)</li> <li>case \\(l: assert\\ t\\ &lt;\\ src\\), \\(s_l = join(s_l)[ t \\mapsto lt(join(s_l)(t), join(s_l)(src))]\\)</li> </ul> <p>Where \\(gte\\) and \\(lt\\) are defined specifically for assertion instructions. The idea is to exploit the comparison operators to \"narrow\" down the range of the abstract signs.</p> gte \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) +0 \\(\\top\\) + \\(\\top\\) +0 \\(\\bot\\) +0 +0 +0 +0 + +0 +0 \\(\\bot\\) -0 -0 0 -0 \\(\\bot\\) 0 -0 \\(\\bot\\) + + + + + + + \\(\\bot\\) - - \\(\\bot\\) - \\(\\bot\\) - \\(\\bot\\) \\(\\bot\\) 0 0 0 0 \\(\\bot\\) 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) lt \\(\\top\\) +0 -0 + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) - \\(\\top\\) - - \\(\\bot\\) +0 +0 +0 \\(\\bot\\) +0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) -0 -0 0 -0 -0 - - \\(\\bot\\) + + + \\(\\bot\\) + \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) - - - - - - - \\(\\bot\\) 0 0 0 \\(\\bot\\) 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>To show that the above definitions of <code>gte</code> and <code>lt</code> are sound. We can consider the range notation of the abstract values. </p> \\[ \\begin{array}{rcl} \\top &amp; = &amp; [-\\infty, +\\infty] \\\\  +0 &amp; = &amp; [0, +\\infty] \\\\  -0 &amp; = &amp; [-\\infty, 0] \\\\  \\ + &amp; = &amp; [1, +\\infty] \\\\  \\ - &amp; = &amp; [-\\infty, -1] \\\\  0 &amp; = &amp; [0, 0] \\\\  \\bot &amp; = &amp; [+\\infty, -\\infty]  \\end{array} \\] <p>\\([l, h]\\) denotes the set of integers \\(i\\) where \\(l \\leq i \\leq h\\). \\(\\bot\\) is an empty range. We can think of <code>gte</code> as</p> \\[ gte([l_1, h_1], [l_2, h_2]) = [ max(l_1,l_2), min(h_1, +\\infty)] \\] <p>Similiarly we can think of <code>lt</code> as </p> \\[ lt([l_1, h_1], [l_2, h_2]) = [ max(l_1,-\\infty), min(h_1, h_2-1)] \\] <p>where \\(+\\infty - 1 = +\\infty\\)</p> <p>With the adjusted monotonic equations, we can now define the monotonic function <code>f2</code> as follows</p> <pre><code>f2((s0, s1, s2, s3, s4, s5, s6, s7, s8)) = ( \n    [ x -&gt; top, t -&gt; top, input -&gt; top ]\n    s0[ x -&gt; s0(input) ], \n    lub(s1,s6) [ t -&gt; lub(s1,s6)(x) &gt;== 0 ],\n    s2, \n    s3[ x -&gt; gte(s3(x), 0) ],\n    s4[ x -&gt; s4(x) -- + ],\n    s5,\n    s3[ x -&gt; lt(s3(x), 0) ],\n    s7 \n)\n</code></pre> <p>By applying the fixed point algorithm to <code>f2</code> we find the following solution</p> <pre><code>s0 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\ns1 = [ x -&gt; top, t -&gt; top, input -&gt; top ]\ns2 = [ x -&gt; top, t -&gt; +0, input -&gt; top ]\ns3 = [ x -&gt; top, t -&gt; +0, input -&gt; top ]\ns4 = [ x -&gt; +0, t -&gt; +0, input -&gt; top ]\ns5 = [ x -&gt; top, t -&gt; +0, input -&gt; top ]\ns6 = [ x -&gt; -, t -&gt; +0, input -&gt; top]\ns7 = [ x -&gt; -, t -&gt; +0, input -&gt; top]\n</code></pre> <p>which detects that the sign of <code>x</code> at instruction 8 is <code>-</code>.</p>"},{"location":"advanced_static_analysis/#information-flow-analysis","title":"Information Flow Analysis","text":"<p>One widely applicable static analysis is information flow analysis.   </p> <p>The information flow in a program describes how data are evaluated and propogated in the program via variables and operations.</p> <p>The goal of information flow analysis is to identify \"incorrect information flow\". There two main kinds.</p> <ol> <li>Low security level data is being written into high security level resources, AKA tainted flow, e.g. SQL injection.</li> <li>High security level data is being sent to low security level observer. i.e, sensitive resource being read by unauthorized users, AKA, information disclosure.</li> </ol>"},{"location":"advanced_static_analysis/#tainted-flow","title":"Tainted Flow","text":"<p>IN this case, we say that the information flow is tainted if some sensitive information is updated / written by unauthorized users, e.g. </p> <pre><code>String id = request.getParameter(\"id\"); // untrusted user input \nString query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = \" + id;\ntry {\n    Statement statement = dbconnection.createStatement();\n    ResultSet res = statement.executeQuery( query ); // access sensitive resource\n}\n</code></pre> <p>The above example was adapted from some online example showing what SQL injection vulnerable python code looks like. In this case we could argue that it is a tainted control flow as the untrusted user data is being used directly to access the sensitive resources. </p> <p>When the <code>id</code> is <code>\"' OR 'a'='a'; delete from customer_data; --\"</code>, the malicious user gains the login access and deletes all records from the <code>customer_data</code> table.</p> <p>This can be prevented by using a prepared statement. </p> <pre><code>String id = request.getParameter(\"id\"); // untrusted user input \nString query = \"SELECT account_number, account_balance FROM customer_data WHERE account_owner_id = ?\";\ntry {\n    PreparedStatement pstmt = connection.prepareStatement( query );\n    pstmt.setString(1, id); // pstmt is sanitized before being used.\n    ResultSet results = pstmt.executeQuery(); \n}\n</code></pre> <p>One may argue that using manual source code review should help to identify this kind of issues. The situation gets complicated when the program control is not simple</p> <p>Let's recast the above into SIMP, we would have the vulunerable code as </p> <p><pre><code>id = input();\nquery = \"select \" + id;\nexec(query);\nreturn id;\n</code></pre> We assume that we've extended SIMP to support string values and string concatenation.  The <code>input</code> is a function that prompts the user for input. The <code>exec</code> function is a database builtin function.</p> <p>The following version fixed the vulnerability, assume the <code>sanitize</code> function, sanitizes the input.</p> <pre><code>id = input();\nquery = \"select \";\nquery = sanitize(query, id)\nexec(query);\nreturn id;\n</code></pre> <p>To increase the level of compexlity, let's add some control flow to the example.</p> <pre><code>id = input();\nquery = \"select \" + id;\nwhile (id == \"\") {\n    id = input();\n    query = sanitize(\"select \", id)\n}\nexec(query);\nreturn id;\n</code></pre> <p>In the above, it is not directly clear that the <code>exec()</code> is given a sanitized query. The manual check becomes exponentially hard as the code size grows. </p> <p>We can solve it using a forward may analysis. We define the abstract domain as the following complete lattice. </p> <pre><code>graph\n    tainted --- clean --- bot(\"\u22a5\")</code></pre> <p>We rewrite the above SIMP program into the following PA equivalent.</p> <pre><code>1: id &lt;- input()\n2: query &lt;- \"select \" + id\n3: b &lt;- id == \"\"\n4: ifn b goto 8\n5: id &lt;- input()\n6: query &lt;- sanitize(\"select\", id) \n7: goto 3\n8: _ &lt;- exec(query)\n9: r_ret &lt;- id \n10: ret\n</code></pre> <p>We define the equation generation rules as follows, </p> \\[ join(s_i) = \\bigsqcup pred(s_i) \\] <p>where \\(pred(s_i)\\) returns the set of predecessors of \\(s_i\\) according to the control flow graph.</p> <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l == 0\\), \\(s_0 = \\lbrack x \\mapsto \\bot \\mid x \\in V\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src\\), \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ \\sqcup \\ join(s_l)(src_2))\\rbrack\\)</li> <li>case \\(l: t \\leftarrow input()\\): \\(s_l = join(s_l) \\lbrack t \\mapsto tainted\\rbrack\\)</li> <li>case \\(l: t \\leftarrow sanitize(src_1, src_2)\\): \\(s_l = join(s_l) \\lbrack t \\mapsto clean\\rbrack\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows </p> \\[ \\begin{array}{rcl} m(c) &amp; = &amp; clean \\\\ m(t) &amp; = &amp; \\left \\{         \\begin{array}{cc}         v &amp; t \\mapsto v \\in m \\\\          error &amp; otherwise         \\end{array}             \\right . \\\\ \\\\  m(r) &amp; = &amp; error  \\end{array} \\] <p>We inline the equations as comments in the PA code.</p> <pre><code>                                    // s0 = [id -&gt; bot, query -&gt; bot, b -&gt; bot]\n1: id &lt;- input()                    // s1 = s0[ id -&gt; tainted ]  \n2: query &lt;- \"select \" + id          // s2 = s1[ query -&gt; lub(clean, s1(id)) ]\n3: b &lt;- id == \"\"                    // s3 = lub(s2,s7)[ b -&gt; lub( lub(s2,s7)(id), clean) ]\n4: ifn b goto 8                     // s4 = s3\n5: id &lt;- input()                    // s5 = s4[ id -&gt; tainted ]\n6: query &lt;- sanitize(\"select\", id)  // s6 = s5[ query -&gt; clean ]\n7: goto 3                           // s7 = s6\n8: exec(query)                      // s8 = s4                  \n9: r_ret &lt;- id                      // s9 = s8\n10: ret                             \n</code></pre> <p>By applying the above we have the followning monotonic function.</p> <pre><code>f((s0,s1,s2,s3,s4,s5,s6,s7,s8,s9)) = (\n    [id -&gt; bot, query -&gt; bot, b -&gt; bot],\n    s0[ id -&gt; tainted], \n    s1[ query -&gt; lub(clean, s1(id)) ],\n    lub(s2,s7)[ b -&gt; lub( lub(s2,s7)(id), clean) ],\n    s3,\n    s4[ id -&gt; tainted ],\n    s5[ query -&gt; clean ],\n    s6,\n    s4,\n    s8    \n)\n</code></pre> <p>By applying the fixed point algorithm, we find the following solution for the monotonic equations.</p> <p><pre><code>s0 = [id -&gt; bot, query -&gt; bot, b -&gt; bot]\ns1 = [id -&gt; tainted, query -&gt; bot, b -&gt; bot] \ns2 = [id -&gt; tainted, query -&gt; tainted, b -&gt; bot]\ns3 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]\ns4 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ] \ns5 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]\ns6 = [id -&gt; tainted, query -&gt; clean, b -&gt; tainted ]   \ns7 = [id -&gt; tainted, query -&gt; clean, b -&gt; tainted ]   \ns8 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]\ns9 = [id -&gt; tainted, query -&gt; tainted, b -&gt; tainted ]   \n</code></pre> which says that the use of variable <code>query</code> at instruction 8 is risky as <code>query</code> may be tainted at this point.</p>"},{"location":"advanced_static_analysis/#sensitive-information-disclosure","title":"Sensitive Information Disclosure","text":"<p>In this case of incorrect information flow, sensitive information may be written / observable by low access level agents, users or system, directly or indirectly. </p> <pre><code>String username = request.getParameter(\"username\"); \nString password = request.getParameter(\"password\"); // sensitive user input \nString cmd = \"INSERT INTO user VALUES (?, ?)\";\ntry {\n    PreparedStatement pstmt = connection.prepareStatement( cmd );\n    pstmt.setString(1, username); \n    pstmt.setString(2, password);  // user password is saved without hashing?\n    ResultSet results = pstmt.execute(); \n}\n</code></pre> <p>In the above code snippet, we retrieve the username and password from a HTTP form request object and create a user record in the database table. The issue with this piece of code is that the user password is inserted into the database without hashing. This violates the security policy of which the user password is confidential where the database table <code>user</code> is having a lower security level, since it is accessed by the database users. </p> <pre><code>String username = request.getParameter(\"username\"); \nString password = request.getParameter(\"password\"); // sensitive user input \nString cmd = \"INSERT INTO user VALUES (?, ?)\";\ntry {\n    PreparedStatement pstmt = connection.prepareStatement( cmd );\n    String hashed_password = hash(password); // the hash serves as a declassification operation.\n    pstmt.setString(1, username); \n    pstmt.setString(2, hashed_password);  // user password is saved without hashing?\n    ResultSet results = pstmt.execute(); \n}\n</code></pre> <p>In the above modified version, we store the hashed password instead, which is safe since it is hard to recover the raw password based on the hashed password.  The <code>hash</code> function here serves as a declassifier that takes a high security level input and returns a lower secirity level output. </p> <p>We can analyses this kind of information flow that will accept the second snippet and reject the first one. The idea is nearly identical to the tainted analysis, except,</p> <ul> <li>We will have a different lattice for security level.  <pre><code>graph\n    secret --- confidential --- bot(\"\u22a5\")</code></pre></li> </ul> <p>where <code>secret</code> has a higher level of security than <code>confidential</code>. </p> <ul> <li>Instead of using <code>sanitize</code> to increase the level of security, we use <code>hash</code> (or <code>declassify</code>) to lower the level of security.</li> </ul>"},{"location":"code_generation/","title":"50.054 - Code Generation","text":""},{"location":"code_generation/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Name the difference among the target code platforms</li> <li>Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly</li> <li>Handle register spilling</li> <li>Implement the target code generation to JVM bytecode given a Pseudo Assembly Program</li> </ol>"},{"location":"code_generation/#recap-compiler-pipeline","title":"Recap Compiler Pipeline","text":"<pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]\nD --&gt; C</code></pre> <p>For Target Code Generation, we consider some IR as input, the target code (executable) as the output.</p>"},{"location":"code_generation/#instruction-selection","title":"Instruction Selection","text":"<p>Instruction selection is a process of choosing the target platform on which the language to be executed. </p> <p>There are mainly 3 kinds of target platforms.</p> <ul> <li>3-address instruction<ul> <li>RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly</li> </ul> </li> <li>2-address instruction<ul> <li>CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86</li> </ul> </li> <li>1-address instruction<ul> <li>Stack machine. E.g. JVM</li> </ul> </li> </ul>"},{"location":"code_generation/#assembly-code-vs-machine-code","title":"Assembly code vs Machine code","text":"<p>Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format.</p>"},{"location":"code_generation/#3-address-instruction","title":"3-address instruction","text":"<p>In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction.</p> <p>For instance in 3 address instruction, we have instructions that look like </p> <pre><code>x &lt;- 1\ny &lt;- 2\nr &lt;- x + y\n</code></pre> <p>where <code>r</code>, <code>x</code> and <code>y</code> are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, </p> <pre><code>load x 1\nload y 2\nadd r x y\n</code></pre> <p>The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.)</p>"},{"location":"code_generation/#2-address-instruction","title":"2-address instruction","text":"<p>In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add <code>x</code> and <code>y</code> and store the result in <code>r</code>, we have to write</p> <pre><code>load x 1\nload y 2\nadd x y\n</code></pre> <p>in the 3<sup>rd</sup> instruction we add the values stored in registers <code>x</code> and <code>y</code>. The sum will be stored in <code>x</code>. In the last statement, we move the result from <code>x</code> to <code>r</code>.</p> <p>As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex.</p>"},{"location":"code_generation/#1-address-instruction","title":"1-address instruction","text":"<p>In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. </p> <p>For example for the same program, we need t9o encode it in 1-address instruction as follows</p> <p><pre><code>push 1\npush 2\nadd \nstore r\n</code></pre> In the first instruction, we push the constant 1 to the left operand register (or the 1<sup>st</sup> register). In the second instruction, we push the constant 2 to the right oeprand register (the 2<sup>nd</sup> register). In the 3<sup>rd</sup> instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2<sup>nd</sup> register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable <code>r</code></p> <p>The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex.</p>"},{"location":"code_generation/#from-pa-to-3-address-target-platform","title":"From PA to 3-address target platform","text":"<p>In this section, we consider generating code for a target platform that using 3-address instruciton.</p>"},{"location":"code_generation/#register-allocation-problem","title":"Register Allocation Problem","text":"<p>Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register <code>rret</code>.</p> <p>Such an assumption is no longer valid in the code generation phase. We face two major constraints.</p> <ol> <li>Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation.</li> <li>The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation.</li> </ol> <p>For example, the following PA program </p> <p><pre><code>// PA1\n1: x &lt;- inpput\n2: y &lt;- x + 1\n3: z &lt;- y + 1\n4: w &lt;- y * z\n5: rret &lt;- w\n6: ret\n</code></pre> has to be translated into</p> <pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: r3 &lt;- r1 * r2\n5: rret &lt;- r3\n6: ret\n</code></pre> <p>assuming we have 4 other registers <code>r0</code>, <code>r1</code>, <code>r2</code> and <code>r3</code>, besides <code>rret</code>. We can map the PA variables <code>{x : r0, y : r1, z : r2, w : r3}</code></p> <p>When we only have 3 other registers excluding <code>rret</code> we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling.</p> <pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: x  &lt;- r0\n5: r0 &lt;- r1 * r2\n6: rret &lt;- r0\n7: ret\n</code></pre> <p>The above program will work within the hardware constraint (3 extra registers besides <code>rret</code>). Now the register allocation, <code>{x : r0, y : r1, z : r2}</code> is only valid for instructions <code>1-4</code> and the alloction for instructions <code>5-7</code> is <code>{w : r0, y : r1, z: r2}</code>.</p> <p>As we can argue, we could avoid the offloading by mapping <code>w</code> to <code>rret</code> since it is the one being retured. </p> <p><pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: rret &lt;- r1 * r2\n5: ret\n</code></pre> However this option is not always possible, as the following the <code>w</code> might not be returned variable in some other examples.</p> <p>We could also avoid the offloading by exploiting the liveness analysis, that <code>x</code> is not live from instruction <code>3</code> onwards, hence we should not even save the result of <code>r0</code> to the temporary variable <code>x</code>.</p> <p><pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: r0 &lt;- r1 * r2\n5: rret &lt;- r0\n6: ret\n</code></pre> However this option is not always possible, as in some other situation <code>x</code> is needed later.</p> <p>The Register Allocation Problem is then define as follows.</p> <p>Given a program \\(p\\), and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized.</p>"},{"location":"code_generation/#interference-graph","title":"Interference Graph","text":"<p>To solve the register allocation problem, we define a data structure called the interference graph. </p> <p>Two temporary variables are interferring each other when they are both \"live\" at the same time in a program.  In the following we include the liveness analysis result as the comments in the program <code>PA1</code>.</p> <pre><code>// PA1\n1: x &lt;- inpput // {input}\n2: y &lt;- x + 1  // {x}\n3: z &lt;- y + 1  // {y}\n4: w &lt;- y * z  // {y,z}\n5: rret &lt;- w   // {w}\n6: ret         // {}\n</code></pre> <p>We conclude that <code>y</code> and <code>z</code> are interfering each other. Hence they should not be sharing the same register. </p> <pre><code>graph TD;\n    input\n    x\n    y --- z\n    w </code></pre> <p>From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the <code>rret</code> register. </p> <p>For example we annotate the graph with the mapped registers <code>r0</code> and <code>r1</code> </p> <pre><code>graph TD;\n    input[\"input(r0)\"]\n    x[\"x(r0)\"]\n    y[\"y(r0)\"] --- z[\"z(r1)\"]\n    w[\"w(r0)\"]</code></pre> <p>And we can generate the following output </p> <pre><code>1: r0 &lt;- inpput   \n2: r0 &lt;- r0 + 1  \n3: r1 &lt;- r0 + 1  \n4: r0 &lt;- r0 * r1  \n5: rret &lt;- r0   \n6: ret         \n</code></pre>"},{"location":"code_generation/#graph-coloring-problem","title":"Graph Coloring Problem","text":"<p>From the above example, we find that we can recast the register allocation problem into a graph coloring problem. </p> <p>The graph coloring problem is defined as follows.</p> <p>Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. </p> <p>Unfortunately, this problem is NP-complete in general. No efficient algorithm is known.</p> <p>Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists. </p>"},{"location":"code_generation/#chordal-graph","title":"Chordal Graph","text":"<p>A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n &gt; 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle.</p> <p>For example, the following graph</p> <pre><code>graph TD\n    v1 --- v2 --- v3 --- v4 --- v1\n    v2 --- v4</code></pre> <p>is chordal, because of \\((v_2,v_4)\\).</p> <p>The following graph </p> <pre><code>graph TD\n    v1 --- v2 --- v3 --- v4 --- v1</code></pre> <p>is not chordal, or chordless.</p> <p>It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time.</p>"},{"location":"code_generation/#an-example","title":"An Example","text":"<p>Consider the following PA program with the variable liveness result as comments</p> <pre><code>// PA2\n1: a &lt;- 0           // {}\n2: b &lt;- 1           // {a}\n3: c &lt;- a + b       // {a, b}\n4: d &lt;- b + c       // {b, c}\n5: a &lt;- c + d       // {c, d}\n6: e &lt;- 2           // {a}\n7: d &lt;- a + e       // {a, e}\n8: r_ret &lt;- e + d   // {e, d}\n9: ret \n</code></pre> <p>We observe the interference graph </p> <p><pre><code>graph TD\n    a --- b --- c --- d \n    a --- e --- d</code></pre> and find that it is chordless.</p>"},{"location":"code_generation/#ssa-saves-the-day","title":"SSA saves the day!","text":"<p>With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph.</p> <p>For example, if we apply SSA conversion to <code>PA2</code></p> <p>We have the following</p> <pre><code>// PA_SSA2\n1: a1 &lt;- 0           // {}\n2: b1 &lt;- 1           // {a1}\n3: c1 &lt;- a1 + b1     // {a1, b1}\n4: d1 &lt;- b1 + c1     // {b1, c1}\n5: a2 &lt;- c1 + d1     // {c1, d1}\n6: e1 &lt;- 2           // {a2}\n7: d2 &lt;- a2 + e1     // {a2, e1}\n8: r_ret &lt;- e1 + d2  // {e1, d2}\n9: ret \n</code></pre> <p>The liveness analysis algorithm can be adapted to SSA with the following adjustment.</p> <p>We define the \\(join(s_i)\\) function as follows</p> \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j)  \\] <p>where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\). </p> \\[ \\begin{array}{rcl} \\Theta_{i,j} &amp; = &amp; \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l: \\overline{\\phi}\\ ret\\), \\(s_l = \\{\\}\\)</li> <li>case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\), \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\)</li> <li>case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\), \\(s_l = join(s_l) \\cup var(src)\\)</li> <li>case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\), \\(s_l = join(s_l) \\cup \\{ t \\}\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Now the interference graph of the <code>PA_SSA2</code> is as follows</p> <p><pre><code>graph TD;\n    a1 --- b1 --- c1 --- d1\n    a2 --- e1 --- d2</code></pre> which is chordal.</p>"},{"location":"code_generation/#coloring-interference-graph-generated-from-ssa","title":"Coloring Interference Graph generated from SSA","text":"<p>According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. </p> <p>In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order.</p> <p>In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.)</p> <p>Therefore we can color the above graph as follows,</p> <pre><code>graph TD;\n    a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\")\n    a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\")</code></pre> <p>From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form.</p> <p>Given that the program interference graph is chordal, the register allocation can be computed in polymomial type.</p> <p>Instead of using building the interference graph, we consider using the live range table of an SSA program, </p> <p>In the following table (of <code>PA_SSA2</code>), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An <code>*</code> in a cell <code>(x, l)</code> represent variable <code>x</code> is live at program location <code>l</code>.</p> var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 <p>At any point, (any column), the number of <code>*</code> denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is <code>2</code> (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling.</p>"},{"location":"code_generation/#register-spilling","title":"Register Spilling","text":"<p>However register spilling is unavoidable due to program complexity and limit of hardware. </p> <p>Let's consider another example </p> <pre><code>// PA3\n1: x &lt;- 1       // {}\n2: y &lt;- x + 1   // {x}\n3: z &lt;- x * x   // {x,y}\n4: w &lt;- y * x   // {x,y,z}\n5: u &lt;- z + w   // {z,w}\n6: r_ret &lt;- u   // {u}  \n7: ret          // {}\n</code></pre> <p>The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis.</p> var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * <p>From the live range table able, we find that at peak i.e. instruction <code>4</code>, there are 3 live variables currently. We would need three registers for the allocation.</p> <p>What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction <code>4</code>, by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here.</p> <ol> <li>Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point.</li> <li>Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. </li> </ol> <p>For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live.</p> <pre><code>// PA3\n1: x &lt;- 1       // {}\n2: y &lt;- x + 1   // {x(3)}\n3: z &lt;- x * x   // {x(3),y(4)}\n4: w &lt;- y * x   // {x(4),y(4),z(5)}\n5: u &lt;- z + w   // {z(5),w(5)}\n6: r_ret &lt;- u   // {u(6)}  \n7: ret          // {}\n</code></pre> <p>From the above results, we can conclude that at instruction <code>4</code>, we should sacrifice the live variable <code>z</code>, because <code>z</code> is marked live at label <code>5</code> which is needed in the instruction one-hop away in the CFG, compared to <code>x</code> and <code>y</code> which are marked live at label <code>4</code>. In other words, <code>z</code> is not as urgently needed compared to <code>x</code> and <code>y</code>. </p> var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * <p>From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label <code>3</code>, variable is <code>z</code> is some register, either <code>r0</code> or <code>r1</code>, assuming in the target code operation <code>*</code> can use the same register for both operands and the result. We encounter another problem. To spill <code>z</code> (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. </p> <p><pre><code>// PA3_REG\n1: r0 &lt;- 1        // x is r0\n2: r1 &lt;- r0 + 1   // y is r1\n3: ?? &lt;- r0 * r0  // what register should hold the result of x * x, before spilling it to `z`?\n</code></pre> where the comments indicate what happens after the label instruction is excuted.</p> <p>There are two option here</p> <ol> <li><code>??</code> is <code>r1</code>. It implies that we need to spill <code>r1</code> to <code>y</code> first after instruction <code>2</code> and then spill <code>r1</code> to <code>z</code> after instruction <code>3</code>, and load <code>y</code> back to <code>r1</code> after instruction <code>3</code> before instruction <code>4.</code></li> <li><code>??</code> is <code>r0</code>. It implies that we need to spill <code>r0</code> to <code>x</code> first after instruction <code>2</code> and then spill <code>r0</code> to <code>z</code> after instruction <code>3</code>, and load <code>x</code> back to <code>r0</code> after instruction <code>3</code> before instruction <code>4.</code></li> </ol> <p>In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling (<code>z</code> in this example) is not needed until then. </p> <p>Now let's say we pick the first option, the register allocation continues </p> var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 <p>where <code>-</code> indicates taht <code>z</code> is being spilled from <code>r1</code> before label <code>4</code> and it needs to be loaded back to <code>r1</code> before label <code>5</code>.  And the complete code of <code>PA3_REG</code> is as follows</p> <p><pre><code>// PA3_REG\n1: r0 &lt;- 1        // x is r0\n2: r1 &lt;- r0 + 1   // y is r1\n   y  &lt;- r1       // temporarily save y\n3: r1 &lt;- r0 * r0  // z is r1 \n   z  &lt;- r1       // spill to z\n   r1 &lt;- y        // y is r1\n4: r0 &lt;- r1 * r0  // w is r0 (x,y are dead afterwards)\n   r1 &lt;- z        // z is r1\n5: r1 &lt;- r1 + r0  // u is r1 (z,w are dead afterwards)\n6: r_ret &lt;- r1\n7: ret\n</code></pre> In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case).</p> <p>As an exercise, work out what if we save <code>x</code> temporarily instead of <code>y</code> at label <code>2</code>.</p>"},{"location":"code_generation/#register-allocation-for-phi-assignments","title":"Register allocation for phi assignments","text":"<p>What remains to address is the treatment of the phi assignments.</p> <p>Let's consider a slightly bigger example. </p> <p><pre><code>// PA4\n1: x &lt;- input   // {input}\n2: s &lt;- 0       // {x}\n3: c &lt;- 0       // {s,x}\n4: b &lt;- c &lt; x   // {c,s,x}\n5: ifn b goto 9 // {b,c,s,x}\n6: s &lt;- c + s   // {c,s,x}\n7: c &lt;- c + 1   // {c,s,x}\n8: goto 4       // {c,s,x}\n9: r_ret &lt;- s   // {s}\n10: ret         // {}\n</code></pre> In the above we find a sum program with liveness analysis results included as comments.</p> <p>Let's convert it into SSA.</p> <p><pre><code>// PA_SSA4\n1: x1 &lt;- input1  // {input1(1)}\n2: s1 &lt;- 0       // {x1(4)}\n3: c1 &lt;- 0       // {s1(4),x1(4)}\n4: c2 &lt;- phi(3:c1, 8:c3)\n   s2 &lt;- phi(3:s1, 8:s3)\n   b1 &lt;- c2 &lt; x1 // {c2(4),s2(6,9),x1(4)}\n5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)}\n6: s3 &lt;- c2 + s2 // {c2(6),s2(6),x1(4)}\n7: c3 &lt;- c2 + 1  // {c2(7),s3(4),x1(4)}\n8: goto 4        // {c3(4),s3(4),x1(4)}\n9: r_ret &lt;- s2   // {s2(9)}\n10: ret          // {}\n</code></pre> We put the liveness analysis results as comments. </p> <p>There are a few options of handling phi assignments.</p> <ol> <li>Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code</li> <li>Ensure the variables in the phi assignments sharing the same registers. </li> </ol> <p>Let's consider the first approach </p>"},{"location":"code_generation/#conservative-approach","title":"Conservative approach","text":"<p>When we translate the SSA back to PA</p> <pre><code>// PA_SSA_PA4\n1: x1 &lt;- input1  // {input1(1)}\n2: s1 &lt;- 0       // {x1(4)}\n3: c1 &lt;- 0       // {s1(3.1),x1(4)}\n3.1: c2 &lt;- c1     \n     s2 &lt;- s1    // {s1(3.1),x1(4),c1(3.1)}\n4: b1 &lt;- c2 &lt; x1 // {c2(4),s2(6,9),x1(4)}\n5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)}\n6: s3 &lt;- c2 + s2 // {c2(6),s2(6),x1(4)}\n7: c3 &lt;- c2 + 1  // {c2(7),s3(7.1),x1(4)}\n7.1: c2 &lt;- c3\n     s2 &lt;- s3    // {s3(7.1),x1(4),c3(7.1)}\n8: goto 4        // {c2(4),s2(6,9),x1(4)}\n9: r_ret &lt;- s2   // {s2(9)}\n10: ret          // {}\n</code></pre> <p>It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers.</p> var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 <p>At the peak of the live variables, i.e. instruction <code>5</code>, we realize that <code>x1</code> is live but not urgently needed until <code>4</code> which is 5-hop away from the current location. Hence we spill it from register <code>r1</code> to the temporary variable to free up <code>r1</code>.  Registers are allocated by the next available in round-robin manner.</p> <pre><code>// PA4_REG1\n1: r0 &lt;- input1  // input is r0\n   r1 &lt;- r0      // x1 is r1\n2: r2 &lt;- 0       // s1 is r2\n3: r0 &lt;- 0       // c1 is r0\n                 // c2 is r0 \n                 // s2 is r2\n                 // no need to load r1 from x1\n                 // b/c x1 is still active in r1\n                 // from 3 to 4\n4: x1 &lt;- r1      // spill r1 to x1\n   r1 &lt;- r0 &lt; r1 // b1 is r1\n5: ifn r1 goto 9 // \n6: r2 &lt;- r0 + r2 // s3 is r2\n7: r0 &lt;- r0 + 1  // c3 is r0\n                 // c2 is r0\n                 // s2 is r2\n8: r1 &lt;- x1      // restore r1 from x1\n   goto 4        // b/c x1 is inactive but needed in 4\n9: r_ret &lt;- r2   // \n10: ret          // \n</code></pre> <p>What if at instruction <code>7</code>, we allocate <code>r1</code> to <code>s3</code> instead of <code>r2</code>? Thanks to some indeterminism, we could have a slightly different register allocation as follows</p> var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 <pre><code>// PA4_REG2\n1: r0 &lt;- input1  // input is r0\n   r1 &lt;- r0      // x1 is r1\n2: r2 &lt;- 0       // s1 is r2\n3: r0 &lt;- 0       // c1 is r0\n                 // c2 is r0 \n                 // s2 is r2\n                 // no need to load r1 from x1\n                 // b/c x1 is still active in r1\n                 // from 3 to 4\n4: x1 &lt;- r1      // spill r1 to x1\n   r1 &lt;- r0 &lt; r1 // b1 is r1\n5: ifn r1 goto 9 // \n6: r1 &lt;- r0 + r2 // s3 is r1\n7: r2 &lt;- r0 + 1  // c3 is r2\n7.1: r0 &lt;- r2    // c2 is r0  \n     r2 &lt;- r1    // s2 is r2\n8: r1 &lt;- x1      // restore r1 from x1 \n   goto 4        // b/c x1 is inactive but needed in 4\n9: r_ret &lt;- s2   \n10: ret          \n</code></pre> <p>In this case we have to introduce some additional register shuffling at <code>7.1</code>. Compared to <code>PA4_REG1</code>, this result is less efficient.</p>"},{"location":"code_generation/#register-coalesced-approach-ensure-the-variables-in-the-phi-assignments-sharing-the-same-registers","title":"Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers","text":"<p>Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. </p> <p>What we could construct the live range table as follow.</p> var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 <p>Although from the above we find <code>c1</code> seems to be always dead, but it is not, because its value is merged into c2 in label <code>4</code>. This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level.</p> <p>We also take note we want to <code>c1</code> and <code>c3</code> to share the same register, and <code>s1</code> and <code>s3</code>to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach <code>PA4_REG1</code>.</p> <p>Note that this approach is not guanranteed to produce more efficient results than the conversvative approach. </p>"},{"location":"code_generation/#summary-so-far","title":"Summary so far","text":"<p>To sum up the code generation process from PA to 3-address target could be carried out as follows,</p> <ol> <li>Convert the PA program into a SSA.</li> <li>Perform Liveness Analysis on the SSA. </li> <li>Generate the live range table based on the liveness analysis results.</li> <li>Allocate registers based on the live range table. Detect potential spilling.</li> <li>Depending on the last approach, either<ol> <li>convert SSA back to PA and generate the target code according to the live range table, or </li> <li>generate the target code from SSA with register coalesced for the phi assignment operands.</li> </ol> </li> </ol>"},{"location":"code_generation/#further-reading-for-ssa-based-register-allocation","title":"Further Reading for SSA-based Register Allocation","text":"<ul> <li>https://compilers.cs.uni-saarland.de/papers/ssara.pdf</li> <li>https://dl.acm.org/doi/10.1145/512529.512534</li> </ul>"},{"location":"code_generation/#jvm-bytecode-reduced-set","title":"JVM bytecode (reduced set)","text":"<p>In this section, we consider the generated JVM codes from PA. </p> \\[ \\begin{array}{rccl} (\\tt JVM\\ Instructions) &amp; jis &amp; ::= &amp; [] \\mid ji\\ jis\\\\  (\\tt JVM\\ Instruction) &amp; ji &amp; ::= &amp; ilabel~l \\mid iload~n \\mid istore~n \\mid iadd \\mid isub \\mid imul \\\\  &amp; &amp; &amp; \\mid if\\_icmpge~l \\mid if\\_icmpne~l \\mid igoto~l \\mid  sipush~c \\mid ireturn\\\\ (\\tt JVM\\ local\\ vars) &amp; n &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\  (\\tt constant) &amp; c &amp; ::= &amp; -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767  \\end{array} \\] <p>As mentioned, JVM has 3 registers</p> <ol> <li>a register for the first operand and result</li> <li>a register for the second operand</li> <li>a register for controlling the state of the stack operation (we can't used.)</li> </ol> <p>Technically speaking we only have 2 registers.</p> <p>An Example of JVM byte codes is illustrated as follows</p> <p>Supposed we have a PA program as follows, <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: b &lt;- c &lt; x\n5: ifn b goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: _ret_r &lt;- s\n10: ret\n</code></pre> For ease of reasoning, we assume that we map PA temporary variables to numerical JVM variables, as <code>input</code> to <code>1</code>, <code>x</code> to <code>2</code>, <code>s</code> to <code>3</code>, <code>c</code> to <code>4</code> (and <code>b</code> to <code>5</code>, though <code>b</code> is not needed in the JVM instruction). We also map the PA label (the useful ones) to JVM label. <code>4</code> to <code>l1</code> and <code>9</code> to <code>l2</code>.</p> <pre><code>iload 1      // push the content of input to register 0\nistore 2     // pop register 0's content to x,  \nsipush 0     // push the value 0 to register 0\nistore 3     // pop register 0 to s\nsipush 0     // push the value 0 to register 0\nistore 4     // pop register 0 to c\nilabel l1    // mark label l1\niload 4      // push the content of c to register 0\niload 2      // push the content of x to register 1\nif_icmpge l2 // if register 0 &gt;= register 1 jump, \n             // regardless of the comparison pop both registers\niload 4      // push the content of c to register 0\niload 3      // push the content of s to register 1\niadd         // sum up the r0 and r1 and result remains in register 0\nistore 3     // pop register 0 to s\niload 4      // push the content of c to register 0\nsipush 1     // push a constant 1 to register 1\niadd        \nistore 4     // pop register 0 to c\nigoto l1\nilabel l2\niload 3      // push the content of s to register 0\nireturn\n</code></pre>"},{"location":"code_generation/#jvm-bytecode-operational-semantics","title":"JVM bytecode operational semantics","text":"<p>To describe the operational semantics of JVM bytecodes, we define the following meta symbols.</p> \\[ \\begin{array}{rccl} (\\tt JVM\\ Program) &amp; J &amp; \\subseteq &amp; jis \\\\ (\\tt JVM\\ Environment) &amp; \\Delta &amp; \\subseteq &amp; n \\times c \\\\  (\\tt JVM\\ Stack) &amp; S &amp; =  &amp; \\_,\\_ \\mid c,\\_ \\mid c,c   \\end{array} \\] <p>An JVM program is a sequence of JVM instructions. \\(\\Delta\\) is local environment that maps JVM variables to constants. \\(S\\) is a 2 slots stack where the left slot is the bottom (\\(r_0\\)) and the right slot is the top (\\(r_1\\)). \\(\\_\\) denotes that a slot is vacant.</p> <p>We can decribe the operational semantics of JVM byte codes using the follow rule form</p> \\[  J \\vdash (\\Delta, S, jis) \\longrightarrow (\\Delta', S', jis') \\] <p>\\(J\\) is the entire program, it is required when we process jumps and conditional jump. The rule rewrites a configuration \\((\\Delta, S, jis)\\) to the next configuration \\((\\Delta', S', jis')\\), where \\(\\Delta\\) and \\(\\Delta'\\) are the current and the next states of the local environment, \\(S\\) and \\(S'\\) are the current and the next states of the value stack, \\(jis\\) and \\(jis'\\) are the currrent and next sets of instructions to be processed. </p> \\[ \\begin{array}{rc} (\\tt sjLoad1) &amp; J \\vdash (\\Delta, \\_, \\_, iload\\ n;jis) \\longrightarrow (\\Delta, \\Delta(n), \\_, jis) \\\\ \\\\  (\\tt sjLoad2) &amp; J \\vdash (\\Delta, c, \\_, iload\\ n;jis) \\longrightarrow (\\Delta, c, \\Delta(n), jis) \\\\ \\\\  (\\tt sjPush1) &amp; J \\vdash (\\Delta, \\_, \\_, sipush\\ c;jis) \\longrightarrow (\\Delta, c, \\_, jis) \\\\ \\\\  (\\tt sjPush2) &amp; J \\vdash (\\Delta, c_0, \\_, sipush\\ c_1;jis) \\longrightarrow (\\Delta, c_0, c_1, jis) \\end{array} \\] <p>The rules \\((\\tt sjLoad1)\\) and  \\((\\tt sjLoad2)\\) handles the loading variable's content to the stack registers.  The rules \\((\\tt sjPush1)\\) and  \\((\\tt sjPush2)\\) handles the loading constant to the stack registers. </p> \\[ \\begin{array}{rc} (\\tt sjLabel) &amp; J \\vdash (\\Delta, r_0, r_1, ilabel\\ l;jis) \\longrightarrow (\\Delta, r_0, r_1, jis) \\\\ \\\\  \\end{array} \\] <p>The rule \\((\\tt sjLabel)\\) processes the \\(ilabel\\ l\\) instruction. It is being skipped, because it serves as a syntactical marking (refer to the \\(codeAfterLabel()\\) function below), has no impact to the semantic operation.</p> \\[ \\begin{array}{rc} (\\tt sjStore) &amp; J \\vdash (\\Delta, c, \\_, istore\\ n;jis) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, jis) \\\\ \\\\  \\end{array} \\] <p>The rule \\((\\tt sjStore)\\) processes the \\(istore\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\).</p> \\[ \\begin{array}{rc} (\\tt sjAdd) &amp; J \\vdash (\\Delta, c_0, c_1, iadd;jis) \\longrightarrow (\\Delta, c_0+c_1, \\_, jis) \\\\ \\\\  (\\tt sjSub) &amp; J \\vdash (\\Delta, c_0, c_1, isub;jis) \\longrightarrow (\\Delta, c_0-c_1, \\_, jis) \\\\ \\\\  (\\tt sjMul) &amp; J \\vdash (\\Delta, c_0, c_1, imul;jis) \\longrightarrow (\\Delta, c_0*c_1, \\_, jis)   \\end{array} \\] <p>The rules \\((\\tt sjAdd)\\), \\((\\tt sjSub)\\) and \\((\\tt sjMul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is held by \\(r_0\\) while \\(r_1\\) becomes empty.</p> \\[ \\begin{array}{rc} (\\tt sjGoto) &amp; J \\vdash (\\Delta, r_0, r_1, igoto\\ l';jis) \\longrightarrow (\\Delta, r_0, r_1, codeAfterLabel(J, l')) \\\\ \\\\  (\\tt sjCmpNE1) &amp; \\begin{array}{c}                  c_0 \\neq c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l')                 \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis')                  \\end{array} \\\\ \\\\ (\\tt sjCmpNE2) &amp; \\begin{array}{c}                  c_0 = c_1                  \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpne\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis)                  \\end{array} \\\\ \\\\  (\\tt sjCmpGE1) &amp; \\begin{array}{c}                  c_0 \\ge c_1 \\ \\ \\ \\ jis' = codeAfterLabel(J, l')                 \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta, \\_, \\_, jis')                  \\end{array} \\\\ \\\\ (\\tt sjCmpGE2) &amp; \\begin{array}{c}                  c_0 &lt; c_1                  \\\\ \\hline                 J \\vdash  (\\Delta, c_0, c_1, if\\_icmpge\\ l';jis) \\longrightarrow (\\Delta , \\_, \\_, jis)                  \\end{array} \\\\ \\\\  \\end{array} \\] <p>The last set of rules handle the jump and conditional jumps. The rule \\((\\tt sjGoto)\\) processes a goto instruction by replacing the instructions to be processed \\(jis\\) by \\(codeAfterLabel(J, l')\\). Recall that \\(J\\) is storing the entire sequence of JVM instructions, \\(codeAfterLabel(J, l')\\) extracts the suffix of \\(J\\) starting from the point where \\(ilabel\\ l'\\) is found. </p> \\[ \\begin{array}{rcl} codeAfterLabel(ireturn, l) &amp; = &amp; error \\\\  codeAfterLabel(ilabel\\ l;jis, l') &amp; = &amp;              \\left \\{ \\begin{array}{lc}                       jis &amp; l == l'  \\\\                       codeAfterLabel(jis, l') &amp; {\\tt otherwise}                      \\end{array}             \\right . \\\\  codeAfterLabel(ji; jis, l) &amp; = &amp; codeAfterLabel(jis, l) \\end{array} \\] <p>The rule \\((\\tt sjCmpNE1)\\) performs the jump when the values held by the stacks are not equal. The rule \\((\\tt sjCmpNE2)\\) moves onto the next instruction (skpping the jump) when the values held by the stacks are equal. The rule \\((\\tt sjCmpGE1)\\) performs the jump when the values in the stack \\(c_0 \\geq c_1\\). The rule \\((\\tt sjCmpGE2)\\) moves onto the next instruction (skpping the jump) when the \\(c_0 &lt; c_1\\).</p>"},{"location":"code_generation/#conversion-from-pa-to-jvm-bytecodes","title":"Conversion from PA to JVM bytecodes","text":"<p>A simple conversion from PA to JVM bytecodes can be described using the following deduction system.</p> <p>Let \\(M\\) be a mapping from PA temporary variables to JVM local variables. Let \\(L\\) be a mapping from PA labels (which are used as the targets in some jump instructions) to JVM labels.</p> <p>We have three types of rules.</p> <ul> <li>\\(M, L \\vdash lis \\Rightarrow jis\\), convert a sequence of PA labeled instructions to a sequence of JVM bytecode instructions.</li> <li>\\(M \\vdash s \\Rightarrow jis\\), convert a PA operand into a sequence of JVM bytecode instructions.</li> <li>\\(L \\vdash l \\Rightarrow jis\\), convert a PA label into a JVM bytecode instructions, usually it is either empty or singleton.</li> </ul>"},{"location":"code_generation/#converting-pa-labeled-instructions","title":"Converting PA labeled instructions","text":"\\[ \\begin{array}{rl}      {\\tt (jMove)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s \\Rightarrow jis_1 \\ \\ \\ M,L\\vdash lis \\Rightarrow jis_2 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s; lis \\Rightarrow jis_0 + jis_1 + [istore\\ M(t)] + jis_2                 \\end{array} \\\\   \\end{array} \\] <p>The rule \\({\\tt (jMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(L \\vdash l_1 \\Rightarrow jis_0\\) to generate the label, in case the label is used as the target in some jump instructions. The auxiliary rule \\(M \\vdash s \\Rightarrow jis_1\\) converts a PA operand into a loading instruction in JVM bytecodes. Details fo these auxiliary functions can be found in the next subsection.</p> \\[ \\begin{array}{rl}      {\\tt (jEq)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpne\\ L(l_3)] + jis_3                 \\end{array} \\\\  \\\\      {\\tt (jLThan)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 &lt; s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [if\\_icmpge\\ L(l_3)] + jis_3                 \\end{array} \\\\   \\end{array} \\] <p>The rules \\((\\tt jEq)\\) and \\((\\tt jLThan)\\) translate the conditional jump instruction from PA to JVM. In these cases, we have to look at the first two instructions in the sequence. This is because in PA the conditional jump is performed in 2 instructions; while in JVM, it is done in a single step with two different instructions.</p> \\[ \\begin{array}{rl}      {\\tt (jAdd)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s_1 + s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [iadd, istore\\ M(t)] + jis_3                 \\end{array} \\\\   \\end{array} \\] \\[ \\begin{array}{rl}      {\\tt (jSub)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s_1 - s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [isub, istore\\ M(t)] + jis_3                 \\end{array} \\\\   \\end{array} \\] \\[ \\begin{array}{rl}      {\\tt (jMul)} &amp; \\begin{array}{c}                     L \\vdash l \\Rightarrow jis_0 \\ \\ \\ M \\vdash s_1 \\Rightarrow jis_1 \\ \\ \\ M \\vdash s_2 \\Rightarrow jis_2 \\ \\ \\ M,L \\vdash lis \\Rightarrow jis_3 \\\\                     \\hline                     M, L \\vdash l:t \\leftarrow s_1 * s_2; lis \\Rightarrow jis_0 + jis_1 + jis_2 + [imul, istore\\ M(t)] + jis_3                 \\end{array} \\\\   \\end{array} \\] <p>The rules \\((\\tt jAdd)\\),  \\((\\tt jSub)\\) and  \\((\\tt jMul)\\) handle the binary operation instruction in PA to JVM.</p> \\[ \\begin{array}{rl}      {\\tt (jGoto)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\  M,L \\vdash lis \\Rightarrow jis_1 \\\\                     \\hline                     M, L \\vdash l_1:goto\\ l_2; lis \\Rightarrow jis_0 + [igoto\\ l_2] + jis_1                 \\end{array} \\\\   \\end{array} \\] \\[ \\begin{array}{rl}      {\\tt (jReturn)} &amp; \\begin{array}{c}                     L \\vdash l_1 \\Rightarrow jis_0\\ \\ \\ M \\vdash s \\Rightarrow jis_1\\ \\ \\ \\\\                     \\hline                     M, L \\vdash l_1:rret \\leftarrow s;  l_2: ret \\Rightarrow jis_0 + jis_1 + [ireturn]                  \\end{array} \\\\   \\end{array} \\] <p>The last two rules \\((\\tt jGoto)\\) and \\((\\tt jReturn)\\) are trivial.</p>"},{"location":"code_generation/#converting-pa-operands","title":"Converting PA Operands","text":"\\[ \\begin{array}{rl} {\\tt (jConst)} &amp; M \\vdash c \\Rightarrow [sipush\\ c] \\\\ \\\\  {\\tt (jVar)} &amp; M \\vdash t \\Rightarrow [iload\\ M(t)] \\\\ \\\\  \\end{array} \\]"},{"location":"code_generation/#converting-pa-labels","title":"Converting PA Labels","text":"\\[ \\begin{array}{rl} {\\tt (jLabel1)} &amp; \\begin{array}{c}                      l \\not \\in L                     \\\\ \\hline                     L \\vdash l \\Rightarrow []                     \\end{array} \\\\ \\\\  {\\tt (jLabel2)} &amp; \\begin{array}{c}                      l  \\in L                     \\\\ \\hline                     L \\vdash l \\Rightarrow [ilabel\\ L(l)]                     \\end{array}   \\end{array} \\]"},{"location":"code_generation/#optimizing-jvm-bytecode","title":"Optimizing JVM bytecode","text":"<p>Though it is limited, there is room to optimize the JVM bytecode. For example, </p> <p>From the following SIMP program </p> <pre><code>r = (1 + 2) * 3\n</code></pre> <p>we generate the following PA code via the Maximal Munch</p> <pre><code>1: t &lt;- 1 + 2\n2: r &lt;- t * 3  \n</code></pre> <p>In turn if we apply the above PA to JVM bytecode conversion</p> <p><pre><code>sipush 1\nsipush 2\niadd\nistore 2 // 2 is t\niload 2\nsipush 3\nimul\nistore 3 // 3 is r\n</code></pre> As observe, the <code>istore 2</code> followed by <code>iload 2</code> are rundandant, because <code>t</code> is not needed later (dead).</p> <pre><code>sipush 1\nsipush 2\niadd\nsipush 3\nimul\nistore 3 // 3 is r\n</code></pre> <p>This can either be done via </p> <ol> <li>Liveness analysis on PA level or </li> <li>Generate JVM byte code directly from SIMP.<ul> <li>This requires the expression of SIMP assignment to be left nested. </li> <li>The conversion is beyond the scope of this module.</li> </ul> </li> </ol>"},{"location":"code_generation/#further-reading-for-jvm-bytecode-generation","title":"Further Reading for JVM bytecode generation","text":"<ul> <li>https://ssw.jku.at/Research/Papers/Wimmer04Master/Wimmer04Master.pdf</li> </ul>"},{"location":"code_generation/#summary-for-jvm-bytecode-generation","title":"Summary for JVM bytecode generation","text":"<ul> <li>To generate JVM bytecode w/o optimization can be done via deduction system</li> <li>To optimize JVM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.</li> </ul>"},{"location":"code_generation_wasm/","title":"50.054 - Code Generation","text":""},{"location":"code_generation_wasm/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Name the difference among the target code platforms</li> <li>Apply SSA-based register allocation to generate 3-address code from Pseudo Assembly</li> <li>Handle register spilling</li> <li>Implement the target code generation to WASM bytecode given a Pseudo Assembly Program</li> </ol>"},{"location":"code_generation_wasm/#recap-compiler-pipeline","title":"Recap Compiler Pipeline","text":"<pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]\nD --&gt; C</code></pre> <p>For Target Code Generation, we consider some IR as input, the target code (executable) as the output.</p>"},{"location":"code_generation_wasm/#instruction-selection","title":"Instruction Selection","text":"<p>Instruction selection is a process of choosing the target platform on which the language to be executed. </p> <p>There are mainly 3 kinds of target platforms.</p> <ul> <li>3-address instruction<ul> <li>RISC (Reduced Instruction Set Computer) architecture. E.g. Apple PowerPC, ARM, Pseudo Assembly</li> </ul> </li> <li>2-address instruction<ul> <li>CISC (Complex Instruction Set Computer) architecture. E.g. Intel x86</li> </ul> </li> <li>1-address instruction<ul> <li>Stack machine. E.g. JVM</li> </ul> </li> </ul>"},{"location":"code_generation_wasm/#assembly-code-vs-machine-code","title":"Assembly code vs Machine code","text":"<p>Note that the instruction formats mentioned here are the human-readable representations of the target code. The actual target code (machine code) is in binary format.</p>"},{"location":"code_generation_wasm/#3-address-instruction","title":"3-address instruction","text":"<p>In 3-address instruction target platform, each instruction is set to use 3 addresses in maximum. For instance, the Pseudo Assembly we studied earlier is a kind of 3-address instruction without the hardware restriction.</p> <p>For instance in 3 address instruction, we have instructions that look like </p> <pre><code>x &lt;- 1\ny &lt;- 2\nr &lt;- x + y\n</code></pre> <p>where <code>r</code>, <code>x</code> and <code>y</code> are registers . Alternatively, in some other 3 address instruction format, we express the code fragement above in a prefix notation, </p> <pre><code>load x 1\nload y 2\nadd r x y\n</code></pre> <p>The advantage of having more register (addresses) per instruction allows us to huge room of code optimization while keeping a relative simple and small set of instructions (for instance, consider our Pseudo Assembly has a simple set.)</p>"},{"location":"code_generation_wasm/#2-address-instruction","title":"2-address instruction","text":"<p>In 2-address instruction target platform, each instruction has maximum 2 addresses. As a result, some of the single line instruction in 3-address instruction has to be encoded as multiple instructions in 2 address platform. For example, to add <code>x</code> and <code>y</code> and store the result in <code>r</code>, we have to write</p> <pre><code>load x 1\nload y 2\nadd x y\n</code></pre> <p>in the 3<sup>rd</sup> instruction we add the values stored in registers <code>x</code> and <code>y</code>. The sum will be stored in <code>x</code>. In the last statement, we move the result from <code>x</code> to <code>r</code>.</p> <p>As the result, we need fewer registers (in minimum) to carry out operations. On the other hands, the set of instructions in 2-address instruction are often more complex.</p>"},{"location":"code_generation_wasm/#1-address-instruction","title":"1-address instruction","text":"<p>In the exterem case, we find some target platform has only 1 address instruction. This kind of target is also known as the P-code (P for Pascal) or the stack machine code. </p> <p>For example for the same program, we need t9o encode it in 1-address instruction as follows</p> <p><pre><code>push 1\npush 2\nadd \nstore r\n</code></pre> In the first instruction, we push the constant 1 to the left operand register (or the 1<sup>st</sup> register). In the second instruction, we push the constant 2 to the right oeprand register (the 2<sup>nd</sup> register). In the 3<sup>rd</sup> instruction, we apply the add operation to sum up the two registers and the result is stored in the first register. The 2<sup>nd</sup> register is cleared (or popped). In the last instruction, we pop the result from the first register store it in a temporary variable <code>r</code></p> <p>The benefit of 1 address intruction is having a minimum and uniform requirement for the hardware. It requrest the least amount registers, for example, JVM has only 3 registers. On the other hand, its instruction set is the most complex.</p>"},{"location":"code_generation_wasm/#from-pa-to-3-address-target-platform","title":"From PA to 3-address target platform","text":"<p>In this section, we consider generating code for a target platform that using 3-address instruciton.</p>"},{"location":"code_generation_wasm/#register-allocation-problem","title":"Register Allocation Problem","text":"<p>Let's consider the register allocation problem. Recall that in Pseudo Assembly, we have unlimited temporary variables and registers. Among all the examples of PA we seen so far, we did not use any register except for the return register <code>rret</code>.</p> <p>Such an assumption is no longer valid in the code generation phase. We face two major constraints.</p> <ol> <li>Most of the operations can be only applied to registers, not to temporary variables. Operands from temporary variables need to be loaded to some registers before the application of the operation.</li> <li>The number of registers is finite and often limited. This implies that we can't possibly load all the temporary variables to registers. At some point, we need to unload the content of some register to the temporary variable to make room for the next operation.</li> </ol> <p>For example, the following PA program </p> <p><pre><code>// PA1\n1: x &lt;- inpput\n2: y &lt;- x + 1\n3: z &lt;- y + 1\n4: w &lt;- y * z\n5: rret &lt;- w\n6: ret\n</code></pre> has to be translated into</p> <pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: r3 &lt;- r1 * r2\n5: rret &lt;- r3\n6: ret\n</code></pre> <p>assuming we have 4 other registers <code>r0</code>, <code>r1</code>, <code>r2</code> and <code>r3</code>, besides <code>rret</code>. We can map the PA variables <code>{x : r0, y : r1, z : r2, w : r3}</code></p> <p>When we only have 3 other registers excluding <code>rret</code> we need to offload some result into some temporary variable. The offloading of the result from registers to temporary variables is also known as register spilling.</p> <pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: x  &lt;- r0\n5: r0 &lt;- r1 * r2\n6: rret &lt;- r0\n7: ret\n</code></pre> <p>The above program will work within the hardware constraint (3 extra registers besides <code>rret</code>). Now the register allocation, <code>{x : r0, y : r1, z : r2}</code> is only valid for instructions <code>1-4</code> and the alloction for instructions <code>5-7</code> is <code>{w : r0, y : r1, z: r2}</code>.</p> <p>As we can argue, we could avoid the offloading by mapping <code>w</code> to <code>rret</code> since it is the one being retured. </p> <p><pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: rret &lt;- r1 * r2\n5: ret\n</code></pre> However this option is not always possible, as the following the <code>w</code> might not be returned variable in some other examples.</p> <p>We could also avoid the offloading by exploiting the liveness analysis, that <code>x</code> is not live from instruction <code>3</code> onwards, hence we should not even save the result of <code>r0</code> to the temporary variable <code>x</code>.</p> <p><pre><code>1: r0 &lt;- input\n2: r1 &lt;- r0 + 1\n3: r2 &lt;- r1 + 1\n4: r0 &lt;- r1 * r2\n5: rret &lt;- r0\n6: ret\n</code></pre> However this option is not always possible, as in some other situation <code>x</code> is needed later.</p> <p>The Register Allocation Problem is then define as follows.</p> <p>Given a program \\(p\\), and \\(k\\) registers, find an optimal register assignment so that the register spilling is minimized.</p>"},{"location":"code_generation_wasm/#interference-graph","title":"Interference Graph","text":"<p>To solve the register allocation problem, we define a data structure called the interference graph. </p> <p>Two temporary variables are interferring each other when they are both \"live\" at the same time in a program.  In the following we include the liveness analysis result as the comments in the program <code>PA1</code>.</p> <pre><code>// PA1\n1: x &lt;- inpput // {input}\n2: y &lt;- x + 1  // {x}\n3: z &lt;- y + 1  // {y}\n4: w &lt;- y * z  // {y,z}\n5: rret &lt;- w   // {w}\n6: ret         // {}\n</code></pre> <p>We conclude that <code>y</code> and <code>z</code> are interfering each other. Hence they should not be sharing the same register. </p> <pre><code>graph TD;\n    input\n    x\n    y --- z\n    w </code></pre> <p>From the graph we can tell that \"at peak\" we need two registers concurrently, hence the above program can be translated to the target code using 2 registers excluding the <code>rret</code> register. </p> <p>For example we annotate the graph with the mapped registers <code>r0</code> and <code>r1</code> </p> <pre><code>graph TD;\n    input[\"input(r0)\"]\n    x[\"x(r0)\"]\n    y[\"y(r0)\"] --- z[\"z(r1)\"]\n    w[\"w(r0)\"]</code></pre> <p>And we can generate the following output </p> <pre><code>1: r0 &lt;- inpput   \n2: r0 &lt;- r0 + 1  \n3: r1 &lt;- r0 + 1  \n4: r0 &lt;- r0 * r1  \n5: rret &lt;- r0   \n6: ret         \n</code></pre>"},{"location":"code_generation_wasm/#graph-coloring-problem","title":"Graph Coloring Problem","text":"<p>From the above example, we find that we can recast the register allocation problem into a graph coloring problem. </p> <p>The graph coloring problem is defined as follows.</p> <p>Given a undirected graph, and \\(k\\) colors, find a coloring plan in which no adjacent vertices sharing the same color, if possible. </p> <p>Unfortunately, this problem is NP-complete in general. No efficient algorithm is known.</p> <p>Fortunatley, we do know a subset of graphs in which a polynomial time coloring algorithm exists. </p>"},{"location":"code_generation_wasm/#chordal-graph","title":"Chordal Graph","text":"<p>A graph \\(G = (V,E)\\) is chordal if, for all cycle \\(v_1,...,v_n\\) in \\(G\\) with \\(n &gt; 3\\) there exists an edge \\((v_i,v_j) \\in E\\) and \\(i, j \\in \\{1,...,n\\}\\) such that \\((v_i, v_j)\\) is not part of the cycle.</p> <p>For example, the following graph</p> <pre><code>graph TD\n    v1 --- v2 --- v3 --- v4 --- v1\n    v2 --- v4</code></pre> <p>is chordal, because of \\((v_2,v_4)\\).</p> <p>The following graph </p> <pre><code>graph TD\n    v1 --- v2 --- v3 --- v4 --- v1</code></pre> <p>is not chordal, or chordless.</p> <p>It is a known result that a the coloring problem of chordal graphs can be solved in polynomial time.</p>"},{"location":"code_generation_wasm/#an-example","title":"An Example","text":"<p>Consider the following PA program with the variable liveness result as comments</p> <pre><code>// PA2\n1: a &lt;- 0           // {}\n2: b &lt;- 1           // {a}\n3: c &lt;- a + b       // {a, b}\n4: d &lt;- b + c       // {b, c}\n5: a &lt;- c + d       // {c, d}\n6: e &lt;- 2           // {a}\n7: d &lt;- a + e       // {a, e}\n8: r_ret &lt;- e + d   // {e, d}\n9: ret \n</code></pre> <p>We observe the interference graph </p> <p><pre><code>graph TD\n    a --- b --- c --- d \n    a --- e --- d</code></pre> and find that it is chordless.</p>"},{"location":"code_generation_wasm/#ssa-saves-the-day","title":"SSA saves the day!","text":"<p>With some research breakthroughs in 2002-2006, it was proven that programs in SSA forms are always having chordal interference graph.</p> <p>For example, if we apply SSA conversion to <code>PA2</code></p> <p>We have the following</p> <pre><code>// PA_SSA2\n1: a1 &lt;- 0           // {}\n2: b1 &lt;- 1           // {a1}\n3: c1 &lt;- a1 + b1     // {a1, b1}\n4: d1 &lt;- b1 + c1     // {b1, c1}\n5: a2 &lt;- c1 + d1     // {c1, d1}\n6: e1 &lt;- 2           // {a2}\n7: d2 &lt;- a2 + e1     // {a2, e1}\n8: r_ret &lt;- e1 + d2  // {e1, d2}\n9: ret \n</code></pre> <p>The liveness analysis algorithm can be adapted to SSA with the following adjustment.</p> <p>We define the \\(join(s_i)\\) function as follows</p> \\[ join(s_i) = \\bigsqcup_{v_j \\in succ(v_i)} \\Theta_{i,j}(s_j)  \\] <p>where \\(\\Theta_{i,j}\\) is a variable substitution derived from phi assignment of the labeled instruction at \\(j : \\overline{\\phi}\\ instr\\). </p> \\[ \\begin{array}{rcl} \\Theta_{i,j} &amp; = &amp; \\{ (t_i/t_k) \\mid t_k = phi(..., i : t_i, ...) \\in \\overline{\\phi} \\} \\end{array} \\] <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l: \\overline{\\phi}\\ ret\\), \\(s_l = \\{\\}\\)</li> <li>case \\(l: \\overline{\\phi}\\ t \\leftarrow src\\), \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\)</li> <li>case \\(l: \\overline{\\phi}\\ t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: \\overline{\\phi}\\ r \\leftarrow src\\), \\(s_l = join(s_l) \\cup var(src)\\)</li> <li>case \\(l: \\overline{\\phi}\\ r \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: \\overline{\\phi}\\ ifn\\ t\\ goto\\ l'\\), \\(s_l = join(s_l) \\cup \\{ t \\}\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Now the interference graph of the <code>PA_SSA2</code> is as follows</p> <p><pre><code>graph TD;\n    a1 --- b1 --- c1 --- d1\n    a2 --- e1 --- d2</code></pre> which is chordal.</p>"},{"location":"code_generation_wasm/#coloring-interference-graph-generated-from-ssa","title":"Coloring Interference Graph generated from SSA","text":"<p>According to the findings of Budimlic's work and Hack's work, coloring the interference graph generated from an SSA program in in-order traversal of dominator tree gives us the optimal coloring. </p> <p>In Hack's paper, it was discussed that the elimination step should be done in the post-order traveral of the dominator tree. From graph coloring problem, we know that the order of coloring is the reverse of the vertex eliminiation order.</p> <p>In the context of PA, the in-order traversal of the dominator tree is always the same order of the instructions being labeled (assuming we generate the PA using the maximal munch algorithm introduced in the earlier lesson.)</p> <p>Therefore we can color the above graph as follows,</p> <pre><code>graph TD;\n    a1(\"a1(r0)\") --- b1(\"b1(r1)\") --- c1(\"c1(r0)\") --- d1(\"d1(r1)\")\n    a2(\"a2(r0)\") --- e1(\"e1(r1)\") --- d2(\"d2(r0)\")</code></pre> <p>From now onwards until the next section (JVM Bytecode generatoin), we assume that program to be register-allocated must be in SSA form.</p> <p>Given that the program interference graph is chordal, the register allocation can be computed in polymomial type.</p> <p>Instead of using building the interference graph, we consider using the live range table of an SSA program, </p> <p>In the following table (of <code>PA_SSA2</code>), the first row contains the program labels and the first column defines the variables and the last column is the allocated register. An <code>*</code> in a cell <code>(x, l)</code> represent variable <code>x</code> is live at program location <code>l</code>.</p> var 1 2 3 4 5 6 7 8 9 reg a1 * * r0 b1 * * r1 c1 * * r0 d1 * r1 a2 * * r0 e1 * * r1 d2 * r0 <p>At any point, (any column), the number of <code>*</code> denotes the number of live variables concurrently. The above tables show that at any point in-time, the peak of the register usage is <code>2</code> (in some literature, it is also known as the chromatic of the interference graph). Therefore, minimumally we need 2 registers to allocate the above program without spilling.</p>"},{"location":"code_generation_wasm/#register-spilling","title":"Register Spilling","text":"<p>However register spilling is unavoidable due to program complexity and limit of hardware. </p> <p>Let's consider another example </p> <pre><code>// PA3\n1: x &lt;- 1       // {}\n2: y &lt;- x + 1   // {x}\n3: z &lt;- x * x   // {x,y}\n4: w &lt;- y * x   // {x,y,z}\n5: u &lt;- z + w   // {z,w}\n6: r_ret &lt;- u   // {u}  \n7: ret          // {}\n</code></pre> <p>The SSA form is identical to the above, since there is no variable re-assignment. In the comment, we include the result of the liveness analysis.</p> var 1 2 3 4 5 6 7 reg x * * * y * * z * * w * u * <p>From the live range table able, we find that at peak i.e. instruction <code>4</code>, there are 3 live variables currently. We would need three registers for the allocation.</p> <p>What if we only have two registers? Clearly, we need to \"sacrifice\" some live variable at instruction <code>4</code>, by spilling it back to the temporary variable and reloading before it is needed again. But which one shall we \"sacrifice\"? There are a few options here.</p> <ol> <li>Spill the least urgently needed live variable. Recall that the liveness analysis is a may analaysis, its result is an over-approximation. Some live variables might not be needed at this point.</li> <li>Spill the live variable that interfere the most. This option works for the bruteforce searching coloring algorithm, the idea was to reduce the level of interference so that the remaining graph without this variable can be colored. </li> </ol> <p>For now let's take the first option. Suppose we extend the liveness analysis to keep track of the label where a variable is marked live.</p> <pre><code>// PA3\n1: x &lt;- 1       // {}\n2: y &lt;- x + 1   // {x(3)}\n3: z &lt;- x * x   // {x(3),y(4)}\n4: w &lt;- y * x   // {x(4),y(4),z(5)}\n5: u &lt;- z + w   // {z(5),w(5)}\n6: r_ret &lt;- u   // {u(6)}  \n7: ret          // {}\n</code></pre> <p>From the above results, we can conclude that at instruction <code>4</code>, we should sacrifice the live variable <code>z</code>, because <code>z</code> is marked live at label <code>5</code> which is needed in the instruction one-hop away in the CFG, compared to <code>x</code> and <code>y</code> which are marked live at label <code>4</code>. In other words, <code>z</code> is not as urgently needed compared to <code>x</code> and <code>y</code>. </p> var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * w * u * <p>From the above, we find that the graph is colorable again. However register spilling requires some extra steps. First at label <code>3</code>, variable is <code>z</code> is some register, either <code>r0</code> or <code>r1</code>, assuming in the target code operation <code>*</code> can use the same register for both operands and the result. We encounter another problem. To spill <code>z</code> (from the register) to the temporary variable, we need to figure out which other live variable to be swapped out so that the spilling can be done. Let's illustrate using the same example. </p> <p><pre><code>// PA3_REG\n1: r0 &lt;- 1        // x is r0\n2: r1 &lt;- r0 + 1   // y is r1\n3: ?? &lt;- r0 * r0  // what register should hold the result of x * x, before spilling it to `z`?\n</code></pre> where the comments indicate what happens after the label instruction is excuted.</p> <p>There are two option here</p> <ol> <li><code>??</code> is <code>r1</code>. It implies that we need to spill <code>r1</code> to <code>y</code> first after instruction <code>2</code> and then spill <code>r1</code> to <code>z</code> after instruction <code>3</code>, and load <code>y</code> back to <code>r1</code> after instruction <code>3</code> before instruction <code>4.</code></li> <li><code>??</code> is <code>r0</code>. It implies that we need to spill <code>r0</code> to <code>x</code> first after instruction <code>2</code> and then spill <code>r0</code> to <code>z</code> after instruction <code>3</code>, and load <code>x</code> back to <code>r0</code> after instruction <code>3</code> before instruction <code>4.</code></li> </ol> <p>In this particular example, both options are equally good (or equally bad). In general, we can apply the heuristic of choosing the conflicting variable whose live range ends earlier, hopefully the main subject of spilling (<code>z</code> in this example) is not needed until then. </p> <p>Now let's say we pick the first option, the register allocation continues </p> var 1 2 3 4 5 6 7 reg x * * * r0 y * * r1 z - * r1 w * r0 u * r1 <p>where <code>-</code> indicates taht <code>z</code> is being spilled from <code>r1</code> before label <code>4</code> and it needs to be loaded back to <code>r1</code> before label <code>5</code>.  And the complete code of <code>PA3_REG</code> is as follows</p> <p><pre><code>// PA3_REG\n1: r0 &lt;- 1        // x is r0\n2: r1 &lt;- r0 + 1   // y is r1\n   y  &lt;- r1       // temporarily save y\n3: r1 &lt;- r0 * r0  // z is r1 \n   z  &lt;- r1       // spill to z\n   r1 &lt;- y        // y is r1\n4: r0 &lt;- r1 * r0  // w is r0 (x,y are dead afterwards)\n   r1 &lt;- z        // z is r1\n5: r1 &lt;- r1 + r0  // u is r1 (z,w are dead afterwards)\n6: r_ret &lt;- r1\n7: ret\n</code></pre> In the above, assume that in the target platform, a label can be associated with a sequence of instructions, (which is often the case).</p> <p>As an exercise, work out what if we save <code>x</code> temporarily instead of <code>y</code> at label <code>2</code>.</p>"},{"location":"code_generation_wasm/#register-allocation-for-phi-assignments","title":"Register allocation for phi assignments","text":"<p>What remains to address is the treatment of the phi assignments.</p> <p>Let's consider a slightly bigger example. </p> <p><pre><code>// PA4\n1: x &lt;- input   // {input}\n2: s &lt;- 0       // {x}\n3: c &lt;- 0       // {s,x}\n4: b &lt;- c &lt; x   // {c,s,x}\n5: ifn b goto 9 // {b,c,s,x}\n6: s &lt;- c + s   // {c,s,x}\n7: c &lt;- c + 1   // {c,s,x}\n8: goto 4       // {c,s,x}\n9: r_ret &lt;- s   // {s}\n10: ret         // {}\n</code></pre> In the above we find a sum program with liveness analysis results included as comments.</p> <p>Let's convert it into SSA.</p> <p><pre><code>// PA_SSA4\n1: x1 &lt;- input1  // {input1(1)}\n2: s1 &lt;- 0       // {x1(4)}\n3: c1 &lt;- 0       // {s1(4),x1(4)}\n4: c2 &lt;- phi(3:c1, 8:c3)\n   s2 &lt;- phi(3:s1, 8:s3)\n   b1 &lt;- c2 &lt; x1 // {c2(4),s2(6,9),x1(4)}\n5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)}\n6: s3 &lt;- c2 + s2 // {c2(6),s2(6),x1(4)}\n7: c3 &lt;- c2 + 1  // {c2(7),s3(4),x1(4)}\n8: goto 4        // {c3(4),s3(4),x1(4)}\n9: r_ret &lt;- s2   // {s2(9)}\n10: ret          // {}\n</code></pre> We put the liveness analysis results as comments. </p> <p>There are a few options of handling phi assignments.</p> <ol> <li>Treat them like normal assignment, i.e. translate them back to move instruction (refer to \"SSA back to Pseudo Assembly\" in the name analysis lesson.) This is the most conservative approach definitely work, but not necessary giving us optimized code</li> <li>Ensure the variables in the phi assignments sharing the same registers. </li> </ol> <p>Let's consider the first approach </p>"},{"location":"code_generation_wasm/#conservative-approach","title":"Conservative approach","text":"<p>When we translate the SSA back to PA</p> <pre><code>// PA_SSA_PA4\n1: x1 &lt;- input1  // {input1(1)}\n2: s1 &lt;- 0       // {x1(4)}\n3: c1 &lt;- 0       // {s1(3.1),x1(4)}\n3.1: c2 &lt;- c1     \n     s2 &lt;- s1    // {s1(3.1),x1(4),c1(3.1)}\n4: b1 &lt;- c2 &lt; x1 // {c2(4),s2(6,9),x1(4)}\n5: ifn b1 goto 9 // {b1(5),c2(6),s2(6,9),x1(4)}\n6: s3 &lt;- c2 + s2 // {c2(6),s2(6),x1(4)}\n7: c3 &lt;- c2 + 1  // {c2(7),s3(7.1),x1(4)}\n7.1: c2 &lt;- c3\n     s2 &lt;- s3    // {s3(7.1),x1(4),c3(7.1)}\n8: goto 4        // {c2(4),s2(6,9),x1(4)}\n9: r_ret &lt;- s2   // {s2(9)}\n10: ret          // {}\n</code></pre> <p>It is clear that the program is allocatable without spilling with 4 registers. Let's challenge ourselves with just 3 registers.</p> var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r2 c3 * r0 <p>At the peak of the live variables, i.e. instruction <code>5</code>, we realize that <code>x1</code> is live but not urgently needed until <code>4</code> which is 5-hop away from the current location. Hence we spill it from register <code>r1</code> to the temporary variable to free up <code>r1</code>.  Registers are allocated by the next available in round-robin manner.</p> <pre><code>// PA4_REG1\n1: r0 &lt;- input1  // input is r0\n   r1 &lt;- r0      // x1 is r1\n2: r2 &lt;- 0       // s1 is r2\n3: r0 &lt;- 0       // c1 is r0\n                 // c2 is r0 \n                 // s2 is r2\n                 // no need to load r1 from x1\n                 // b/c x1 is still active in r1\n                 // from 3 to 4\n4: x1 &lt;- r1      // spill r1 to x1\n   r1 &lt;- r0 &lt; r1 // b1 is r1\n5: ifn r1 goto 9 // \n6: r2 &lt;- r0 + r2 // s3 is r2\n7: r0 &lt;- r0 + 1  // c3 is r0\n                 // c2 is r0\n                 // s2 is r2\n8: r1 &lt;- x1      // restore r1 from x1\n   goto 4        // b/c x1 is inactive but needed in 4\n9: r_ret &lt;- r2   // \n10: ret          // \n</code></pre> <p>What if at instruction <code>7</code>, we allocate <code>r1</code> to <code>s3</code> instead of <code>r2</code>? Thanks to some indeterminism, we could have a slightly different register allocation as follows</p> var 1 2 3 3.1 4 5 6 7 7.1 8 9 10 reg input1 * r0 x1 * * * * - - - - - r1 s1 * * r2 c1 * r0 s2 * * * * * r2 c2 * * * * * r0 b1 * r1 s3 * * r1 c3 * r2 <pre><code>// PA4_REG2\n1: r0 &lt;- input1  // input is r0\n   r1 &lt;- r0      // x1 is r1\n2: r2 &lt;- 0       // s1 is r2\n3: r0 &lt;- 0       // c1 is r0\n                 // c2 is r0 \n                 // s2 is r2\n                 // no need to load r1 from x1\n                 // b/c x1 is still active in r1\n                 // from 3 to 4\n4: x1 &lt;- r1      // spill r1 to x1\n   r1 &lt;- r0 &lt; r1 // b1 is r1\n5: ifn r1 goto 9 // \n6: r1 &lt;- r0 + r2 // s3 is r1\n7: r2 &lt;- r0 + 1  // c3 is r2\n7.1: r0 &lt;- r2    // c2 is r0  \n     r2 &lt;- r1    // s2 is r2\n8: r1 &lt;- x1      // restore r1 from x1 \n   goto 4        // b/c x1 is inactive but needed in 4\n9: r_ret &lt;- s2   \n10: ret          \n</code></pre> <p>In this case we have to introduce some additional register shuffling at <code>7.1</code>. Compared to <code>PA4_REG1</code>, this result is less efficient.</p>"},{"location":"code_generation_wasm/#register-coalesced-approach-ensure-the-variables-in-the-phi-assignments-sharing-the-same-registers","title":"Register coalesced approach - Ensure the variables in the phi assignments sharing the same registers","text":"<p>Note that we should not enforce the variable on the LHS of a phi assignment to share the same register as the operands on the RHS. Otherwise, we could lose the chordal graph property of SSA. </p> <p>What we could construct the live range table as follow.</p> var 1 2 3 4 5 6 7 8 9 10 reg input1 * r0 x1 * * * - - - - r1 s1 * r2 c1 r0 s2 * * * * r2 c2 * * * * r0 b1 * r1 s3 * * r2 c3 * r0 <p>Although from the above we find <code>c1</code> seems to be always dead, but it is not, because its value is merged into c2 in label <code>4</code>. This is because in our SSA language, the phi assignment is not an instruction alone while liveness analysis is performed on per instruction level.</p> <p>We also take note we want to <code>c1</code> and <code>c3</code> to share the same register, and <code>s1</code> and <code>s3</code>to share the same register. Hence we can allocate the 3 registers according to the above plan. In this case, we have the same result as the first attempt in the conservative approach <code>PA4_REG1</code>.</p> <p>Note that this approach is not guanranteed to produce more efficient results than the conversvative approach. </p>"},{"location":"code_generation_wasm/#summary-so-far","title":"Summary so far","text":"<p>To sum up the code generation process from PA to 3-address target could be carried out as follows,</p> <ol> <li>Convert the PA program into a SSA.</li> <li>Perform Liveness Analysis on the SSA. </li> <li>Generate the live range table based on the liveness analysis results.</li> <li>Allocate registers based on the live range table. Detect potential spilling.</li> <li>Depending on the last approach, either<ol> <li>convert SSA back to PA and generate the target code according to the live range table, or </li> <li>generate the target code from SSA with register coalesced for the phi assignment operands.</li> </ol> </li> </ol>"},{"location":"code_generation_wasm/#further-reading-for-ssa-based-register-allocation","title":"Further Reading for SSA-based Register Allocation","text":"<ul> <li>https://compilers.cs.uni-saarland.de/papers/ssara.pdf</li> <li>https://dl.acm.org/doi/10.1145/512529.512534</li> </ul>"},{"location":"code_generation_wasm/#wasm-bytecode-reduced-set","title":"WASM bytecode (reduced set)","text":"<p>In this section, we consider the generated Web Assembly (WASM) codes from PA. </p> \\[ \\begin{array}{rccl} (\\tt WASM\\ Instructions) &amp; wis &amp; ::= &amp; [] \\mid wi;wis\\\\  (\\tt WASM\\ Instruction) &amp; wi &amp; ::= &amp; pi \\mid bi \\\\  (\\tt Plain\\ Instruction) &amp; pi &amp; ::= &amp; nop \\mid br\\ L \\mid brIf\\ L \\mid return \\mid get\\ n \\mid set\\ n \\mid \\\\  &amp; &amp; &amp; const\\ c \\mid add \\mid sub \\mid mul \\mid eq \\mid lt \\\\ (\\tt Block\\ Instruction) &amp; bi &amp; ::= &amp; block \\{ wis \\} \\mid loop \\{ wis \\} \\mid if \\{wis\\} else \\{wis\\} \\\\  (\\tt WASM\\ vars) &amp; n &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\  (\\tt constant) &amp; c &amp; ::= &amp; -32768 \\mid ... \\mid 0 \\mid ... \\mid 32767  \\end{array} \\] <p>As mentioned, WASM has 3 registers</p> <ol> <li>a register for the first operand and result</li> <li>a register for the second operand</li> <li>a register for controlling the state of the stack operation (we can't used.)</li> </ol> <p>Technically speaking we only have 2 registers.</p> <p>An Example of WASM byte codes is illustrated as follows</p> <p>Supposed we have a PA program as follows, <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: b &lt;- c &lt; x\n5: ifn b goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: _ret_r &lt;- s\n10: ret\n</code></pre></p> <p>For ease of reasoning, we assume that we map PA temporary variables to WASM variables with the same names. </p> <pre><code>get input\nset x\nconst 0\nset s\nconst 0\nset c\nget c\nget x\nlt\nif {\n    loop {\n        block {\n            get s\n            get c\n            add\n            set s\n            get c\n            const 1\n            add\n            set c\n        }\n        get c\n        get x\n        lt\n        if {\n            br 1\n        } else { }\n    }\n} else { }\nget s\nreturn\n</code></pre>"},{"location":"code_generation_wasm/#wasm-bytecode-operational-semantics","title":"WASM bytecode operational semantics","text":"<p>To describe the operational semantics of WASM bytecodes, we define the following meta symbols.</p> \\[ \\begin{array}{rccl} (\\tt WASM\\ Environment) &amp; \\Delta &amp; \\subseteq &amp; n \\times c \\\\  (\\tt Value\\ Stack) &amp; S &amp; = &amp; \\_,\\_ \\mid c,\\_ \\mid c,c  \\\\  (\\tt Block\\ Instruction\\ Stack) &amp; B &amp; = &amp; [] \\mid (bi,wis);B \\end{array} \\] <p>\\(\\Delta\\) is local environment that maps WASM variables to constants. \\(S\\) is a 2-slot stack where the left slot is the bottom (\\(r_0\\)) and the right slot is the top (\\(r_1\\)). \\(\\_\\) denotes that a slot is vacant. \\(B\\) is a many-slot stack. Each stack frame captures the current block instruction and its following instructions. We assume that the size of \\(B\\) is unbounded. </p> <p>We can decribe the operational semantics of WASM byte codes using the follow rule form</p> \\[  (\\Delta, S, wis, B) \\longrightarrow (\\Delta', S', wis', B') \\] <p>The rule rewrites a configuration \\((\\Delta, S, wis, B)\\) to the next configuration \\((\\Delta', S', wis', B')\\), where \\(\\Delta\\) and \\(\\Delta'\\) are the current and the next states of the local environment, \\(S\\) and \\(S'\\) are the current and the next states of the value stack, \\(wis\\) and \\(wis'\\) are the currrent and next sets of instructions to be processed, \\(B\\) and \\(B'\\) are the current and the next states of the block stack.</p> \\[ \\begin{array}{rc} (\\tt get1) &amp; (\\Delta, \\_, \\_, get\\ n; wis, B) \\longrightarrow (\\Delta, \\Delta(n), \\_, wis, B) \\\\ \\\\  (\\tt get2) &amp; (\\Delta, c, \\_, get\\ n; wis, B) \\longrightarrow (\\Delta, c, \\Delta(n), wis, B) \\\\ \\\\  (\\tt const1) &amp; (\\Delta, \\_, \\_, const\\ c;wis, B) \\longrightarrow (\\Delta, c, \\_, wis, B) \\\\ \\\\  (\\tt const2) &amp; (\\Delta, c_0, \\_, cosnt\\ c_1;wis, B) \\longrightarrow (\\Delta, c_0, c_1, wis, B) \\end{array} \\] <p>The rules \\((\\tt get1)\\) and  \\((\\tt get2)\\) handles the loading variable's content to the stack registers.  The rules \\((\\tt const1)\\) and  \\((\\tt const2)\\) handles the loading constant to the stack registers. </p> \\[ \\begin{array}{rc} (\\tt set) &amp; (\\Delta, c, \\_, set\\ n;wis, B) \\longrightarrow (\\Delta \\oplus(n,c), \\_, \\_, wis, B) \\\\ \\\\  \\end{array} \\] <p>The rule \\((\\tt set)\\) processes the \\(set\\ n\\) instruction by popping the register \\(r_0\\) from the stack and store its content with variable \\(n\\) in \\(\\Delta\\).</p> \\[ \\begin{array}{rc} (\\tt add) &amp; (\\Delta, c_0, c_1, add;wis, B) \\longrightarrow (\\Delta, c_0+c_1, \\_, wis, B) \\\\ \\\\  (\\tt sub) &amp; (\\Delta, c_0, c_1, sub;wis, B) \\longrightarrow (\\Delta, c_0-c_1, \\_, wis, B) \\\\ \\\\  (\\tt mul) &amp; (\\Delta, c_0, c_1, mul;wis, B) \\longrightarrow (\\Delta, c_0*c_1, \\_, wis, B)   \\end{array} \\] <p>The rules \\((\\tt add)\\), \\((\\tt sub)\\) and \\((\\tt mul)\\) process the binary operation assuming both registers in the stack holding some constants. The result of the computation is stored in \\(r_0\\) while \\(r_1\\) becomes empty.</p> \\[ \\begin{array}{rc} (\\tt eq1) &amp; \\begin{array}{c}                  c_0 == c_1                  \\\\ \\hline                 (\\Delta, c_0, c_1, eq;wis, B) \\longrightarrow (\\Delta, 1, \\_, wis, B)                  \\end{array} \\\\ \\\\ (\\tt eq2) &amp; \\begin{array}{c}                  c_0 \\neq c_1                  \\\\ \\hline                 (\\Delta, c_0, c_1, eq;wis, B) \\longrightarrow (\\Delta , 0, \\_, wis, B)                  \\end{array} \\\\ \\\\  (\\tt lt1) &amp; \\begin{array}{c}                  c_0 &lt; c_1                  \\\\ \\hline                 (\\Delta, c_0, c_1, lt;wis, B) \\longrightarrow (\\Delta, 1, \\_, wis, B)                  \\end{array} \\\\ \\\\ (\\tt lt2) &amp; \\begin{array}{c}                  c_0 &gt;= c_1                  \\\\ \\hline                 (\\Delta, c_0, c_1, lt;wis, B) \\longrightarrow (\\Delta , 0, \\_, wis, B)                  \\end{array} \\\\ \\\\  \\end{array} \\] <p>The rules \\((\\tt eq1)\\), \\((\\tt eq2)\\), \\({\\tt lt1}\\) and \\((\\tt lt2)\\) process the boolean operation assuming both registers in the stack holding some constants. The result of the computation is stored in \\(r_0\\) while \\(r_1\\) becomes empty.</p> <p>The next set of rules evaluate by pushing block instructions to the block instruction stack. </p> \\[ \\begin{array}{cc} (\\tt block) &amp; (\\Delta, r_0, r_1, block \\{wis\\};wis', B) \\longrightarrow (\\Delta, r_0, r_1, wis, (block\\{wis\\}, wis');B) \\\\ \\\\  (\\tt loop) &amp; (\\Delta, r_0, r_1, loop \\{wis\\};wis', B) \\longrightarrow (\\Delta, r_0, r_1, wis, (loop\\{wis\\}, wis');B) \\\\ \\\\  (\\tt ifT) &amp;   (\\Delta, 1, \\_, if\\{wis\\} else\\{wis'\\};wis'', B) \\longrightarrow (\\Delta, \\_, \\_, wis, (block \\{wis\\}, wis''); B)  \\\\ \\\\  (\\tt ifF) &amp;   (\\Delta, 0, \\_, if\\{wis\\} else\\{wis'\\};wis'', B) \\longrightarrow (\\Delta, \\_, \\_, wis', (block \\{wis'\\}, wis''); B)  \\end{array} \\] <ul> <li>The rule \\((\\tt block)\\) processes a sequence of instructions starting with a block instruction. It proceeds by evaluating the body of the block instruction and pushing the block instruction and its following instructions into the block instruction stack. </li> <li>The rule \\({\\tt loop}\\) works in a similar manner. </li> <li>The rules \\((\\tt ifT)\\) and \\((\\tt ifF)\\) handle the if-instruction. Depending on the value residing in register <code>0</code> in the value stack, it proceeds with evaluating the then-instructions or the else-instructions, by pushing the to-be-evaluated instruction and the following instructions into the block instruction stack as a \"block instruction\".  </li> </ul> <p>The next set of rules evaluate by popping the top of the block instruction stack.</p> \\[ \\begin{array}{cc} (\\tt br0Block) &amp;  (\\Delta, r_0, r_1, br\\ 0,(block \\{wis'\\}, wis'');B) \\longrightarrow (\\Delta, r_0, r_1, wis'', B)  \\\\ \\\\  (\\tt br0Loop) &amp;  (\\Delta, r_0, r_1, br\\ 0,(loop \\{wis'\\}, wis'');B) \\longrightarrow (\\Delta, r_0, r_1, wis', (loop \\{wis'\\}, wis'');B)  \\\\ \\\\  (\\tt brN) &amp;  (\\Delta, r_0, r_1, br\\ n,(bi, wis);B) \\longrightarrow (\\Delta, r_0, r_1, br\\ (n-1), B) \\\\ \\\\  (\\tt brIfT0Block) &amp;  (\\Delta, 1, \\_, brIf\\ 0;wis,(block \\{wis'\\}, wis'');B) \\longrightarrow (\\Delta, \\_, \\_, wis'', B) \\\\ \\\\  (\\tt brIfT0Loop) &amp;  (\\Delta, 1, \\_, brIf\\ 0;wis,(loop \\{wis'\\}, wis'');B) \\longrightarrow (\\Delta, \\_, \\_, wis', (loop \\{wis'\\}, wis'');B)  \\\\ \\\\  (\\tt brIfTN) &amp;  (\\Delta, 1, \\_, brIf\\ n;wis,(bi, wis');B) \\longrightarrow (\\Delta, 1, \\_, brIf\\ (n-1), B) \\\\ \\\\  (\\tt brIfF) &amp; (\\Delta, 0, \\_, brIf\\ \\_;wis, B) \\longrightarrow (\\Delta, \\_, \\_, wis, B) \\end{array} \\] <ul> <li> <p>The rules \\((\\tt br0Block)\\) and \\((\\tt br0Loop)\\) handle the case in which a branch instruction is applied with <code>0</code>. </p> <ol> <li>When the top of the block instruction stack is a block instruction, the computation proceeds with the \"continuation\" of the block instruction, namely \\(wis''\\). </li> <li>When the top of hte stack is a loop instruction, the computation proceeds by going through another iteration of the loop. </li> </ol> </li> <li> <p>The rule \\((\\tt brN)\\) handle the case in which the branch instruction's operaand is a positive integer \\(n\\), the computation proceeds by evaluating the branch operation with \\(n-1\\) with the top of the block instruction stack removed.  </p> </li> <li> <p>The rules \\((\\tt brIfT0Block)\\) and \\((\\tt brIfT0Loop)\\) process the conditional branch instructions in the similar with as \\((\\tt br0Block)\\) and \\((\\tt br0Loop)\\), except that they are applicable only when the register 0 is storing the value <code>1</code>. </p> </li> <li>The rule \\((\\tt brIfTN)\\) is similar to \\((\\tt brN)\\), excep that it is applicable when the register 0 is having value <code>1</code>.</li> <li>The rule \\((\\tt brIfF)\\) is applied when the register <code>0</code> is having value <code>0</code> and skips the the conditional branch.</li> </ul> <p>The last set of rules handle the no-op and empty instruction sequence. </p> \\[ \\begin{array}{cc} (\\tt Nop) &amp;  (\\Delta, r_0, r_1, nop;wis,B) \\longrightarrow (\\Delta, r_0, r_1, wis, B) \\\\ \\\\  (\\tt empty) &amp; (\\Delta, r_0, r_1, [], (bi, wis');B ) \\longrightarrow (\\Delta, r_0, r_1, wis', B) \\end{array} \\] <p>The \\((\\tt Nop)\\) rule skip the \\(nop\\) instruction. The \\((\\tt empty)\\) rule denotes the end of the current sequence and proceeds with the \"following\" instructions in the block instruction stack. </p>"},{"location":"code_generation_wasm/#conversion-from-pa-to-wasm-bytecodes","title":"Conversion from PA to WASM bytecodes","text":"<p>A simple conversion from PA to WASM bytecodes can be described using the following deduction system.</p> <p>Let \\(M\\) be a mapping from PA temporary variables to WASM local variables.</p> <p>We have three types of rules.</p> <ul> <li>\\(M \\vdash lis \\Rightarrow wis\\), converts a sequence of PA labeled instructions to a sequence of WASM bytecode instructions.</li> <li>\\(M \\vdash_{src} s \\Rightarrow wis\\), converts a PA (source) operand into a sequence of WASM bytecode instructions.</li> <li>\\(M(t)\\), converts a PA variable into a WASM variable. </li> </ul>"},{"location":"code_generation_wasm/#converting-pa-labeled-instructions","title":"Converting PA labeled instructions","text":"\\[ \\begin{array}{rl}     {\\tt (wReturn)} &amp; \\begin{array}{c}                 M \\vdash_{src} s \\Rightarrow wis_1 \\\\                 \\hline                 M \\vdash l_1:rret \\leftarrow s;  l_2: ret \\Rightarrow wis_1 + [return]              \\end{array} \\\\    \\end{array} \\] <p>The rule \\({\\tt (wReturn)}\\) convers the PA return instructions. It first converts the operand \\(s\\) into a sequence of WASM instructions \\(wis_1\\). At this stage, the content of \\(s\\) must have been loaded to the register 0. Next we invoke the WASM \\(return\\).</p> \\[ \\begin{array}{rl}     {\\tt (wMove)} &amp; \\begin{array}{c}                 M \\vdash_{src} s \\Rightarrow wis_1\\ M \\vdash lis \\Rightarrow wis_2\\\\                 \\hline                 M \\vdash l: t \\leftarrow s;  lis \\Rightarrow wis_1 + [set\\ M(t) ] +wis_2              \\end{array} \\\\    \\end{array} \\] <p>The rule \\({\\tt (wMove)}\\) handles the case of a move instruction. In this case we make use of the auxiliary rule \\(M \\vdash s \\Rightarrow wis_1\\) to convert the operand \\(s\\) into a loading instruction in WASM bytecodes. The content of \\(s\\) should be now in the regstier. We make use of \\(get\\ M(t)\\) to transfer the value into the WASM variable \\(M(t)\\). Details fo these auxiliary functions can be found in the next subsection. Using recursion, we convert the instructions sequence \\(lis\\) into \\(wis_2\\). </p> \\[ \\begin{array}{rc}      {\\tt (wEqLoop)} &amp; \\begin{array}{c}                     (l_3-1):goto\\ l_4 \\in lis' \\\\ l_4 == l_1 \\\\ lis_1, lis_2 = split(l_3, lis') \\\\                       M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\\\                     M \\vdash lis_1 \\Rightarrow wis_3 \\ \\ \\ M \\vdash lis_2 \\Rightarrow wis_4 \\\\                      \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis' \\Rightarrow \\\\                       wis_1 + wis_2 + [eq, if \\{ loop \\{ wis_3 + wis_1 + wis_2 + [ eq, brIf\\ 0 ] \\} \\} else \\{ nop \\}] + wis_4                 \\end{array}  \\\\ \\\\      {\\tt (wEqIf)} &amp; \\begin{array}{c}                     (l_3-1):goto\\ l_4 \\in lis' \\\\ l_4 \\neq l_1 \\\\                      lis_1, lis_2 = split(l_3, lis') \\\\                      lis_3, lis_4 = split(l_4, lis_2) \\\\                       M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\\\                     M \\vdash lis_1 \\Rightarrow wis_3 \\ \\ \\ M \\vdash lis_2 \\Rightarrow wis_4 \\ \\ \\ M \\vdash lis_4 \\Rightarrow wis_5 \\\\                      \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 == s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis' \\Rightarrow \\\\                       wis_1 + wis_2 + [eq, if \\{ wis_3 \\} else \\{ wis_4 \\}] + wis_5                 \\end{array}  \\end{array} \\] <p>The rules \\({\\tt (wEqLoop)}\\) and \\({\\tt (wEqIf)}\\) deal with the scenarios in which the leading PA instructions are an equality test followed by a conditional jump. There are two sub cases. </p> <ol> <li>When the target of the condional jump, \\(l_3\\) has an preceding instruction is a \\(goto\\ l_4\\) where \\(l_4\\) is the label of the equality test. We conclude that this PA sequence is translated from a loop in the source SIMP program.  We apply an auxilary function \\(split(l_3, lis')\\) to split \\(lis'\\) into two sub sequences \\(lis_1\\) and \\(lis_2\\) by \\(l_3\\), \\(lis_1\\) must be the body of the loop. and \\(lis_2\\) are the following instructions after the loop. We compile operands of the equality test into \\(wis_1\\) and \\(wis_2\\). We concatenate an \\(eq\\) instruction with a \\(if\\) nested \\(loop\\) block. \\(wis_3\\) is the compiled WASM codes of the loop body, and \\(wis_4\\) is the compiled codes of the instructions following the loop.</li> <li>When the target of the condition jump, \\(l_3\\) has a preceding instruction is a \\(goto\\ l_4\\) where the l_4$ is not the label of the equality test. We conclude that this PA sequence is translated from a if-else statement from the source SIMP program and \\(l_4\\) should be the end of loop. This is because our maximal munch algorithm is structure-preserving, i.e. we insert jumping labels at the end of the then and else branches. </li> </ol> \\[ \\begin{array}{rc}      {\\tt (wEq)} &amp; \\begin{array}{c}                     M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\\\                     M \\vdash lis' \\Rightarrow wis_3 \\ \\ \\ head(lis')  \\texttt{is not an } ifn\\ \\texttt{instruction} \\\\                      \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 == s_2; lis' \\Rightarrow \\\\                       wis_1 + wis_2 + [eq] + wis_3                 \\end{array}  \\end{array} \\] <p>The rule \\((\\tt wEq)\\) handles a normal equality test which is not followed by a conditional jump.</p> \\[ \\begin{array}{rc}      {\\tt (wLtLoop)} &amp; \\begin{array}{c}                     (l_3-1):goto\\ l_4 \\in lis' \\\\ l_4 == l_1 \\\\ lis_1, lis_2 = split(l_3, lis') \\\\                       M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\\\                     M \\vdash lis_1 \\Rightarrow wis_3 \\ \\ \\ M \\vdash lis_2 \\Rightarrow wis_4 \\\\                      \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 &lt; s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis' \\Rightarrow \\\\                       wis_1 + wis_2 + [lt, if \\{ loop \\{ wis_3 + wis_1 + wis_2 + [ lt, brIf\\ 0 ] \\} \\} else \\{ nop \\}] + wis_4                 \\end{array}  \\\\ \\\\      {\\tt (wLtIf)} &amp; \\begin{array}{c}                     (l_3-1):goto\\ l_4 \\in lis' \\\\ l_4 \\neq l_1 \\\\                      lis_1, lis_2 = split(l_3, lis') \\\\                      lis_3, lis_4 = split(l_4, lis_2) \\\\                       M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\\\                     M \\vdash lis_1 \\Rightarrow wis_3 \\ \\ \\ M \\vdash lis_2 \\Rightarrow wis_4 \\ \\ \\ M \\vdash lis_4 \\Rightarrow wis_5 \\\\                      \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 &lt; s_2; l_2:ifn\\ t\\ goto\\ l_3 ; lis' \\Rightarrow \\\\                       wis_1 + wis_2 + [lt, if \\{ wis_3 \\} else \\{ wis_4 \\}] + wis_5                 \\end{array}  \\\\ \\\\       {\\tt (wLt)} &amp; \\begin{array}{c}                                          M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\\\                     M \\vdash lis' \\Rightarrow wis_3 \\ \\ \\ head(lis')  \\texttt{is not an } ifn\\ \\texttt{instruction}\\\\                      \\hline                     M, L \\vdash l_1:t \\leftarrow s_1 &lt; s_2; lis' \\Rightarrow \\\\                       wis_1 + wis_2 + [lt] + wis_3                 \\end{array}  \\end{array} \\] <p>The above rules \\({\\tt (wLtLoop)}\\), \\({\\tt (wLtIf)}\\) and \\({\\tt (wLt)}\\) handle the less than test. They are similar to their equality test counter-parts described earlier. </p> \\[ \\begin{array}{rc}      {\\tt (wPlus)} &amp; \\begin{array}{c}                     M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\ \\ \\ M \\vdash lis' \\Rightarrow wis_3 \\\\                     \\hline                     M \\vdash l:t \\leftarrow s_1 + s_2; lis' \\Rightarrow wis_1 + wis_2 + [add, set\\ M(t)] + wis_3                 \\end{array} \\\\ \\\\        {\\tt (wMinus)} &amp; \\begin{array}{c}                     M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\ \\ \\ M \\vdash lis' \\Rightarrow wis_3 \\\\                     \\hline                     M \\vdash l:t \\leftarrow s_1 - s_2; lis' \\Rightarrow wis_1 + wis_2 + [sub, set\\ M(t)] + wis_3                 \\end{array} \\\\ \\\\        {\\tt (wMult)} &amp; \\begin{array}{c}                     M \\vdash_{src} s_1 \\Rightarrow wis_1 \\ \\ \\ M \\vdash_{src} s_2 \\Rightarrow wis_2 \\ \\ \\ M \\vdash lis' \\Rightarrow wis_3 \\\\                     \\hline                     M \\vdash l:t \\leftarrow s_1 * s_2; lis' \\Rightarrow wis_1 + wis_2 + [mul, set\\ M(t)] + wis_3                 \\end{array} \\\\ \\\\       {\\tt (wGoto)} &amp; \\begin{array}{c}                      M \\vdash lis' \\Rightarrow wis \\\\                     \\hline                     M \\vdash l: goto\\ l'; lis' \\Rightarrow wis                 \\end{array} \\\\ \\\\   \\end{array} \\] <p>Lastly, the rules \\({\\tt (wPlus)}\\), \\({\\tt (wMinus)}\\) and  \\({\\tt (wMult)}\\) convert the binary arithmetic operations. \\({\\tt (wGoto)}\\) skips the goto instruction, which should be handled by the other rules. </p>"},{"location":"code_generation_wasm/#converting-pa-source-operands","title":"Converting PA Source Operands","text":"\\[ \\begin{array}{rl} {\\tt (Const)} &amp; M \\vdash_{src} c \\Rightarrow [const\\ c] \\\\ \\\\  {\\tt (Var)} &amp; M \\vdash_{src} t \\Rightarrow [get\\ M(t)] \\\\ \\\\  \\end{array} \\]"},{"location":"code_generation_wasm/#optimizing-wasm-bytecode","title":"Optimizing WASM bytecode","text":"<p>Though it is limited, there is room to optimize the WASM bytecode. For example, </p> <p>From the following SIMP program </p> <pre><code>r = (1 + 2) * 3\n</code></pre> <p>we generate the following PA code via the Maximal Munch</p> <pre><code>1: t &lt;- 1 + 2\n2: r &lt;- t * 3  \n</code></pre> <p>In turn if we apply the above PA to JVM bytecode conversion</p> <p><pre><code>const 1\nconst 2\nadd\nset t \nget t\nconst 3\nmul\nset r \n</code></pre> As observe, the <code>set t</code> followed by <code>get t</code> are rundandant, because <code>t</code> is not needed later (dead).</p> <pre><code>const 1\nconst 2\nadd\nconst 3\nmul\nset r \n</code></pre> <p>This can either be done via </p> <ol> <li>Liveness analysis on PA level or </li> <li>Generate WASM byte code directly from SIMP.<ul> <li>This requires the expression of SIMP assignment to be left nested. </li> <li>The conversion is beyond the scope of this module.</li> </ul> </li> </ol>"},{"location":"code_generation_wasm/#prettier-wasm-syntax-folded-expression","title":"Prettier WASM syntax - Folded Expression","text":"<p>WASM supports a prettier syntax, known as folded expression. </p> \\[ \\begin{array}{rccl} (\\tt Plain\\ Instruction (Folded)) &amp; pi &amp; ::= &amp; nop \\mid br\\ L \\mid brIf\\ L \\mid return\\ oi \\mid set\\ n\\ oi \\mid oi \\\\  (\\tt Operand\\ Instruction) &amp; oi &amp; ::= &amp; get\\ n \\mid const\\ c \\mid add\\ oi\\ oi \\mid sub\\ oi\\ oi  \\mid mul\\ oi\\ oi  \\mid eq\\ oi\\ oi  \\mid lt\\ oi\\ oi  \\\\  (\\tt Block\\ Instruction (Folded)) &amp; bi &amp; ::= &amp; block \\{ wis \\} \\mid loop \\{ wis \\} \\mid if\\ oi \\{wis\\} else \\{wis\\} \\\\  \\end{array} \\] <p>The earlier WASM example can be rewritten as the following in folded expression form.</p> <p><pre><code>set x (get input)\nset s (const 0)\nset c (const 0)\nif (lt (get c) (get x)) {\n    loop {\n        block {            \n            set s (add (get s) (get c))\n            set c (add (get c) (const 1))\n        }\n        if (lt (get c) (get x)) {\n            br 1\n        } else { }\n    }\n} else { }\nreturn (get s)\n</code></pre> which is closer to source program. </p> <p>In the project, we'll find the PA to WASM conversion rules being re-phrased into the folded expression form.</p>"},{"location":"code_generation_wasm/#further-reading-for-wasm-bytecode-generation","title":"Further Reading for WASM bytecode generation","text":"<ul> <li>https://webassembly.org/</li> <li>https://developer.mozilla.org/en-US/docs/WebAssembly/Reference</li> </ul>"},{"location":"code_generation_wasm/#summary-for-wasm-bytecode-generation","title":"Summary for WASM bytecode generation","text":"<ul> <li>To generate WASM bytecode w/o optimization can be done via deduction system</li> <li>To optimize WASM bytecode, we could apply liveness analysis to eliminate redundant store-then-load sequence.</li> </ul>"},{"location":"dynamic_semantics/","title":"50.054 - Dynamic Semantics","text":""},{"location":"dynamic_semantics/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Explain the small step operational semantics of a programming language.</li> <li>Explain the big step operational semantics of a programming language.</li> <li>Formalize the run-time behavior of a programming language using small step operational semantics.</li> <li>Formalize the run-time behavior of a programming language using big step operational semantics.</li> </ol> <p>Recall that by formalizing the dynamic semantics of a program we are keen to find out</p> <ol> <li>How does the program get executed?</li> <li>What does the program compute / return?</li> </ol>"},{"location":"dynamic_semantics/#operational-semantics","title":"Operational Semantics","text":"<p>Operational Semantics specifies how a program get executed.</p> <p>For example, in the earlier unit, when studying lambada expression, we made use of the \\(\\beta\\)-reduction, the substitution and alpha renaming rules to formalize the execution of a simple lambda expression. As the language grows to include let-binding, conditional expression, we extend the set of rules to include \\({\\tt (Let)}\\), \\({\\tt (IfI)}\\), \\({\\tt (IfT)}\\) and \\({\\tt (IfF)}\\). The set of rules in this example defines the operational semantics of the programming language lambda expression. We can apply these rules to \"evaluate\" a lambda expression by rewriting it by picking a matching rule (w.r.t to the LHS) and turn it into the form of the RHS. This style of semantics specification is called the small step operational semantics as we only specify the intermediate result when we apply a rule.  </p> <p>As we are going to design and implement a compiler for the SIMP language, it is essential to find out how a SIMP program gets executed.</p> <p>To formalize the execution of SIMP program, we can define a set of rewriting rules similar to those for lambda calculus. We need to consider different cases.</p>"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-simp","title":"Small-Step Operational Semantics of SIMP","text":"<p>Let's try to formalize the Operational Semantics of SIMP language,</p> <p>$$ \\begin{array}{rccl} (\\tt SIMP Environment) &amp; \\Delta &amp; \\subseteq &amp; (X \\times C) \\end{array} $$ We model the memory environment of a SIMP program as pair of variable and values. We write \\(dom(\\Delta)\\) to denote the domain of \\(\\Delta\\), i.e. \\(\\{ X \\mid (X,C) \\in \\Delta \\}\\). We assume for all \\(X \\in dom(\\Delta)\\), there exists only one entry of \\((X,C) \\in \\Delta\\).</p> <p>Given \\(S\\) is a set of pairs, we write \\(S(x)\\) to denote \\(a\\) if \\((x,a) \\in S\\), an error otherwise. We write \\(S \\oplus (x,a)\\) to denote \\(S - \\{(x, S(x))\\} \\cup \\{(x, a)\\}\\). </p> <p>We define the operational semantics of SIMP with two sets of rules.</p> <p>The first set of rules deal with expression.</p>"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-simp-expression","title":"Small Step Operational Semantics of SIMP Expression","text":"<p>The set of small step operational semantics for expressions is defined in a relation \\(\\Delta \\vdash E \\longrightarrow E'\\).</p> \\[ {\\tt (sVar)} ~~~ \\Delta \\vdash X \\longrightarrow \\Delta(X) \\] <p>The \\({\\tt (sVar)}\\) rule looks up the value of variable \\(X\\) from the memory environment. If the variable is not found, it gets stuck and an error is returned.</p> \\[ \\begin{array}{rc} {\\tt (sOp1)} &amp; \\begin{array}{c}         \\Delta \\vdash E_1 \\longrightarrow E_1'           \\\\ \\hline         \\Delta \\vdash E_1\\ OP\\ E_2 \\longrightarrow E_1'\\ OP\\ E_2         \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp2)} &amp; \\begin{array}{c}         \\Delta \\vdash E_2 \\longrightarrow E_2'           \\\\ \\hline         \\Delta \\vdash C_1 \\ OP\\ E_2 \\longrightarrow C_1\\ OP\\ E_2'         \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (sOp3)} &amp; \\begin{array}{c}         C_3 = C_1 \\ OP\\ C_2         \\\\ \\hline         \\Delta \\vdash C_1 \\ OP\\ C_2 \\longrightarrow C_3         \\end{array} \\end{array} \\] <p>The above three rules handle the binary operation expression.</p> <ol> <li>\\({\\tt (sOp1)}\\) matches with the case where both operands are not constant values. It evalues the first operand by one step.  </li> <li>\\({\\tt (sOp2)}\\) matches with the case where the first operand becomes constant, it evaluates the second operand by one step.</li> <li>\\({\\tt (sOp3)}\\) matches with the case where both operands are constant. It returns the result by applying the binary operation to the two constant values.</li> </ol> \\[ \\begin{array}{rc} {\\tt (sParen1)} &amp; \\begin{array}{c}                  \\Delta \\vdash E \\longrightarrow E'                  \\\\ \\hline                   \\Delta \\vdash (E) \\longrightarrow (E')                  \\end{array} \\\\ \\\\ {\\tt (sParen2)} &amp; \\begin{array}{c}                  \\Delta \\vdash (C) \\longrightarrow C                  \\end{array} \\end{array} \\] <p>The rules \\({\\tt  (sParen1)}\\) and \\({\\tt (sParent2)}\\) evaluate an expression enclosed by parantheses. </p>"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-simp-statement","title":"Small Step Operational Semantics of SIMP statement","text":"<p>The small step operational semantics of statements are defined by the relation \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\). The pair of a environment and a statement is called a program configuration.</p> \\[ \\begin{array}{cc} {\\tt (sAssign1)} &amp; \\begin{array}{c}      \\Delta\\vdash E \\longrightarrow E'      \\\\ \\hline      (\\Delta, X = E;) \\longrightarrow  (\\Delta, X = E';)      \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sAssign2)} &amp; \\begin{array}{c}       \\Delta' = \\Delta \\oplus (X, C)      \\\\ \\hline      (\\Delta, X = C;) \\longrightarrow (\\Delta', nop)      \\end{array} \\end{array} \\] <p>The rules \\({\\tt (sAssign1)}\\) and \\({\\tt (sAssign2)}\\) handle the assignment statements.</p> <ol> <li>\\({\\tt (sAssign1)}\\) matches with the case that the RHS of the assignment is not a constant, it evaluates the RHS expression by one step.</li> <li>\\({\\tt (sAssign2)}\\) matches with the case that the RHS is a constant, it updates the environment by setting \\(C\\) as the new value of variable \\(X\\). The statement of the resulting configuration a \\(nop\\).</li> </ol> \\[ \\begin{array}{cc} {\\tt (sIf1)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\longrightarrow E'     \\\\ \\hline     (\\Delta, if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\longrightarrow (\\Delta,  if\\ E'\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\end{array} \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf2)} &amp;     (\\Delta, if\\ true\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\longrightarrow (\\Delta, \\overline{S_1}) \\end{array} \\] \\[ \\begin{array}{cc} {\\tt (sIf3)} &amp;     (\\Delta, if\\ false\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\})     \\longrightarrow (\\Delta, \\overline{S_2}) \\end{array} \\] <p>The rules \\({\\tt (sIf1)}\\), \\({\\tt (sIf2)}\\) and \\({\\tt (sIf3)}\\) handle the if-else statement.</p> <ol> <li>\\({\\tt (sIf1)}\\) matches with the case where the condition expression \\(E\\) is not a constant value. It evaluates \\(E\\) to \\(E'\\) one step.</li> <li>\\({\\tt (sIf2)}\\) matches with the case where the condition expression is \\(true\\), it proceeds to evaluate the statements in the then clauses.</li> <li>\\({\\tt (sIf3)}\\) matches with the case where the condition expression is \\(false\\), it proceeds to evaluate the statements in the else clauses.</li> </ol> \\[ \\begin{array}{cc} {\\tt (sWhile)} &amp;     (\\Delta, while\\ E\\ \\{\\overline{S}\\} )     \\longrightarrow (\\Delta,  if\\ E\\ \\{\\overline{S}; while\\ E\\ \\{\\overline{S}\\}\\}\\ else\\ \\{ nop \\}) \\end{array} \\] <p>The rule \\({\\tt (sWhile)}\\) evaluates the while statement by reiwrting it into a if-else statement.</p> <ul> <li>In the then branch, we unroll the while loop body once followed by the while loop.</li> <li>In the else branch, we should exit the while loop thus, a \\(nop\\) statement is used.</li> </ul> \\[ {\\tt (sNopSeq)} ~~ (\\Delta, nop; \\overline{S}) \\longrightarrow (\\Delta, \\overline{S}) \\] \\[ \\begin{array}{cc} {\\tt (sSeq)} &amp; \\begin{array}{c}     S \\neq nop\\ \\ \\ (\\Delta, S) \\longrightarrow (\\Delta', S')     \\\\ \\hline    (\\Delta, S \\overline{S}) \\longrightarrow (\\Delta', S' \\overline{S})    \\end{array} \\end{array} \\] <p>The rules \\({\\tt (sNopSeq)}\\) and \\({\\tt (sSeq)}\\) handle a sequence of statements.</p> <ol> <li>\\({\\tt (sNopSeq)}\\) rule handles the special case where the leading statement is a \\(nop\\).</li> <li>\\({\\tt (Seq)}\\) rule handles the case where the leading statement is not a \\(nop\\). It evalues \\(S\\) by one step.</li> </ol> <p>For example,</p> <pre><code>{(input, 1)},\nx = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n---&gt; # using (sSeq)\n   {(input,1)}, x = input \n   ---&gt; # (sAssign1) \n   {(input,1)}, x = 1\n   ---&gt; # (sAssign2) \n   {(input, 1), (x,1)}, nop\n---&gt;\n{(input,1), (x,1)},\nnop;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\n---&gt; # (sNopSeq)\n\n\n{(input,1), (x,1)},\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\n---&gt; # (sSeq), (sAssign2), (sNoSeq)\n\n{(input,1), (x,1), (s,0)},\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\n---&gt; # (sSeq), (sAssign2), (sNoSeq)\n\n{(input,1), (x,1), (s,0), (c,0)},\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n---&gt; # (sSeq) \n    {(input,1), (x,1), (s,0), (c,0)},\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sWhile)\n\n    {(input,1), (x,1), (s,0), (c,0)},\n    if (c &lt; x) {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n\n    ---&gt; # (sIf1)\n        {(input,1), (x,1), (s,0), (c,0)}, c &lt; x \n        ---&gt; # (sOp1) \n        {(input,1), (x,1), (s,0), (c,0)}, 0 &lt; x \n        ---&gt; # (sOp2) \n        {(input,1), (x,1), (s,0), (c,0)}, 0 &lt; 1 \n        ---&gt; # (sOp3) \n        {(input,1), (x,1), (s,0), (c,0)}, true \n    ---&gt; \n    {(input,1), (x,1), (s,0), (c,0)},\n    if true {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n    ---&gt; # (sIf2)\n    {(input,1), (x,1), (s,0), (c,0)},\n    s = c + s;\n    c = c + 1;\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sSeq)\n        {(input,1), (x,1), (s,0), (c,0)},\n        s = c + s ---&gt; # (sAssign1)\n            {(input,1), (x,1), (s,0), (c,0)},\n            c + s ---&gt; # (sOp1) \n            0 + s ---&gt; # (sOp2) \n            0 + 0 ---&gt; # (sOp3) \n            0\n        {(input,1), (x,1), (s,0), (c,0)},\n        s = 0 ---&gt; # (sAssign2)\n        {(input,1), (x,1), (s,0), (c,0)},\n        nop \n    ---&gt; # (sNopSeq)\n    {(input,1), (x,1), (s,0), (c,0)},\n    c = c + 1;\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sSeq)\n        {(input,1), (x,1), (s,0), (c,0)},\n        c = c + 1 ---&gt; # (sAssign1)\n            {(input,1), (x,1), (s,0), (c,0)},\n            c + 1 ---&gt; # (sOp1)\n            0 + 1 ---&gt; # (SOp3)\n            1\n        {(input,1), (x,1), (s,0), (c,1)},\n        c = 1 ---&gt; # (sAssign2)\n        {(input,1), (x,1), (s,0), (c,1)},\n        nop\n    ---&gt; # (sNopSeq) \n    {(input,1), (x,1), (s,0), (c,1)},\n    while c &lt; x {\n        s = c + s;\n        c = c + 1;\n    }\n    ---&gt; # (sWhile)\n    {(input,1), (x,1), (s,0), (c,1)},\n    if (c &lt; x) {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n    ---&gt; # (sIf1)\n        {(input,1), (x,1), (s,0), (c,1)}, c &lt; x \n        ---&gt; # (sOp1) \n        {(input,1), (x,1), (s,0), (c,1)}, 1 &lt; x \n        ---&gt; # (sOp2) \n        {(input,1), (x,1), (s,0), (c,1)}, 1 &lt; 1 \n        ---&gt; # (sOp3) \n        {(input,1), (x,1), (s,0), (c,1)}, false \n    ---&gt; \n    {(input,1), (x,1), (s,0), (c,1)},\n    if false {\n        s = c + s;\n        c = c + 1;\n        while c &lt; x {\n            s = c + s;\n            c = c + 1;\n        }\n    } else {\n        nop\n    }\n    ---&gt; # (sIf3)\n    {(input,1), (x,1), (s,0), (c,1)},\n    nop\n---&gt; # (sNopSeq)\n{(input,1), (x,1), (s,0), (c,1)}\nreturn s;\n</code></pre> <p>At last the derivation stop at the return statement. We can return the value <code>0</code> as result.</p>"},{"location":"dynamic_semantics/#big-step-operational-semantics","title":"Big Step Operational Semantics","text":"<p>Small step operational semantics defines the run-time behavior of programs step by step (kinda like slow motion.) Some times we want to define the run-time behaviors by \"fast-forwarding\" to the result. This leads us to the big step operatinal semantics. Big step operatinal semantics in some literature is also called the structural operational semantics as it leverages on the syntactic structure of the program.</p>"},{"location":"dynamic_semantics/#big-step-operational-semantics-for-simp-expressions","title":"Big Step Operational Semantics for SIMP expressions","text":"<p>We define the big step oeprational semantics for SIMP expressions via a relation \\(\\Delta \\vdash E \\Downarrow C\\), which reads under the memory environment \\(\\Delta\\) the expression \\(E\\) is evaluated constant \\(C\\).</p> <p>We consider the following three rules</p> \\[ {\\tt (bConst)} ~~~~ \\Delta \\vdash C \\Downarrow C \\] <p>In case that the expression is a constant, we return the constant itself.</p> \\[ {\\tt (bVar)} ~~~~ \\Delta \\vdash X \\Downarrow \\Delta(X) \\] <p>In case that the expression is a variable \\(X\\), we return the value associated with \\(X\\) in \\(\\Delta\\).</p> \\[ \\begin{array}{rc} {\\tt (bOp)} &amp; \\begin{array}{c}             \\Delta \\vdash E_1 \\Downarrow C_1 ~~~ \\Delta \\vdash E_2 \\Downarrow C_2 ~~~~             C_1\\ OP\\ C_2 = C_3             \\\\ \\hline             \\Delta \\vdash E_1\\ OP\\ E_2 \\Downarrow C_3             \\end{array} \\end{array} \\] <p>in case that the expression is a binary operation, we evaluate the two operands to values and apply the binary operation to the constant values.</p> \\[ \\begin{array}{rc} {\\tt (bParen)} &amp; \\begin{array}{c}                 \\Delta \\vdash E \\Downarrow C                 \\\\ \\hline                 \\Delta \\vdash (E) \\Downarrow C                \\end{array} \\end{array} \\] <p>the last rule \\({\\tt (bParen)}\\) evaluates an expression enclosed by parantheses.  </p>"},{"location":"dynamic_semantics/#big-step-operational-semantics-for-simp-statements","title":"Big Step Operational Semantics for SIMP statements","text":"<p>We define the big step operational semantics for SIMP statement using a relation \\((\\Delta, S) \\Downarrow \\Delta'\\), which says the program configuration \\((\\Delta, S)\\) is evaluated to result memory environment \\(\\Delta'\\) assuming \\(S\\) is terminating under \\(\\Delta\\). Note that big step operational semantics for SIMP statement can only defines the behavior of terminating program configurations.</p> <p>We consider the following rules</p> \\[ \\begin{array}{rc} {\\tt (bAssign)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow C     \\\\ \\hline     (\\Delta, X = E) \\Downarrow \\Delta \\oplus (X, C)     \\end{array} \\end{array} \\] <p>In case that the statement is an assignment, we evaluate the RHS expression to a constant value \\(c\\) and update the memory environment.</p> \\[ \\begin{array}{rc} {\\tt (bIf1)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow true ~~~~~~     (\\Delta, \\overline{S_1}) \\Downarrow \\Delta_1     \\\\ \\hline     (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_1     \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bIf2)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow false ~~~~~~     (\\Delta, \\overline{S_2}) \\Downarrow \\Delta_2     \\\\ \\hline     (\\Delta, if\\ E\\ \\{ \\overline{S_1} \\}\\ else\\ \\{ \\overline{S_2} \\} ) \\Downarrow \\Delta_2     \\end{array} \\end{array} \\] <p>In case that the statement is an if-else statement, we evaluate \\(\\overline{S_1}\\) if the conditional expression is \\(true\\), otherwise evaluate \\(\\overline{S_2}\\).</p> \\[ \\begin{array}{rc} {\\tt (bWhile1)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow true ~~~~~~     (\\Delta, \\overline{S}; while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta'     \\\\ \\hline     (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta'     \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (bWhile2)} &amp; \\begin{array}{c}     \\Delta \\vdash E \\Downarrow false     \\\\ \\hline     (\\Delta, while\\ E\\ \\{ \\overline{S} \\}) \\Downarrow \\Delta     \\end{array} \\end{array} \\] <p>In case that the statment is a while loop. We evaluate the body followed by the while loop again when the loop condition expression is \\(true\\), otherwise, we exit the while loop and return the existing memory environment.</p> \\[ {\\tt (bNop)} ~~~~ (\\Delta, nop) \\Downarrow \\Delta \\] <p>In case that the statement is a <code>nop</code> statement, there is no change to the memory environment.</p> \\[ {\\tt (bReturn)} ~~~~ (\\Delta, return\\ X) \\Downarrow \\Delta \\] \\[ \\begin{array}{rc} {\\tt (bSeq)} &amp; \\begin{array}{c}             (\\Delta, S) \\Downarrow \\Delta' ~~~~ (\\Delta', \\overline{S}) \\Downarrow \\Delta''             \\\\ \\hline             (\\Delta, S \\overline{S}) \\Downarrow \\Delta''             \\end{array} \\end{array} \\] <p>In case of a sequence of statement, we evaluate the leading statement to an updated environment and use the updated environment to evaluate the following statements.</p> <p>For example, the following derivation (tree) is the evaluate of our running example using the big step operational semantics. The reason of having a tree derivation as we are evaluating the SIMP program to the final result directly by evaluating its sub components recursively / inductively.</p> <pre><code>                                   {(input,1),(x,1)} |- 0 \u21d3 0 (bConst)\n                                   ---------------------(bAssign)     [sub tree 1]\n                                   {(input,1), (x,1)}, \n                                   s = 0; \n{(input,1)} |- input \u21d3 1 (bVar)    \u21d3 {(input,1), (x,1), (s,0)}      \n---------------- (bAssign)         -------------------------------------------(bSeq)\n{(input,1)},                       {(input,1), (x,1)},  \nx = input;                         s = 0;\n\u21d3 {(input,1), (x,1)}               c = 0;\n                                   while c &lt; x {\n                                   s = c + s;\n                                   c = c + 1;\n                                   }\n                                   return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)}\n---------------------------------------------------------------------------- (bSeq)\n{(input, 1)}, \nx = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s; \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)}\n</code></pre> <p>where sub derivation<code>[sub tree 1]</code> is as follows</p> <pre><code>{(input,1), (x,1), (s,0)}              [sub tree 2]  -------------------- (bReturn)\n|- 0 \u21d3 0 (bConst)                                    {(input,1), (x,1), (s,0), (c,1)}, \n                                                     return s; \u21d3 \n                                                     {(input,1), (x,1), (s,0), (c,1)}\n--------------------------(bAssign)   -------------------------------------- (bSeq)\n{(input,1), (x,1), (s,0)},            {(input,1), (x,1), (s,0), (c,0)},\nc = 0;                                while c &lt; x {s = c + s; c = c + 1;} \n\u21d3                                     return s; \u21d3 {(input,1), (x,1), (s,0), (c,1)}\n{(input,1),(x,1),(s,0),(c,0)}                    \n---------------------------------------------------------------------------- (bSeq)\n{(input,1), (x,1), (s,0)},\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n\u21d3 {(input,1), (x,1), (s,0), (c,1)}\n</code></pre> <p>where <code>[sub tree 2]</code> is</p> <pre><code>{(input,1), (x,1), (s,0), (c,0)} \n|- c \u21d3 0 (bVar)\n\n{(input,1), (x,1), (s,0), (c,0)} \n|- x \u21d3 1 (bVar)\n\n0 &lt; 1  == true                         [sub tree 3]          [sub tree 4]\n-------------------------------- (bOp) ---------------------------------- (bSeq)\n{(input,1), (x,1), (s,0), (c,0)}       {(input,1), (x,1), (s,0), (c,0)},\n|- c &lt; x \u21d3 true                         s = c + s; c = c + 1; \n                                        while c &lt; x {s = c + s; c = c + 1;} \u21d3\n                                        {(input,1), (x,1), (s,0), (c,1)}\n-----------------------------------------------------------------------  (bWhile1)\n{(input,1), (x,1), (s,0), (c,0)},\nwhile c &lt; x {s = c + s; c = c + 1;} \u21d3 {(input,1), (x,1), (s,0), (c,1)}\n</code></pre> <p>where <code>[sub tree 3]</code> is</p> <pre><code>{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- c \u21d3 0 (bVar)\n\n{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- s \u21d3 0 (bVar)\n\nc + s == 0\n------------------------------------ (bOp)\n{(input, 1), (x, 1), (s, 0), (c, 0)} \n|- c + s \u21d3 0  \n------------------------------------- (bAssign)\n{(input, 1), (x, 1), (s, 0), (c, 0)},\ns = c + s; \u21d3\n{(input, 1), (x, 1), (s, 0), (c, 0)}\n</code></pre> <p>where <code>[sub tree 4]</code> is</p> <pre><code>{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- c \u21d3 0 (bVar)\n\n{(input, 1), (x, 1), (s, 0), (c, 0)}\n|- 1 \u21d3 1 (bConst)\n\nc + 1 == 1\n------------------------------------ (bOp)\n{(input, 1), (x, 1), (s, 0), (c, 0)} \n|- c + 1 \u21d3 1  \n-------------------------------------- (bAssign)\n{(input, 1), (x, 1), (s, 0), (c, 0)},\nc = c + 1; \u21d3\n{(input, 1), (x, 1), (s, 0), (c, 1)}                [sub tree 5]\n--------------------------------------------------------------------- (bSeq)\n{(input, 1), (x, 1), (s, 0), (c, 0)},\nc = c + 1; \nwhile c &lt; x {s = c + s; c = c + 1;} \u21d3 {(input, 1), (x, 1), (s, 0), (c, 1)}\n</code></pre> <p>where <code>[sub tree 5]</code> is</p> <pre><code>{(input, 1), (x, 1), (s, 0), (c, 1)} (bVar)\n|- c \u21d3 1 \n\n{(input, 1), (x, 1), (s, 0), (c, 1)} (bVar)\n|- x \u21d3 1 \n\n1 &lt; 1 == false\n------------------------------------ (bOp)\n{(input, 1), (x, 1), (s, 0), (c, 1)} \n|- c &lt; x \u21d3 false\n---------------------------------------------------- (bWhile2)\n{(input, 1), (x, 1), (s, 0), (c, 1)}, \nwhile c &lt; x {s = c + s; c = c + 1;} \u21d3\n{(input, 1), (x, 1), (s, 0), (c, 1)}\n</code></pre>"},{"location":"dynamic_semantics/#quick-summary-small-step-vs-big-step-operational-semantics","title":"Quick Summary: Small step vs Big Step operational semantics","text":"Small step operational semantics Big step operational semantics mode one step of change at a time many steps of changes at a time derivation it is linear it is a tree cons it is slow-paced and lengthy, requires more rules it is a fast-forward version, requirews fewer rules pros it is expressive, supports non-terminiating program it assumes program is terminating"},{"location":"dynamic_semantics/#formal-results","title":"Formal Results","text":"<p>We use \\(\\longrightarrow^*\\) to denote multiple steps of derivation with \\(\\longrightarrow\\).</p>"},{"location":"dynamic_semantics/#lemma-1-agreement-of-small-step-and-big-step-operational-semantics-of-simp","title":"Lemma 1 (Agreement of Small Step and Big Step Operational Semantics of SIMP)","text":"<p>Let \\(\\overline{S}\\) be a SIMP program, \\(\\Delta\\) be a memory environment. Then \\(\\Delta, \\overline{S} \\Downarrow \\Delta'\\) iff \\((\\Delta, \\overline{S}) \\longrightarrow^* (\\Delta', return\\ X)\\) for some \\(X\\).</p> <p>Proof of this lemma requires some knowledge which will be discussed in the upcoming classes.</p>"},{"location":"dynamic_semantics/#operational-semantics-of-pseudo-assembly","title":"Operational Semantics of Pseudo Assembly","text":"<p>Next we consider the operational semantics of pseudo assembly.</p> <p>Let's define the environments required for the rules.</p> \\[ \\begin{array}{rccl} (\\tt PA\\ Program) &amp; P &amp; \\subseteq &amp; (l \\times li)  \\\\ (\\tt PA\\ Environment) &amp; L &amp; \\subseteq &amp; (t \\times c) \\cup (r \\times c) \\end{array} \\] <p>We use \\(P\\) to denote a PA program, which is a mapping from label to labeled instructions. We use \\(L\\) to denote a memory environment which is a mapping from temp variable or register to constant values.</p>"},{"location":"dynamic_semantics/#small-step-operational-semantics-of-pseudo-assembly","title":"Small Step Operational Semantics of Pseudo Assembly","text":"<p>The dynamic semantics of the pseudo assembly program can be defined using a rule of shape \\(P \\vdash (L, li) \\longrightarrow (L', li')\\), which reads, given a PA program \\(P\\), the current program context \\((L,li)\\) is evaluated to \\((L', li')\\). Note that we use a memory environment and program label instruction pair to denote a program context.</p> \\[ {\\tt (pConst)}~~~P \\vdash (L, l: d \\leftarrow c) \\longrightarrow (L \\oplus (d,c), P(l+1)) \\] <p>In the \\({\\tt (pConst)}\\) rule, we evaluate an assignment instruction of which the RHS is a constant. We update the value of the LHS in the memory environment as \\(c\\) and move on to the next instruction.</p> \\[ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r) \\longrightarrow (L \\oplus (d,L(r)), P(l+1))   \\] \\[{\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t) \\longrightarrow (L \\oplus (d,L(t)), P(l+1)) \\] <p>In the \\({\\tt (pRegister)}\\) and the \\({\\tt (pTempVar)}\\) rules, we evaluate an assignment instruction of which the RHS is a register (or a temp variable). We look up the value of the register (or the temp variable) from the memory environment and use it as the updated value of the LHS in the memory environment. We move on to the next label instruction.</p> \\[ \\begin{array}{rc} {\\tt (pOp)} &amp;  \\begin{array}{c}         c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2         \\\\ \\hline         P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2) \\longrightarrow (L \\oplus (d,c_3), P(l+1))           \\end{array} \\end{array} \\] <p>The \\({\\tt (pOp)}\\) rule handles the case where the RHS of the assignment is a binary operation. We first look up the values of the operands from the memory environment. We then apply the binary operation to the values. The result will be used to update the value of the LHS in the memory environment.</p> \\[ \\begin{array}{rc} {\\tt (pIfn0)} &amp; \\begin{array}{c}      L(s) = 0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l'))      \\end{array} \\end{array} \\] \\[ \\begin{array}{rc} {\\tt (pIfnNot0)} &amp; \\begin{array}{c}      L(s) \\neq  0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l') \\longrightarrow (L, P(l+1))      \\end{array} \\end{array} \\] <p>The rules \\({\\tt (pIfn0)}\\) and \\({\\tt (pIfnNot0)}\\) deal with the conditional jump instruction. We first look up the conditional operand's value in the memory environment. If it is 0, we ignore the jump and move on to the next instruction, otherwiwse, we perform a jump but changing the program context to the target label instruction.</p> \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l') \\longrightarrow (L, P(l')) \\] <p>The rule \\({\\tt (pGoto)}\\) jumps to to the target label instruction.</p> <p>Note that there is no rule for \\(ret\\) as the program execution will stop there. Further more, the set of rules does not mention the scenario in which the look up of a register (or a temp variable) in the environment fails. In these cases, the program exit with an error.</p> <p>For example, let \\(P\\) be</p> <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t &lt;- c &lt; x \n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s\n10: ret\n</code></pre> <p>and \\(input = 1\\).</p> <p>We have the following derivation</p> <pre><code>P |- {(input,1)}, 1: x &lt;- input ---&gt; # (pTempVar)\nP |- {(input,1), (x,1)}, 2: s &lt;- 0 ---&gt; # (pConst)\nP |- {(input,1), (x,1), (s,0)}, 3: c &lt;- 0 ---&gt; # (pConst)\nP |- {(input,1), (x,1), (s,0), (c,0)}, 4: t &lt;- c &lt; x ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 5: ifn t goto 9 ---&gt; # (pIfn0)\nP |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 6: s &lt;- c + s ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,0), (t,1)}, 7: c &lt;- c + 1 ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 8: goto 4 ---&gt; # (pGoto)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,1)}, 4: t &lt;- c &lt; x ---&gt; # (pOp)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 5: ifn t goto 9 ---&gt; # (pIfnNot0)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,0)}, 9: rret &lt;- s ---&gt; # (pTempVar)\nP |- {(input,1), (x,1), (s,0), (c,1), (t,0), (rret, 0)}, 10: ret\n</code></pre>"},{"location":"dynamic_semantics/#formal-results_1","title":"Formal Results","text":""},{"location":"dynamic_semantics/#definition-consistency-of-the-memory-environments","title":"Definition: Consistency of the memory environments","text":"<p>Let \\(\\Delta\\) be a SIMP memory environment and \\(L\\) be a pseudo assembly memory environment. We say \\(\\Delta\\) is consistent with \\(L\\) (written \\(\\Delta \\Vdash L\\)), iff</p> <ol> <li>\\(\\forall (x,v) \\in \\Delta\\), \\((x,conv(v)) \\in L\\), and</li> <li>\\(\\forall (y,u) \\in L\\), \\((y, v) \\in \\Delta\\) where \\(u=conv(v)\\).</li> </ol>"},{"location":"dynamic_semantics/#lemma-correctness-of-the-maximal-munch-algorithm","title":"Lemma: Correctness of the Maximal Munch Algorithm","text":"<p>Let \\(S\\) and \\(S'\\) be SIMP program statements. Let \\(\\Delta\\) and \\(\\Delta'\\) be SIMP memory environments such that \\((\\Delta, S) \\longrightarrow (\\Delta', S')\\). Let \\(P\\) be a pseudo assembly program such that \\(G_s(S) = P\\). Let \\(L\\) and \\(L'\\) be pseudo assembly memory enviornments. Let \\(\\Delta \\Vdash L\\). Then we have \\(P \\vdash (L, l:i) \\longrightarrow (L', l':i')\\) and \\(\\Delta' \\Vdash L'\\)</p>"},{"location":"dynamic_semantics/#proof","title":"Proof","text":"<p>Since the \\(S\\) could be a non-terminating program, the derivation via small step operational semantics could be infinite. We need a co-inductive proof, which is beyond the scope of this module. We will only discuss about this when we have time.</p>"},{"location":"dynamic_semantics/#what-about-big-step-operational-semantics-of-pseudo-assembly","title":"What about big step operational semantics of Pseudo Assembly?","text":"<p>As Pseudo Assembly is a flatten language with goto statement, there is no nesting of statement or expression. There is no much value in defining the big step operatnal semantics, i.e. there is no way to \"fast-forward\" a sub statement / a sub expression per se.</p> <p>If you are interested in details of big step operational semantics, you may refer to this paper, which presented the operational and denotational semantics with a language with GOTO (more structures than our Pseudo Assembly.)</p> <pre><code>https://link.springer.com/article/10.1007/BF00264536\n</code></pre>"},{"location":"dynamic_semantics/#denotational-semantics-optional-materials","title":"Denotational Semantics (Optional Materials)","text":"<p>Next we briefly discuss another form of dynamic semantics specification. Denotational Semantics aims to provide a meaning to a program. The \"meaning\" here is to find the result returned by the program. Now we may argue that is it the same as the big step operational semantics? There is some difference between the denotational semantics and big step operational semantics. We will defer the discussion and comparison towards the end of this unit.</p> <p>In denotational semantics, the \"meaning\" of a program is given by a set of semantic functions. These functions are mapping program objects from the syntactic domain to math objects in the semantic domain.</p>"},{"location":"dynamic_semantics/#syntactic-domains","title":"Syntactic Domains","text":"<p>In many cases, the syntactic domains are defined by the grammar rules.</p> <p>For SIMP program, we have the following syntactic domains.</p> <ol> <li>\\(S\\) denotes the domain of all valid single statement</li> <li>\\(E\\) denotes the domain of all valid expressions</li> <li>\\(\\overline{S}\\) denotes the domain of all valid sequence statements</li> <li>\\(OP\\) denotes the domain of all valid operators.</li> <li>\\(C\\) denotes the domain of all constant values.</li> <li>\\(X\\) denotes the domain of all variables.</li> </ol>"},{"location":"dynamic_semantics/#semantic-domains","title":"Semantic Domains","text":"<ol> <li>\\(Int\\) denotes the set of all integers values</li> <li>\\(Bool\\) denotes the set of \\(\\{true, false\\}\\)</li> <li>Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\times D_2\\) denotes the cartesian product of the two.</li> <li>Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\cup D_2\\) denotes the union and \\(D_1 \\cap D_2\\) denotes the intersection.</li> <li>Given that \\(D_1\\) and \\(D_2\\) are domains, \\(D_1 \\rightarrow D_2\\) denotes a functional mapping from domain \\(D_1\\) to domain \\(D_2\\).<ul> <li>Note that \\(D_1 \\rightarrow D_2 \\rightarrow D_3\\) is intepreted as \\(D_1 \\rightarrow (D_2 \\rightarrow D_3)\\).</li> </ul> </li> <li>Given that \\(D\\) is a domain, \\({\\cal P}(D)\\) denots the power set of \\(D\\).</li> </ol>"},{"location":"dynamic_semantics/#denotational-semantics-for-simp-expressions","title":"Denotational Semantics for SIMP expressions","text":"<p>The denotational semantics for the SIMP expression is defined by the following semantic functions.</p> <p>Let \\(\\Sigma = {\\cal P} (X \\times (Int\\cup Bool))\\)</p> \\[ \\begin{array}{lll} {\\mathbb E} [\\![ \\cdot ]\\!]\\  :\\  E  &amp;\\rightarrow&amp;\\  \\Sigma \\rightarrow (Int \\cup Bool) \\\\ {\\mathbb E}[\\![ X ]\\!] &amp; = &amp; \\lambda\\sigma.\\sigma(X) \\\\ {\\mathbb E}[\\![ c ]\\!] &amp; = &amp; \\lambda\\sigma. c \\\\ {\\mathbb E}[\\![ E_1\\ OP\\ E_2 ]\\!] &amp; = &amp;\\lambda\\sigma.  {\\mathbb E}[\\![ E_1]\\!]\\sigma\\ [\\![ OP ]\\!]\\  {\\mathbb E}[\\![ E_2]\\!]\\sigma\\\\\\ \\end{array} \\] <p>The signature of the semantic function indicates that we map a SIMP expression into a function that takes a memory environment and returns a contant value.</p> <p>Implicitly, we assume that there exists a builtin semantic function that maps operator symbols to the (actual) semantic operators, i.e., \\([\\![ + ]\\!]\\) gives us the sum operation among two integers.  Sometimes we omit the parenthesis for function application when there is no ambiguity, e.g. \\({\\mathbb E}[\\![ E]\\!]\\sigma\\) is the short hand for \\(({\\mathbb E}[\\![ E]\\!])(\\sigma)\\)</p> <p>As we observe, \\({\\mathbb E}[\\![ \\cdot ]\\!]\\) takes an object from the expression syntactic domain and a memory store object from the domain of \\(\\Sigma\\), returns a value frmo the union of \\(Int\\) and \\(Bool\\) semantic domains.</p>"},{"location":"dynamic_semantics/#denotational-semantics-for-simp-statements","title":"Denotational Semantics for SIMP statements","text":"<p>To define the denotational semantics, we need some extra preparation, in order to support non-terminating programs.</p> <p>Let \\(\\bot\\) be a special element, called undefined, that denotes failure or divergence. Let \\(f\\) and \\(g\\) be functions, we define</p> \\[ \\begin{array}{rcl} f \\circ_\\bot g &amp; = &amp; \\lambda \\sigma. \\left [ \\begin{array}{cc}                   \\bot &amp; g(\\sigma) = \\bot \\\\                   f(g(\\sigma)) &amp; otherwise                  \\end{array} \\right . \\end{array} \\] <p>which is a function composition that propogates \\(\\bot\\) if present. Now we define the semantic function for SIMP statements.</p> \\[ \\begin{array}{lll} {\\mathbb S}[\\![ \\cdot ]\\!] :   \\overline{S}  &amp; \\rightarrow\\ &amp; \\Sigma \\ \\rightarrow \\ \\Sigma \\cup \\{ \\bot \\} \\\\ {\\mathbb S} [\\![  nop ]\\!]&amp; = &amp; \\lambda\\sigma. \\sigma \\\\ {\\mathbb S} [\\![ return\\ X ]\\!]&amp; = &amp; \\lambda\\sigma. \\sigma \\\\ {\\mathbb S} [\\![  X = E ]\\!]&amp; = &amp; \\lambda\\sigma. \\sigma \\oplus (X, {\\mathbb E}[\\![ E ]\\!]\\sigma) \\\\ {\\mathbb S} [\\![ S \\overline{S} ]\\!]&amp; = &amp; {\\mathbb S} [\\![ \\overline{S} ]\\!] \\circ_\\bot {\\mathbb S} [\\![ S ]\\!]\\\\ {\\mathbb S} [\\![ if \\ E\\ \\{\\overline{S_1}\\} \\ else\\ \\{\\overline{S_2} \\} ]\\!]&amp; = &amp; \\lambda\\sigma. \\left [ \\begin{array}{cc}                     {\\mathbb S} [\\![ \\overline{S_1} ]\\!]\\sigma  &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = true \\\\                     {\\mathbb S} [\\![ \\overline{S_2} ]\\!]\\sigma &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ {\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\} ]\\!]&amp; = &amp; fix(F) \\\\  {\\tt where}\\ &amp;  &amp; F= \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc}                     (g \\circ_\\bot {\\mathbb S} [\\![ \\overline{S} ]\\!])(\\sigma)  &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = true \\\\                     \\sigma &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ \\end{array} \\] <p>The signature of the semantic function indicates that we map a SIMP statement into a function that takes a memory environment and returns another memory environment or divergence.</p> <p>In case of \\(nop\\) and return statement, the semantic function returns an identiy function. In case of an assignment, the semantic function takes an memory environment object and update the binding of \\(X\\) to the meaning of \\(E\\). In case of sequence statements, the semantic function returns a \\(\\bot\\)-function composition of the semantic function of the leading statement and the semantic function of the the trailing statements. In case of if-else statement, the semantic function returns the semantics of the then or the else branch statement depending on the meaning of the condition expression. In case of while statement, the semantic function returns a fixed point function. This is due to the fact that the underlying domain theory framework we are using does not support recursion. Hence a fixed point operator \\(fix\\) is used, which is kind of like recursion, (as we learnd in lambda caluclus), and it is more expresive as it gives a fixed term notiation for a sequence of infinitely many function objects applications. To help our understanding, we give a cheating version as if recursive function is supported in the underlying domain theory framework and we are allow to refer to a function application as a name function, we would have</p> \\[ \\begin{array}{lll} {\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\} ]\\!]&amp; = &amp; \\lambda\\sigma. \\left \\{ \\begin{array}{cc}                     ({\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\}]\\!]  \\circ_\\bot {\\mathbb S} [\\![ \\overline{S} ]\\!])(\\sigma)  &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = true \\\\                     \\sigma &amp; {\\mathbb E}[\\![ E ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ \\end{array} \\] <p>which means the function \\(g\\) in the earlier version is a recursive reference to \\({\\mathbb S} [\\![ while \\ E\\ \\{\\overline{S}\\} ]\\!]\\)</p> <p>For example, let \\(\\sigma = \\{ (input, 1)\\}\\)</p> \\[ \\begin{array}{ll} &amp; {\\mathbb S} [\\![ x=input; s = 0; c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma \\\\ = &amp; ({\\mathbb S} [\\![ s = 0; c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ x=input ]\\!]) (\\sigma) \\\\ = &amp; {\\mathbb S} [\\![ s = 0; c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma_1 \\\\ = &amp; ({\\mathbb S} [\\![ c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ s=0 ]\\!]) (\\sigma_1) \\\\ = &amp; {\\mathbb S} [\\![ c=0; while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma_2 \\\\ = &amp; ({\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ c=0 ]\\!]) (\\sigma_2) \\\\ = &amp; {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\}return\\ s; ]\\!] \\sigma_3 \\\\ = &amp; ({\\mathbb S} [\\![ return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\} ]\\!]) (\\sigma_3) \\\\ = &amp; ({\\mathbb S} [\\![ return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\} ]\\!] \\circ_\\bot{\\mathbb S} [\\![ s = c + s; c = c + 1; ]\\!]) (\\sigma_3) \\\\ = &amp; ({\\mathbb S} [\\![ return\\ s; ]\\!] \\circ_\\bot {\\mathbb S} [\\![ while\\ c &lt; x \\{s = c + s; c = c + 1;\\} ]\\!])(\\sigma_4) \\\\ = &amp; {\\mathbb S} [\\![ return\\ s; ]\\!]\\sigma_4 \\\\ = &amp; \\sigma_4 \\end{array} \\] <p>where</p> \\[ \\begin{array}{l} \\sigma_1 = \\sigma \\oplus (x,1) = \\{ (input,1), (x,1) \\} \\\\ \\sigma_2 = \\sigma_1 \\oplus (s,0)  = \\{ (input,1), (x,1), (s,0) \\} \\\\ \\sigma_3 = \\sigma_2 \\oplus (c,0)  = \\{ (input,1), (x,1), (s,0), (c,0) \\}\\\\ \\sigma_4 = \\sigma_3 \\oplus (s,0) \\oplus (c,1)  = \\{ (input,1), (x,1), (s,0), (c,1) \\}\\\\ \\end{array} \\] <p>Let's consider another example of a non-terminating program, we can't use the cheating version here as it would gives the infinite sequence of function compositions. Let \\(\\sigma = \\{(input, true)\\}\\)</p> \\[ \\begin{array}{ll} &amp; {\\mathbb S} [\\![ while\\ input \\{nop;\\}return\\ input; ]\\!] \\sigma \\\\ = &amp; fix(F) \\sigma \\\\ = &amp; \\bot \\end{array} \\] <p>where</p> \\[ \\begin{array}{l} F = \\lambda g.\\lambda\\sigma. \\left [ \\begin{array}{cc}                     (g \\circ_\\bot {\\mathbb S} [\\![ nop ]\\!])(\\sigma)  &amp; {\\mathbb E}[\\![ input ]\\!]\\sigma = true \\\\                     \\sigma &amp; {\\mathbb E}[\\![ input ]\\!]\\sigma = false \\\\                 \\end{array} \\right . \\\\ \\end{array} \\] <p>Since \\({\\mathbb E}[\\![ input ]\\!]\\sigma\\) is always \\(true\\),</p> \\[ F = \\lambda g.\\lambda\\sigma.(g \\circ_\\bot {\\mathbb S} [\\![ nop ]\\!])(\\sigma)   \\] <p>With some math proof, we find that \\(fix(F)\\) is function of type \\(\\Sigma \\rightarrow \\bot\\). We won't be able to discuss the proof until we look into lattice theory in the upcoming classes.</p> <p>In simple term, using the \\(fix\\) operator to define the while statement denotational semantics allows us to \"collapse\" the infinite sequence of function composition/application into a fixed point, which is a non-terminating function.</p>"},{"location":"dynamic_semantics/#denotational-semantics-vs-big-step-operational-semantics-vs-small-step-semantics","title":"Denotational Semantics vs Big Step operational Semantics vs Small Step Semantics","text":"support non-terminating programs don't support non-terminating programs focused on the step by step derivation Small Step Operational Semantics focused on the returned results Denotational Semantics Big Step Operational Semantics <p>Denotational Semantics is often used characterizing programming language model in a compositional way. It allows us to relates syntax objects to semantic objects. For example, if we want to argue that two languages are equivalent, we can map their syntax objects into the same semantic objects. We could also use denotational semantics to reason about concurrency.</p>"},{"location":"dynamic_semantics/#extra-readings-for-denotational-semantics","title":"Extra readings for denotational semantics","text":"<pre><code>https://web.eecs.umich.edu/~weimerw/2008-615/lectures/weimer-615-07.pdf\nhttps://homepage.divms.uiowa.edu/~slonnegr/plf/Book/Chapter9.pdf\n</code></pre>"},{"location":"fp_applicative_monad/","title":"50.054 - Applicative and Monad","text":""},{"location":"fp_applicative_monad/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Describe and define derived type class</li> <li>Describe and define Applicative Functors</li> <li>Describe and define Monads</li> <li>Apply Monad to in design and develop highly modular and resusable software.</li> </ol>"},{"location":"fp_applicative_monad/#derived-type-class","title":"Derived Type Class","text":"<p>Recall that in our previous lesson, we talk about the <code>Ordering</code> type class.</p> <pre><code>trait Ordering[A] {\n    def compare(x:A,y:A):Int // less than: -1, equal: 0, greater than 1\n}\n</code></pre> <p>Let's consider a variant called <code>Order</code> (actually it is defined in a popular Scala library named <code>cats</code>).</p> <pre><code>trait Eq[A] {\n    def eqv(x:A, y:A):Boolean\n}\n\ntrait Order[A] extends Eq[A] { \n    def compare(x:A, y:A):Int\n    def eqv(x:A, y:A):Boolean = compare(x,y) == 0\n    def gt(x:A,  y:A):Boolean = compare(x,y) &gt; 0\n    def lt(x:A,  y:A):Boolean = compare(x,y) &lt; 0\n}\n</code></pre> <p>In the above,  the <code>Eq</code> type class is a supertype of the <code>Order</code> type class, because all instances of <code>Order</code> type class should have the method <code>eqv</code> implemented.</p> <p>We also say <code>Order</code> is a derived type class of <code>Eq</code>.</p> <p>Let's consider some instances</p> <pre><code>given eqInt:Eq[Int] = new Eq[Int] {\n    def eqv(x:Int, y:Int):Boolean = x == y\n}\n\ngiven orderInt:Order[Int] = new Order[Int] {\n    def compare(x:Int, y:Int):Int = x - y\n}\n\neqInt.eqv(1,1)\norderInt.eqv(1,1)\n</code></pre>"},{"location":"fp_applicative_monad/#an-alternative-approach","title":"An alternative approach","text":"<pre><code>trait Order[A] extends Eq[A] { \n    def compare(x:A, y:A):Int\n    // def eqv(x:A, y:A):Boolean = compare(x,y) == 0\n    def gt(x:A,  y:A):Boolean = compare(x,y) &gt; 0\n    def lt(x:A,  y:A):Boolean = compare(x,y) &lt; 0\n}\n\ngiven eqInt:Eq[Int] = new Eq[Int] {\n    def eqv(x:Int, y:Int):Boolean = x == y\n}\n\ngiven orderInt(using eqInt:Eq[Int]):Order[Int] = new Order[Int] {\n    def eqv(x:Int,y:Int):Boolean = eqInt.eqv(x,y)\n    def compare(x:Int, y:Int):Int = x - y\n}\n\neqInt.eqv(1,1)\norderInt.eqv(1,1)\n</code></pre> <p>In the above definition, we skip the default implementatoin of <code>eqv</code> in <code>Order</code> and make use of the type class instance context to synthesis the <code>eqv</code> method based on the existing type class instances of <code>Eq</code>. (This approach is closer to the one found in Haskell.)</p>"},{"location":"fp_applicative_monad/#which-one-is-better","title":"Which one is better?","text":"<p>Both have their own pros and cons. In the first approach, we give a default implementation for the <code>eqv</code> overridden method in <code>Order</code> type class, it frees us from re-defining the <code>eqv</code> in every type class instance of <code>Order</code>. In this case, the rule/logic is fixed at the top level. In the second approach, <code>eqv</code> in <code>Order</code> type class is not defined. We are required to define it for every single type class instance of <code>Order</code>, that means more work. The advantage is that we have flexibility to redefine/re-mix definition of <code>eqv</code> coming from other type class instances.</p>"},{"location":"fp_applicative_monad/#functor-recap","title":"Functor (Recap)","text":"<p>Recall from the last lesson, we make use of the <code>Functor</code> type class to define generic programming style of data processing.</p> <pre><code>trait Functor[T[_]] {\n    def map[A,B](t:T[A])(f:A =&gt; B):T[B]\n}\n\ngiven listFunctor:Functor[List] = new Functor[List] {\n    def map[A,B](t:List[A])(f:A =&gt; B):List[B] = t.map(f)\n}\n\nenum BTree[+A]{\n    case Empty\n    case Node(v:A, lft:BTree[A], rgt:BTree[A])\n}\n\n\ngiven btreeFunctor:Functor[BTree] = new Functor[BTree] {\n    import BTree.*\n    def map[A,B](t:BTree[A])(f:A =&gt; B):BTree[B] = t match {\n        case Empty =&gt; Empty\n        case Node(v, lft, rgt) =&gt; Node(f(v), map(lft)(f), map(rgt)(f))\n    }\n}\n\nval l = List(1,2,3)\nlistFunctor.map(l)((x:Int) =&gt; x + 1)\n\nval t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty))\nbtreeFunctor.map(t)((x:Int) =&gt; x + 1)\n</code></pre> <p>Note that we also swap the first and the second arguments of the <code>map</code> function.</p>"},{"location":"fp_applicative_monad/#applicative-functor","title":"Applicative Functor","text":"<p>The <code>Applicative</code> Functor is a derived type class of <code>Functor</code>, which is defined as follows</p> <pre><code>trait Applicative[F[_]] extends Functor[F] {\n    def ap[A, B](ff: F[A =&gt; B])(fa: F[A]): F[B]\n    def pure[A](a: A): F[A]\n    def map[A, B](fa: F[A])(f: A =&gt; B): F[B] = ap(pure(f))(fa) \n}\n</code></pre> <p>Note that we \"fix\" the <code>map</code> for <code>Applicative</code> in the type class level in this case. (i.e. we are following the first approach.)</p> <pre><code>given listApplicative:Applicative[List] = new Applicative[List] {\n    def pure[A](a:A):List[A] = List(a) \n    def ap[A, B](ff: List[A =&gt; B])(fa: List[A]):List[B] = \n        ff.map( f =&gt; fa.map(f)).flatten\n}\n</code></pre> <p>Recall that <code>flatten</code> flattens a list of lists.</p> <p>Alternatively, we can define the <code>ap</code> method of the <code>Applicative[List]</code> instance <code>flatMap</code>. Given <code>l</code> is a list, <code>l.flatMap(f)</code> is the same as <code>l.map(f).flatten</code>.</p> <pre><code>    def ap[A, B](ff: List[A =&gt; B])(fa: List[A]):List[B] = \n        ff.flatMap( f =&gt; fa.map(f))\n</code></pre> <p>Recall that Scala compiler desugars expression of shape</p> <pre><code>e1.flatMap( v1 =&gt; e2.flatMap( v2 =&gt; ... en.map(vn =&gt; e ... )))\n</code></pre> <p>into</p> <pre><code>for {\n    v1 &lt;- e1\n    v2 &lt;- e2\n    ...\n    vn &lt;- en\n} yield (e)\n</code></pre> <p>Hence we can rewrite the <code>ap</code> method of the <code>Applicative[List]</code> instance as</p> <pre><code>    def ap[A, B](ff: List[A =&gt; B])(fa: List[A]):List[B] = \n        for {\n            f &lt;- ff\n            a &lt;- fa \n        } yield (f(a))\n</code></pre> <p>It is not suprising the following produces the same results as the functor instance.</p> <pre><code>listApplicative.map(l)((x:Int) =&gt; x + 1)\n</code></pre> <p>What about <code>pure</code> and <code>ap</code>? when can we use them?</p> <p>Let's consider the following contrived example. Suppose we would like to apply two sets of operations to elements from <code>l</code>, each operation will produce its own set of results, and the inputs do not depend on the output of the other set. i.e. If the two operations, are <code>(x:Int)=&gt; x+1</code> and <code>(y:Int)=&gt;y*2</code>.</p> <pre><code>val intOps= List((x:Int)=&gt;x+1, (y:Int)=&gt;y*2)\nlistApplicative.ap(intOps)(l)\n</code></pre> <p>we get</p> <pre><code>List(2, 3, 4, 2, 4, 6)\n</code></pre> <p>as the result.</p> <p>Let's consider another example. Recall that <code>Option[A]</code> algebraic datatype which captures a value of type <code>A</code> could be potentially empty.</p> <p>We define the <code>Applicative[Option]</code> instance as follows</p> <pre><code>given optApplicative:Applicative[Option] = new Applicative[Option] {\n    def pure[A](v:A):Option[A] = Some(v)\n    def ap[A,B](ff:Option[A=&gt;B])(fa:Option[A]):Option[B]  = ff match {\n        case None =&gt; None\n        case Some(f) =&gt; fa match {\n            case None =&gt; None\n            case Some(a) =&gt; Some(f(a))\n        }\n    }\n}\n</code></pre> <p>In the above Applicative instance, the <code>ap</code> method takes a optional operation and optional value as inputs, tries to apply the operation to the value when both of them are present, otherwise, signal an error by returning <code>None</code>. This allows us to focus on the high-level function-value-input-output relation and abstract away the details of handling potential absence of function or value.</p> <p>Recall the builtin <code>Option</code> type is defined as follows,</p> <pre><code>// no need to run this.\nenum Option[+A] {\n    case None\n    case Some(v)\n    def map[B](f:A=&gt;B):Option[B] = this match {\n        case None =&gt; None \n        case Some(v) =&gt; Some(f(v))\n    }\n    def flatMap[B](f:A=&gt;Option[B]):Option[B] = this match {\n        case None =&gt; None \n        case Some(v) =&gt; f(v) match {\n            case None =&gt; None \n            case Some(u) =&gt; Some(u) \n        }\n    }\n}\n</code></pre> <p>Hence <code>optApplicative</code> can be simplified as </p> <pre><code>given optApplicative:Applicative[Option] = new Applicative[Option] {\n    def pure[A](v:A):Option[A] = Some(v)\n    def ap[A,B](ff:Option[A=&gt;B])(fa:Option[A]):Option[B] = \n        ff.flatMap(f =&gt; fa.map(f)) // same as listApplicative\n}\n</code></pre> <p>or  <pre><code>given optApplicative:Applicative[Option] = new Applicative[Option] {\n    def pure[A](v:A):Option[A] = Some(v)\n    def ap[A,B](ff:Option[A=&gt;B])(fa:Option[A]):Option[B] = for \n    {\n        f &lt;- ff\n        a &lt;- fa\n    } yield f(a) // same as listApplicative\n}\n</code></pre></p>"},{"location":"fp_applicative_monad/#applicative-laws","title":"Applicative Laws","text":"<p>Like Functor laws, every Applicative instance must follow the Applicative laws to remain computationally predictable.</p> <ol> <li>Identity: <code>ap(pure(x=&gt;x))</code> \\(\\equiv\\) <code>x=&gt;x</code></li> <li>Homomorphism: <code>ap(pure(f))(pure(x))</code> \\(\\equiv\\) <code>pure(f(x))</code></li> <li>Interchange: <code>ap(u)(pure(y))</code> \\(\\equiv\\) <code>ap(pure(f=&gt;f(y)))(u)</code></li> <li> <p>Composition: <code>ap(ap(ap(pure(f=&gt;f.compose))(u))(v))(w)</code> \\(\\equiv\\) <code>ap(u)(ap(v)(w))</code></p> </li> <li> <p>Identity law states that applying a lifted identity function of type <code>A=&gt;A</code> is same as an identity function of type <code>F[A] =&gt; F[A]</code> where <code>F</code> is an applicative functor.</p> </li> <li>Homomorphism says that applying a lifted function (which has type <code>A=&gt;A</code> before being lifted) to a lifted value, is equivalent to applying the unlifted function to the unlifted value directly and then lift the result.</li> <li> <p>To understand Interchange law let's consider the following equation $$ u y \\equiv (\\lambda f.(f y)) u $$</p> <ul> <li>Interchange law says that the above equation remains valid when \\(u\\) is already lifted, as long as we also lift \\(y\\). </li> </ul> </li> <li> <p>To understand the Composition law, we consider the following equation in lambda calculus</p> </li> </ol> \\[ (((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)\\ v)\\ w \\equiv u\\ (v\\ w) \\] \\[ \\begin{array}{rl} (\\underline{((\\lambda f.(\\lambda g.(f \\circ g)))\\ u)}\\ v)\\ w &amp; \\longrightarrow_{\\beta} \\\\  (\\underline{(\\lambda g.(u \\circ g))\\ v})\\ w &amp; \\longrightarrow_{\\beta} \\\\  (u\\circ v)\\ w &amp; \\longrightarrow_{\\tt composition} \\\\  u\\ (v\\ w) \\end{array} \\] <p>The Composition Law says that the above equation remains valid when \\(u\\), \\(v\\) and \\(w\\) are lifted, as long as we also lift \\(\\lambda f.(\\lambda g.(f \\circ g))\\).</p>"},{"location":"fp_applicative_monad/#monad","title":"Monad","text":"<p>Monad is one of the essential coding/design pattern for many functional programming languages. It enables us to develop high-level resusable code and decouple code dependencies and generate codes by (semi-) automatic code-synthesis. FYI, Monad is a derived type class of Applicative thus Functor.</p> <p>Let's consider a motivating example.  Recall that in the earlier lesson, we came across the following example.</p> <pre><code>enum MathExp {\n    case Plus(e1:MathExp, e2:MathExp)\n    case Minus(e1:MathExp, e2:MathExp)\n    case Mult(e1:MathExp, e2:MathExp)\n    case Div(e1:MathExp, e2:MathExp)\n    case Const(v:Int)\n}\n\ndef eval(e:MathExp):Option[Int] = e match {\n    case MathExp.Plus(e1, e2)  =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(v2) =&gt; Some(v1 + v2)            \n        }\n    }\n    case MathExp.Minus(e1, e2) =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(v2) =&gt; Some(v1 - v2)            \n        }\n    }\n    case MathExp.Mult(e1, e2)  =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(v2) =&gt; Some(v1 * v2)            \n        }\n    }\n    case MathExp.Div(e1, e2)   =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(0)  =&gt; None\n            case Some(v2) =&gt; Some(v1 / v2)            \n        }\n    }\n    case MathExp.Const(i)      =&gt; Some(i)\n}\n</code></pre> <p>In which we use <code>Option[A]</code> to capture the potential div-by-zero error. One issue with the above is that it is very verbose, we lose some readability of the code thus, it takes us a while to migrate to <code>Either[A,B]</code> if we want to have better error messages. Monad is a good application here.</p> <p>Let's consider the type class definition of <code>Monad[F[_]]</code>.</p> <pre><code>trait Monad[F[_]] extends Applicative[F] {\n    def bind[A,B](fa:F[A])(f:A =&gt; F[B]):F[B]\n    def pure[A](v:A):F[A]\n    def ap[A, B](ff: F[A =&gt; B])(fa: F[A]): F[B] = \n        bind(ff)((f:A=&gt;B) =&gt; bind(fa)((a:A)=&gt; pure(f(a))))\n}\n\ngiven optMonad:Monad[Option] = new Monad[Option] {\n    def bind[A,B](fa:Option[A])(f:A=&gt;Option[B]):Option[B] = fa match {\n        case None =&gt; None\n        case Some(a) =&gt; f(a)\n    }\n    def pure[A](v:A):Option[A] = Some(v)\n}\n</code></pre> <p>The <code>eval</code> function can be re-expressed using <code>Monad[Option]</code>.</p> <pre><code>def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match {\n    case MathExp.Plus(e1, e2)  =&gt; \n        m.bind(eval(e1))( v1 =&gt; {\n            m.bind(eval(e2))( {v2 =&gt; m.pure(v1+v2)})\n        })        \n    case MathExp.Minus(e1, e2) =&gt;         \n        m.bind(eval(e1))( v1 =&gt; {\n            m.bind(eval(e2))( {v2 =&gt; m.pure(v1-v2)})\n        }) \n    case MathExp.Mult(e1, e2)  =&gt;\n        m.bind(eval(e1))( v1 =&gt; {\n            m.bind(eval(e2))( {v2 =&gt; m.pure(v1*v2)})\n        }) \n    case MathExp.Div(e1, e2)   =&gt; \n        m.bind(eval(e1))( v1 =&gt; {\n            m.bind(eval(e2))( {v2 =&gt; if (v2 == 0) {None} else {m.pure(v1/v2)}})\n        }) \n    case MathExp.Const(i)      =&gt; m.pure(i)\n}\n</code></pre> <p>It certainly reduces the level of verbosity, but the readability is worsened. Thankfully, we can make use of for comprehension since <code>Option</code> has the member functions <code>flatMap</code> and <code>map</code> defined.</p> <p>Recall that Scala desugars <code>for {...} yield</code> expression into <code>flatMap</code> and <code>map</code>.</p> <p>Thus the above can be rewritten as</p> <pre><code>def eval(e:MathExp)(using m:Monad[Option]):Option[Int] = e match {\n    case MathExp.Plus(e1, e2)  =&gt; for {\n        v1 &lt;- eval(e1)\n        v2 &lt;- eval(e2)\n    } yield (v1+v2) \n    case MathExp.Minus(e1, e2) =&gt; for {\n        v1 &lt;- eval(e1)\n        v2 &lt;- eval(e2)\n    } yield (v1-v2) \n    case MathExp.Mult(e1, e2)  =&gt; for {\n        v1 &lt;- eval(e1)\n        v2 &lt;- eval(e2)\n    } yield (v1*v2) \n    case MathExp.Div(e1, e2)   =&gt; for {\n        v1 &lt;- eval(e1)\n        v2 &lt;- eval(e2)\n        if (v2 !=0)\n    } yield (v1/v2) \n    case MathExp.Const(i)      =&gt; m.pure(i)\n}\n</code></pre> <p>Now the readability is restored.</p> <p>Another advantage of coding with <code>Monad</code> is that its abstraction allows us to switch underlying data structure without major code change.</p> <p>Suppose we would like to use <code>Either[String, A]</code> or some other equivalent as return type of <code>eval</code> function to support better error message. But before that, let's consider some subclasses of the <code>Applicative</code> and the <code>Monad</code> type classes.</p> <pre><code>trait ApplicativeError[F[_], E] extends Applicative[F] {\n    def raiseError[A](e:E):F[A]\n}\n\ntrait MonadError[F[_], E] extends Monad[F] with ApplicativeError[F, E] {\n    override def raiseError[A](e:E):F[A]\n}   \n\ntype ErrMsg = String\n</code></pre> <p>In the above, we define an extension to the <code>Applicative</code> type class, named <code>ApplicativeError</code> which expects an extra type class parameter <code>E</code> that denotes an error. The <code>raiseError</code> method takes a value of type <code>E</code> and returns the Applicative result.</p> <p>Similarly, we extend <code>Monad</code> type class with <code>MonadError</code> type class. Next we include the following type class instance to include <code>Option</code> as one f the <code>MonadError</code> functor.</p> <pre><code>given optMonadError:MonadError[Option, ErrMsg] = new MonadError[Option, ErrMsg] {\n    def raiseError[A](e:ErrMsg):Option[A] = None\n    def bind[A,B](fa:Option[A])(f:A=&gt;Option[B]):Option[B] = fa match {\n        case None =&gt; None\n        case Some(a) =&gt; f(a)\n    }\n    def pure[A](v:A):Option[A] = Some(v)\n}\n</code></pre> <p>Next, we adjust the <code>eval</code> function to takes in a <code>MonadError</code> context instead of a <code>Monad</code> context. In addition, we make the error signal more explicit by calling the <code>raiseError()</code> method from the <code>MonadError</code> type class context.</p> <pre><code>def eval2(e:MathExp)(using m:MonadError[Option, ErrMsg]):Option[Int] = e match {\n    case MathExp.Plus(e1, e2)  =&gt; for {\n        v1 &lt;- eval2(e1)\n        v2 &lt;- eval2(e2)\n    } yield (v1+v2) \n    case MathExp.Minus(e1, e2) =&gt; for {\n        v1 &lt;- eval2(e1)\n        v2 &lt;- eval2(e2)\n    } yield (v1-v2) \n    case MathExp.Mult(e1, e2)  =&gt; for {\n        v1 &lt;- eval2(e1)\n        v2 &lt;- eval2(e2)\n    } yield (v1*v2) \n    case MathExp.Div(e1, e2)   =&gt; for {\n        v1 &lt;- eval2(e1)\n        v2 &lt;- eval2(e2)\n        _  &lt;- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())}\n    } yield (v1/v2) \n    case MathExp.Const(i)      =&gt; m.pure(i)\n}\n</code></pre> <p>Now let's try to refactor the code to make use of <code>Either[ErrMsg, A]</code> as the functor instead of <code>Option[A]</code>.</p> <pre><code>enum Either[+A, +B] {\n    case Left(v: A)\n    case Right(v: B)\n    // to support for comprehension\n    def flatMap[C&gt;:A,D](f: B =&gt; Either[C,D]):Either[C,D] = this match {\n        case Left(a) =&gt; Left(a)\n        case Right(b) =&gt; f(b)\n    }\n    def map[D](f:B =&gt; D):Either[A,D] = this match {\n        case Right(b) =&gt; Right(f(b))\n        case Left(e) =&gt; Left(e)\n    }\n} \n</code></pre> <p>In the above, we have to define <code>flatMap</code> and <code>map</code> member functions for <code>Either</code> type so that we could make use of the for comprehension later on. One might argue with the type signature of <code>flatMap</code> should be <code>flatMap[D](f: B =&gt; Either[A,D]):Either[A,D]</code>. The issue here is that the type variable <code>A</code> will appear in both co- and contra-variant positions.  The top-level annotation <code>+A</code> is no longer true. Hence we \"relax\" the type constraint here by introducing a new type variable <code>C</code> which has a lower bound of <code>A</code> (even though we do not need to upcast the result of the Left alternative.)</p> <pre><code>type EitherErr = [B] =&gt;&gt; Either[ErrMsg,B]\n</code></pre> <p>In the above we define <code>Either</code> algebraic datatype and the type construcor <code>EitherErr</code>. <code>[B] =&gt;&gt; Either[ErrMsg, B]</code> denotes a type lambda, which means that <code>EitherErr</code> is a type constructor (or type function) that takes a type <code>B</code> and return an <code>Either[ErrMsg, B]</code> type.</p> <p>Next, we define the type class instance for <code>MonadError[EitherErr, ErrMsg]</code></p> <pre><code>given eitherErrMonad: MonadError[EitherErr, ErrMsg] =\n    new MonadError[EitherErr, ErrMsg] {\n        import Either.*\n        def raiseError[B](e: ErrMsg): EitherErr[B] = Left(e)\n        def bind[A, B](\n            fa: EitherErr[A]\n        )(f: A =&gt; EitherErr[B]): EitherErr[B] = fa match {\n            case Right(b) =&gt; f(b)\n            case Left(s)  =&gt; Left(s)\n        }\n        def pure[B](v: B): EitherErr[B] = Right(v)\n    }\n</code></pre> <p>And finally, we refactor the <code>eval</code> function by changing its type signature. And its body remains unchanged.</p> <pre><code>def eval3(e:MathExp)(using m:MonadError[EitherErr, ErrMsg]):EitherErr[Int] = e match {\n    case MathExp.Plus(e1, e2)  =&gt; for {\n        v1 &lt;- eval3(e1)\n        v2 &lt;- eval3(e2)\n    } yield (v1+v2) \n    case MathExp.Minus(e1, e2) =&gt; for {\n        v1 &lt;- eval3(e1)\n        v2 &lt;- eval3(e2)\n    } yield (v1-v2) \n    case MathExp.Mult(e1, e2)  =&gt; for {\n        v1 &lt;- eval3(e1)\n        v2 &lt;- eval3(e2)\n    } yield (v1*v2) \n    case MathExp.Div(e1, e2)   =&gt; for {\n        v1 &lt;- eval3(e1)\n        v2 &lt;- eval3(e2)\n        _  &lt;- if (v2 ==0) {m.raiseError(\"div by zero encountered.\")} else { m.pure(())}\n    } yield (v1/v2) \n    case MathExp.Const(i)      =&gt; m.pure(i)\n}\n</code></pre>"},{"location":"fp_applicative_monad/#commonly-used-monads","title":"Commonly used Monads","text":"<p>We have seen the option Monad and the either Monad. Let's consider a few commonly used Monads.</p>"},{"location":"fp_applicative_monad/#list-monad","title":"List Monad","text":"<p>We know that <code>List</code> is a Functor and an Applicative. It is not surprising that <code>List</code> is also a Monad.</p> <pre><code>given listMonad:Monad[List] = new Monad[List] {\n    def pure[A](v:A):List[A] = List(v)\n    def bind[A,B](fa:List[A])(f:A =&gt; List[B]):List[B] = \n        fa.flatMap(f)\n}\n</code></pre> <p>With the above instance, we can write list processing method in for comprehension which is similar to query languages.</p> <pre><code>import java.util.Date\nimport java.util.Calendar\nimport java.util.GregorianCalendar\nimport java.text.SimpleDateFormat\ncase class Staff(id:Int, dob:Date)\n\ndef mkStaff(id:Int, dobStr:String):Staff = {\n    val sdf = new SimpleDateFormat(\"yyyy-MM-dd\")\n    val dobDate = sdf.parse(dobStr)\n    Staff(id, dobDate)\n}\nval staffData = List(\n    mkStaff(1, \"1976-01-02\"),\n    mkStaff(2, \"1986-07-24\")\n)\n\ndef ageBelow(staff:Staff, age:Int): Boolean = staff match {\n    case Staff(id, dob) =&gt; {\n        val today = new Date()\n        val calendar = new GregorianCalendar();\n        calendar.setTime(today)\n        calendar.add(Calendar.YEAR, -age)\n        val ageYearsAgo = calendar.getTime()\n        dob.after(ageYearsAgo)\n    }\n}\n\ndef query(data:List[Staff]):List[Staff] = for {\n    staff &lt;- data          // from data \n    if ageBelow(staff, 40) // where staff.age &lt; 40\n} yield staff              // select *\n</code></pre>"},{"location":"fp_applicative_monad/#reader-monad","title":"Reader Monad","text":"<p>Next we consider the <code>Reader</code> Monad.  <code>Reader</code> Monad denotes a shared input environment used by multiple computations. Once shared, this environment stays immutable.</p> <p>For example, suppose we would like to implement some test with a sequence of API calls. Most of these API calls are having the same host IP. We can set the host IP as part of the reader's environment.</p> <pre><code>case class Reader[R, A] (run: R=&gt;A) { \n    // we need flatMap and map for for-comprehension\n    def flatMap[B](f:A =&gt;Reader[R,B]):Reader[R,B] = this match {\n        case Reader(ra) =&gt; Reader (\n            r =&gt; f(ra(r)) match {\n                case Reader(rb) =&gt; rb(r)\n            }\n        )\n    }\n    def map[B](f:A=&gt;B):Reader[R, B] = this match {\n        case Reader(ra) =&gt; Reader (\n            r =&gt; f(ra(r))\n        )\n    }\n}\n\ntype ReaderM = [R] =&gt;&gt; [A] =&gt;&gt; Reader[R, A]\n\ntrait ReaderMonad[R] extends Monad[ReaderM[R]] {\n    override def pure[A](v:A):Reader[R, A] = Reader (r =&gt; v)\n    override def bind[A,B](fa:Reader[R, A])(f:A=&gt;Reader[R,B]):Reader[R,B] = fa match {\n        case Reader(ra) =&gt; Reader (\n            r=&gt; f(ra(r)) match {\n                case Reader(rb) =&gt; rb(r)\n            }\n        ) \n    }\n    def ask:Reader[R,R] = Reader( r =&gt; r)\n    def local[A](f:R=&gt;R)(r:Reader[R,A]):Reader[R,A] = r match {\n        case Reader(ra) =&gt; Reader( r =&gt; {\n            val localR = f(r)\n            ra(localR)\n        })\n    }    \n}\n</code></pre> <p>In the above <code>Reader[R,A]</code> case class defines the structure of the Reader type, where <code>R</code> denotes the shared information for the computation, (source for reader), <code>A</code> denotes the output of the computation. We would like to define <code>Reader[R,_]</code> as a Monad instance. To do so, we define a type-curry version of <code>Reader</code>, i.e. <code>ReaderM</code>.</p> <p>One crucial observation is that <code>bind</code> method in <code>ReaderMonad</code> is nearly identical to <code>flatMap</code> in <code>Reader</code>, with the arguments swapped.</p> <p>In fact, we can re-express <code>bind</code> for all Monads as the <code>flatMap</code> in their underlying case class.</p> <pre><code>override def bind[A,B](fa:Reader[R, A])(f:A=&gt;Reader[R,B]):Reader[R,B] = fa.flatMap(f)\n</code></pre> <p>The following example shows how Reader Monad can be used in making several API calls (computation) to the same API server (shared input <code>https://127.0.0.1/</code>). For authentication we need to call the authentication server <code>https://127.0.0.10/</code> temporarily. </p> <pre><code>case class API(url:String)\n\ngiven APIReader:ReaderMonad[API] = new ReaderMonad[API] {}\n\ndef get(path:String)(using pr:ReaderMonad[API]):Reader[API,Unit] = for {\n    r &lt;- pr.ask\n    s &lt;- r match {\n        case API(url) =&gt; pr.pure(println(s\"${url}${path}\"))\n    }\n} yield s\n\ndef authServer(api:API):API = API(\"https://127.0.0.10/\")\n\ndef test1(using pr:ReaderMonad[API]):Reader[API, Unit] = for {\n    a &lt;- pr.local(authServer)(get(\"auth\"))\n    t &lt;- get(\"time\")\n    j &lt;- get(\"job\")\n} yield (())\n\n\ndef runtest1():Unit = test1 match {\n    case Reader(run) =&gt; run(API(\"https://127.0.0.1/\"))\n}\n</code></pre>"},{"location":"fp_applicative_monad/#state-monad","title":"State Monad","text":"<p>We consider the <code>State</code> Monad. A <code>State</code> Monad allows programmers capture and manipulate stateful computation without using assignment and mutable variable. One advantage of doing so is that program has full control of the state without having direct access to the computer memory. In a typeful language like Scala, the type system segregates the pure computation from the stateful computation. This greatly simplify software verification and debugging.</p> <p>The following we define a <code>State</code> case class, which has a member computation <code>run:S =&gt; (S,A)</code>.</p> <pre><code>case class State[S,A]( run:S=&gt;(S,A)) { \n    def flatMap[B](f: A =&gt; State[S,B]):State[S,B] = this match {\n        case State(ssa) =&gt; State(\n            s=&gt; ssa(s) match {\n                case (s1,a) =&gt; f(a) match {\n                    case State(ssb) =&gt; ssb(s1)\n                }\n            }\n        )\n    }\n    def map[B](f:A =&gt; B):State[S,B] = this match {\n        case State(ssa) =&gt; State(\n            s=&gt; ssa(s) match {\n                case (s1, a) =&gt; (s1, f(a))\n            }\n        )\n    }\n}\n</code></pre> <p>As suggested by the type, the computationn <code>S=&gt;(S,A)</code>, takes in a state <code>S</code> as input and return a tuple of output, consists a new state and the result of the computation.</p> <p>The State Monad type class is defined as a derived type class of <code>Monad[StateM[S]]</code>.</p> <pre><code>type StateM = [S] =&gt;&gt; [A] =&gt;&gt; State[S,A]\n\ntrait StateMonad[S] extends Monad[StateM[S]] {\n    override def pure[A](v:A):State[S,A] = State( s=&gt; (s,v))\n    override def bind[A,B](\n        fa:State[S,A]\n        )(\n            ff:A =&gt; State[S,B]\n        ):State[S,B] = fa.flatMap(ff)\n    def get:State[S, S] = State(s =&gt; (s,s))\n    def set(v:S):State[S,Unit] = State(s =&gt; (v,()))\n}\n</code></pre> <p>In the <code>pure</code> method's default implementation, we takes a value <code>v</code> of type <code>A</code> and return a <code>State</code> case class oject by wrapping a lambda which takes a state <code>s</code> and returns back the same state <code>s</code> with the input value <code>v</code>. In the default implementation of the <code>bind</code> method, we take a computation <code>fa</code> of type <code>State[S,A]</code>, i.e. a stateful computation over state type <code>S</code> and return a result of type <code>A</code>. In addition, we take a function that expects input of type <code>A</code> and returns a stateful computation <code>State[S,B]</code>. We apply <code>flatMap</code> of <code>fa</code> to <code>ff</code>., which can be expanded to</p> <pre><code>fa.flatMap(ff) --&gt;\nfa match {\n    case State(ssa) =&gt; State ( s =&gt; {\n        ssa(s) match {\n            case (s1,a) =&gt; ff(a) match {\n                case State(ssb) =&gt; ssb(s1) \n            }\n        }\n    })\n}\n</code></pre> <p>In essence it \"opens\" the computation in <code>fa</code> to extract the run function <code>ssa</code> which takes a state returns result <code>A</code> with the output state. As the output, we construct stateful computation in which a state <code>s</code> is taken as input, we immediately apply <code>s</code> with <code>ssa</code> (i.e. the computation extracted from <code>fa</code>) to compute the intermediate state <code>s1</code> and the output <code>a</code> (of type <code>A</code>).  Next we apply <code>ff</code> to <code>a</code> which returns a Stateful computation <code>State[S,B]</code>. We extract the run function from this stateful copmutation, namley <code>ssb</code> and apply it to <code>s1</code> to continue with the result of the computation. In otherwords, <code>bind</code> function chains up a stateful computation  <code>fa</code> with a lambda expressoin that consumes the result from <code>fa</code> and continue with another stateful copmutation.</p> <p>The <code>get</code> and the <code>set</code> methods give us access to the state environment of type <code>S</code>.</p> <p>For instance,</p> <pre><code>case class Counter(c:Int)\n\ngiven counterStateMonad:StateMonad[Counter] = new StateMonad[Counter]  {\n}\n\ndef incr(using csm:StateMonad[Counter]):State[Counter,Unit] = for {\n    Counter(c) &lt;- csm.get\n    _ &lt;- csm.set(Counter(c+1))\n} yield ()\n\ndef app(using csm:StateMonad[Counter]):State[Counter, Int] = for {\n    _ &lt;- incr\n    _ &lt;- incr\n    Counter(v) &lt;- csm.get\n} yield v\n</code></pre> <p>In the above we define the state environment as an integer counter. Monadic function <code>incr</code> increase the counter in the state.</p>"},{"location":"fp_applicative_monad/#monad-laws","title":"Monad Laws","text":"<p>Similar to Functor and Applicative, all instances of Monad must satisfy the following three Monad Laws.</p> <ol> <li>Left Identity: <code>bind(pure(a))(f)</code> \\(\\equiv\\) <code>f(a)</code></li> <li>Right Identity: <code>bind(m)(pure)</code> \\(\\equiv\\) <code>m</code></li> <li> <p>Associativity: <code>bind(bind(m)(f))(g)</code> \\(\\equiv\\) <code>bind(m)(x =&gt; bind(f(x))(g))</code></p> </li> <li> <p>Intutively speaking, a <code>bind</code> operation is to extract results of type <code>A</code> from its first argument with type <code>F[A]</code> and apply <code>f</code> to the extracted results.</p> </li> <li>Left identity law enforces that binding a lifted value to <code>f</code>, is the same as applying <code>f</code> to the unlifted value directly, because the lifting and the extraction of the bind cancel each other.</li> <li>Right identity law enforces that binding a lifted value to <code>pure</code>,  is the same as the lifted value, because extracting results from <code>m</code> and <code>pure</code> cancel each other.</li> <li>The Associativity law enforces that binding a lifted value <code>m</code> to <code>f</code> then to <code>g</code> is the same as binding <code>m</code> to a monadic bind composition <code>(x =&gt; bind(f(x)(g)))</code></li> </ol>"},{"location":"fp_applicative_monad/#summary","title":"Summary","text":"<p>In this lesson we have discussed the following</p> <ol> <li>A derived type class is a type class that extends from another one.</li> <li>An Applicative Functor is a sub-class of Functor, with the methods <code>pure</code> and <code>ap</code>.</li> <li>The four laws for Applicative Functor.</li> <li>A Monad Functor is a sub-class of Applicative Functor, with the method <code>bind</code>.</li> <li>The three laws of Monad Functor.</li> <li>A few commonly used Monad such as, List Monad, Option Monad, Reader Monad and State Monad.</li> </ol>"},{"location":"fp_applicative_monad/#extra-materials","title":"Extra Materials","text":""},{"location":"fp_applicative_monad/#writer-monad","title":"Writer Monad","text":"<p>The dual of the <code>Reader</code> Monad is the <code>Writer</code> Monad, which has the following definition.</p> <pre><code>// inspired by https://kseo.github.io/posts/2017-01-21-writer-monad.html\ntrait Monoid[A]{ // We omitted the super class SemiRing[A]\n    def mempty:A\n    def mappend:A =&gt; A =&gt; A\n}\n\ngiven listMonoid[A]:Monoid[List[A]] = new Monoid[List[A]] {\n    def mempty:List[A] = Nil\n    def mappend:List[A]=&gt;List[A]=&gt;List[A] = \n        (l1:List[A])=&gt;(l2:List[A]) =&gt; l1 ++ l2 \n}\n\ncase class Writer[W,A]( run: (W,A))(using mw:Monoid[W]) {\n    def flatMap[B](f:A =&gt; Writer[W,B]):Writer[W,B] = this match {\n        case Writer((w,a)) =&gt; f(a) match {\n            case Writer((w2,b)) =&gt; Writer((mw.mappend(w)(w2), b))\n        } \n    }\n    def map[B](f:A=&gt;B):Writer[W, B] = this match {\n        case Writer((w,a)) =&gt; Writer((w, f(a)))\n    }\n}\n</code></pre> <p>Similar to the <code>Reader</code> Monad, in the above we define a case class <code>Writer</code>, which has a member value <code>run</code> that returns a tuple of <code>(W,A)</code>.  The subtle difference is that the writer memory <code>W</code> has to be an instance of the <code>Monoid</code> type class, in which <code>mempty</code> and <code>mappend</code> operations are defined.</p> <pre><code>type WriterM = [W] =&gt;&gt; [A] =&gt;&gt; Writer[W,A] \n\ntrait WriterMonad[W] extends Monad[WriterM[W]] {\n    implicit def W0:Monoid[W]\n    override def pure[A](v: A): Writer[W, A] = Writer((W0.mempty, v))\n    override def bind[A, B](\n        fa: Writer[W, A]\n    )(f: A =&gt; Writer[W, B]): Writer[W, B] = fa match {\n        case Writer((w, a)) =&gt;\n            f(a) match {\n                case Writer((w2, b)) =&gt; {\n                    Writer((W0.mappend(w)(w2), b))\n                }\n            }\n    }\n    def tell(w: W): Writer[W, Unit] = Writer((w, ()))\n    def pass[A](ma: Writer[W, (A, W =&gt; W)]): Writer[W, A] = ma match {\n        case Writer((w, (a, f))) =&gt; Writer((f(w), a))\n    }\n}\n</code></pre> <p>In the above we define <code>WriterMonad</code> to be a derived type class of <code>Monad[WriterM[W]]</code>. For a similar reason, we need to include the type class <code>Monoid[W]</code> to ensure that <code>mempty</code> and <code>mappend</code> are defined on <code>W</code>. Besides the <code>pure</code> and <code>bind</code> members, we introduce <code>tell</code> and <code>pass</code>. <code>tell</code> writes the given argument into the writer's memory. <code>pass</code> execute a given computation which returns a value of type <code>A</code> and a memory update function <code>W=&gt;W</code>, and return a <code>Writer</code> whose memory is updated by applied the update function to the memory.</p> <p>In the following we define a simple application with logging mechanism using the <code>Writer</code> Monad.</p> <pre><code>case class LogEntry(msg:String)\n\ngiven logWriterMonad:WriterMonad[List[LogEntry]] = new WriterMonad[List[LogEntry]] {\n    override def W0:Monoid[List[LogEntry]] = new Monoid[List[LogEntry]] {\n        override def mempty = Nil\n        override def mappend = (x:List[LogEntry]) =&gt; (y:List[LogEntry]) =&gt; x ++ y\n    }\n}\n\ndef logger(m: String)(using\n    wm: WriterMonad[List[LogEntry]]\n): Writer[List[LogEntry], Unit] = wm.tell(List(LogEntry(m)))\n\ndef app(using\n    wm: WriterMonad[List[LogEntry]]\n): Writer[List[LogEntry], Int] = for {\n    _ &lt;- logger(\"start\")\n    x &lt;- wm.pure(1 + 1)\n    _ &lt;- logger(s\"result is ${x}\")\n    _ &lt;- logger(\"done\")\n} yield x\n\ndef runApp(): Int = app match {\n    case Writer((w, i)) =&gt; {\n        println(w)\n        i\n    }\n}\n</code></pre>"},{"location":"fp_applicative_monad/#monad-transformer","title":"Monad Transformer","text":"<p>Is the following class a Monad?</p> <pre><code>case class MyState[S,A]( run:S=&gt;Option[(S,A)]) \n</code></pre> <p>The difference between this class and the <code>State</code> class we've seen earlier is that the execution method <code>run</code> yields result of type <code>Option[(S,A)]</code> instead of <code>(S,A)</code> which means that it can potentially fail.</p> <p>It is ascertained that <code>MyState</code> is also a Monad, and it is a kind of special State Monad.</p> <pre><code>case class MyState[S, A](run: S =&gt; Option[(S, A)]) {\n    def flatMap[B](f: A =&gt; MyState[S, B]): MyState[S, B] = this match {\n        case MyState(ssa) =&gt;\n            MyState(s =&gt;\n                ssa(s) match {\n                    case None =&gt; None\n                    case Some((s1, a)) =&gt;\n                        f(a) match {\n                            case MyState(ssb) =&gt; ssb(s1)\n                        }\n                }\n            )\n    }\n    def map[B](f: A =&gt; B): MyState[S, B] = this match {\n        case MyState(ssa) =&gt;\n            MyState(s =&gt;\n                ssa(s) match {\n                    case None          =&gt; None\n                    case Some((s1, a)) =&gt; Some((s1, f(a)))\n                }\n            )\n    }\n}\n\ntype MyStateM = [S] =&gt;&gt; [A] =&gt;&gt; MyState[S,A]\n\ntrait MyStateMonad[S] extends Monad[MyStateM[S]] {\n    override def pure[A](v:A):MyState[S,A] = MyState( s=&gt; Some((s,v)))\n    override def bind[A,B](\n        fa:MyState[S,A]\n        )(\n            ff:A =&gt; MyState[S,B]\n        ):MyState[S,B] = fa.flatMap(ff)\n    def get:MyState[S, S] = MyState(s =&gt; Some((s,s)))\n    def set(v:S):MyState[S,Unit] = MyState(s =&gt; Some((v,())))\n}\n</code></pre> <p>Besides \"stuffing-in\" an <code>Option</code> type, one could use an <code>Either</code> type and etc. Is there a way to generalize this by parameterizing? Seeking the answer to this question leads us to Monad Transformer.</p> <p>We begin by parameterizing the <code>Option</code> functor in <code>MyState</code></p> <pre><code>case class StateT[S, M[_], A](run: S =&gt; M[(S, A)])(using m:Monad[M]) {\n    def flatMap[B](f: A =&gt; StateT[S, M, B]): StateT[S, M, B] = this match {\n        case StateT(ssa) =&gt;\n            StateT(s =&gt; m.bind(ssa(s))\n                (sa =&gt; sa match {\n                    case (s1,a) =&gt; f(a) match {\n                        case StateT(ssb) =&gt; ssb(s1)\n                        }\n                    }\n                )\n            ) \n        }\n\n    def map[B](f: A =&gt; B): StateT[S, M, B] = this match {\n        case StateT(ssa) =&gt;\n            StateT(s =&gt; m.bind(ssa(s))\n                (sa =&gt; sa match {\n                    case (s1, a) =&gt; m.pure((s1, f(a)))\n                })\n            )\n    }\n}\n</code></pre> <p>In the above it is largely similar to <code>MyState</code> class, except that we parameterize <code>Option</code> by a type parameter <code>M</code>. <code>M[_]</code> indicates that it is of kind <code>*=&gt;*</code>. <code>(using m:Monad[M])</code> further contraints <code>M</code> must be an instance of Monad, so that we could make use of the <code>bind</code> and <code>pure</code> from <code>M</code>'s Monad instance.</p> <p>Naturally, we can define a derived type class called <code>StateTMonad</code>.</p> <pre><code>type StateTM = [S] =&gt;&gt; [M[_]] =&gt;&gt; [A] =&gt;&gt; StateT[S, M, A]\n\ntrait StateTMonad[S,M[_]] extends Monad[StateTM[S][M]]  {\n    implicit def M0:Monad[M]\n    override def pure[A](v: A): StateT[S, M, A] = StateT(s =&gt; M0.pure((s, v)))\n    override def bind[A, B](\n        fa: StateT[S, M, A]\n    )(\n        ff: A =&gt; StateT[S, M, B]\n    ): StateT[S, M, B] = fa.flatMap(ff)\n    def get: StateT[S, M, S] = StateT(s =&gt; M0.pure((s, s)))\n    def set(v: S): StateT[S, M, Unit] = StateT(s =&gt; M0.pure(v, ()))\n}\n</code></pre> <p>Given that <code>Option</code> is a Monad, we can redefine <code>MyStateMonad</code>  in terms of <code>StateTMonad</code> and <code>optMonad</code>.</p> <pre><code>trait StateOptMonad[S] extends StateTMonad[S, Option] { \n    override def M0 = optMonad\n}\n</code></pre> <p>What about the original vanilla <code>State</code> Monad? We could introduce an Identity Monad.</p> <pre><code>case class Identity[A](run:A) {\n    def flatMap[B](f:A=&gt;Identity[B]):Identity[B] = this match {\n        case Identity(a) =&gt; f(a)\n    }\n    def map[B](f:A=&gt;B):Identity[B] = this match {\n        case Identity(a) =&gt; Identity(f(a))\n    }\n}\n\ngiven identityMonad:Monad[Identity] = new Monad[Identity] {\n    override def pure[A](v:A):Identity[A] = Identity(v)\n    override def bind[A,B](fa:Identity[A])(f: A =&gt; Identity[B]):Identity[B] = fa.flatMap(f)\n}\n</code></pre> <p>Then we can re-define the vanilla <code>State</code> Monad as follows, (in fact like many existing Monad libraries out there.)</p> <pre><code>trait StateIdentMonad[S] extends StateTMonad[S, Identity] { // same as StateMonad\n    override def M0 = identityMonad\n}\n</code></pre> <p>One advantage of having Monad Transformer is that now we can create new Monad by composition of existing Monad Transformers. We are able to segregate and interweave methods from different Monad serving different purposes.</p> <p>Similarly we could generalize the <code>Reader</code> Monad  to its transformer variant.</p> <pre><code>case class ReaderT[R, M[_], A](run: R =&gt; M[A])(using m:Monad[M]) {\n    def flatMap[B](f: A =&gt; ReaderT[R, M, B]):ReaderT[R, M, B] = this match {\n        case ReaderT(ra) =&gt; ReaderT( r =&gt; m.bind(ra(r))\n            ( a =&gt; f(a) match {\n            case ReaderT(rb) =&gt; rb(r)\n            }))\n    }\n    def map[B](f: A =&gt; B):ReaderT[R, M, B] = this match {\n        case ReaderT(ra) =&gt; ReaderT( r =&gt; m.bind(ra(r))\n            ( a =&gt; m.pure(f(a))))\n    }\n}\n\n\ntype ReaderTM = [R] =&gt;&gt;[M[_]] =&gt;&gt; [A] =&gt;&gt; ReaderT[R, M, A]\n\ntrait ReaderTMonad[R,M[_]] extends Monad[ReaderTM[R][M]] {\n    implicit def M0:Monad[M]\n    override def pure[A](v: A): ReaderT[R, M, A] = ReaderT(r =&gt; M0.pure(v))\n    override def bind[A, B](\n        fa: ReaderT[R, M, A]\n    )(f: A =&gt; ReaderT[R, M, B]): ReaderT[R, M, B] = fa.flatMap(f)\n    def ask: ReaderT[R, M, R] = ReaderT(r =&gt; M0.pure(r))\n    def local[A](f: R =&gt; R)(r: ReaderT[R, M, A]): ReaderT[R, M, A] = r match {\n        case ReaderT(ra) =&gt;\n            ReaderT(r =&gt; {\n                val localR = f(r)\n                ra(localR)\n            })\n    }\n}\n\ntrait ReaderIdentMonad[R] extends ReaderTMonad[R, Identity] { // same as ReaderMonad\n    override def M0 = identityMonad\n}\n</code></pre> <p>Note that the order of how Monad Transfomers being stacked up makes a difference,</p> <p>For instance, can you explain what the difference between the following two is?</p> <pre><code>trait ReaderStateIdentMonad[R, S] extends ReaderTMonad[R, StateTM[S][Identity]] {\n    override def M0:StateIdentMonad[S] = new StateIdentMonad[S]{}\n}\n\ntrait StateReaderIdentMonad[S, R] extends StateTMonad[S, ReaderTM[R][Identity]] {\n    override def M0:ReaderIdentMonad[R] = new ReaderIdentMonad[R]{}\n}\n</code></pre>"},{"location":"fp_intro/","title":"50.054 - Introduction to functional programming","text":""},{"location":"fp_intro/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you should be able to:</p> <ul> <li>Characterize functional programming</li> <li>Comprehend, evaluate lambda terms</li> <li>Differentiate different evaluation strategies</li> <li>Implement simple algorithms using Lambda Calculus</li> </ul>"},{"location":"fp_intro/#what-is-functional-programming","title":"What is Functional programming?","text":"<p>Functional programming as suggested by its name, is a programming paradigm in which functions are first class values. In the ideal model of FP, computations are stateless. Variables are bound once and remain unchanged afterwards. Computation is performed by rewriting the current expression into the (reduced) expression via evaluation rules.</p>"},{"location":"fp_intro/#how-fp-differs-from-other-programming-languages","title":"How FP differs from other programming languages?","text":"<p>The main differences were listed in the earlier section.</p> <p>However many modern program languages (including those are not FP) adopted many \"features\" from the functional programming paradigm. It has been proven that the FP coding style improves code qualities in many aspects.</p> <p>Consider the following two different implementations of the insertion sort algorithm (assuming that the readers having prior knowledge of Python and insertion sort algorithm):</p> <pre><code>def isort(vals):\n   for i in range(1, len(vals)):\n      curr = i   \n      for j in range(i, 0, -1):\n         # scan backward to insert vals[curr] into the right pos\n         if vals[curr] &gt; vals[j-1]:\n            vals[curr], vals[j-1] = vals[j-1], vals[curr]\n            curr = j-1\n   return vals\n</code></pre> <pre><code>def isort2(vals):\n   def insert(x, xs):\n      # invarant: xs is already sorted in descending order\n      if len(xs) &gt; 0:\n         if x &gt; xs[0]:\n            return [x] + xs\n         else:\n            return [xs[0]] + insert(x, xs[1:])\n      else:\n         return [x]\n   def isort_sub(sorted, to_be_sorted):\n      # invariant sorted is already sorted in descending order\n      if len(to_be_sorted) &gt; 0:\n         val = to_be_sorted[0]\n         to_be_sorted_next = to_be_sorted[1:]\n         sorted_next = insert(val, sorted)\n         return isort_sub(sorted_next, to_be_sorted_next)\n      else:\n         return sorted\n   return isort_sub([], vals)\n</code></pre> <p><code>isort</code> is implemented in the imperative style; the way we are familiar with.</p> <p><code>isort2</code> is implemented in a functional programming style, we've seen it but we are not too familar with it. We probably won't code in <code>isort2</code> in Python, because:</p> <ol> <li>it is lengthy</li> <li>it is less efficient, as it involves recursion (function call stack is building up) and there are too many list slicing and concatenation operations.</li> </ol> <p>But why are people are interested in FP? The reason is that the invariant of <code>isort</code> is much harder to derive compared to <code>isort2</code> in which the nested functions' parameters are the subject of the invariants, and the variables in <code>isort2</code> are mostly immutable, i.e. they don't change over execution, and we won't need symbolic execution or variable renaming.</p> <p>Furthermore in some FP languages with advanced typing systems such as type constraints and dependent types, these invariants in <code>isort2</code> can be expressed as type constraints, which can be verified by the compiler.</p> <p>What about the inefficiency? Most of the FP compilers handle recursions with care and are able to optimize them into efficient code. Data structure in FP are inductively defined, and optimizations such as shallow cloning are used to avoid data structure reconstruction.</p> <p>In fact many modern FP languagues are quite fast. For example:</p> <ul> <li>https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/ghc-clang.html</li> <li>https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/</li> </ul>"},{"location":"fp_intro/#why-fp-in-compiler-design","title":"Why FP in Compiler Design?","text":"<p>Implementing a compiler requires rigorous software design and engineering principles. Bugs arising from a compiler have severe implication in softwares developed in the language that it compiles.</p> <p>To establish correctness results, testing is not sufficient to eliminate errors in the compilers. When designing a compiler, we often begin with formal reasoning with mathematical foundation as the specification. As we learn later in this module, these specifications are presented in a form in which resembles the data structures and accessor methods found in many functional programming languages. Thus, it is arguably easier to implement the given specification in FP programs compared to other programming paradigms.</p> <p>One key difference is that in FP, there is no for-loop or while-loop. Iteration has to be implemented via recursive functions.</p> <p>This implies that loop invariances are not constraints among the input and output of these recurisve function.</p> <p>In many main stream functional programming languages, such as Ocaml, Haskell and Scala are shipped with powerful type systems which allow us to express some of the properties of the algorithms in terms of type constraints, by doing so, these (invariant) properties are verifiable by the compilers of function programming languages.</p>"},{"location":"fp_intro/#lambda-calculus","title":"Lambda Calculus","text":"<p>Lambda Calculus is the minimal core of many functional programming languages. It consists of the Lambda expression and the evaluation rule(s).</p>"},{"location":"fp_intro/#lambda-expression","title":"Lambda Expression","text":"<p>This comic gives a very easy way to understand Lambda Expressions</p> <p>The valid syntax of lambda expression is described as the following EBNF (Extended Backus Naur Form) grammar:</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\end{array} \\] <p>Where:</p> <ul> <li>Each line denotes a grammar rule</li> <li>The left hand side (LHS) of the <code>::=</code> is a non-terminal symbol, in this case \\(t\\) is a non-terminal symbol.</li> <li>The RHS of the <code>::=</code> is a set of alternatives, separated by <code>|</code>. Each alternative denote a possible outcome of expanding the LHS non-terminal. In this case \\(t\\) has three possibilities, i.e. \\(x\\), \\(\\lambda x.t\\) or \\(t\\ t\\).</li> <li>\\(x\\) denotes a variable,</li> <li>\\(\\lambda x.t\\) denotes a lambda abstraction.<ul> <li>Within a lambda abstraction,  \\(x\\) is the bound variable (c.f. formal argument of the function) and \\(t\\) is the body.</li> </ul> </li> <li>\\(t\\ t\\) denotes a function application.</li> </ul> <p>For example, the following are three instances of \\(t\\).</p> <ol> <li>\\(x\\)</li> <li>\\(\\lambda x.x\\)</li> <li>\\((\\lambda x.x)\\ y\\)</li> </ol> <p>Note that given a lambda term, there might be multiple ways of parsing (interpreting) it. For instance, Given \\(\\lambda x.x\\ \\lambda y.y\\), we could interpret it as either</p> <ol> <li>\\((\\lambda x.x)\\ (\\lambda y.y)\\), or</li> <li>\\(\\lambda x.(x\\ \\lambda y.y)\\)</li> </ol> <p>As a convention, in the absence of parentheses, we take 2 as the default interpretation. We should include parentheses whenever ambiguity arise as much as we can.</p>"},{"location":"fp_intro/#evaluation-rules","title":"Evaluation Rules","text":"<p>Lambda calculus is very simple and elegant. To execute (or we often say \"to evaluate\") a given lambda term, we apply the evaluation rules to rewrite the term.</p> <p>There are only two rules to consider.</p> <p>Each rule is defined via a reduction relation \\(t \\longrightarrow t'\\), which reads as \\(t\\) is reduced to \\(t'\\) by a step.</p>"},{"location":"fp_intro/#beta-reduction","title":"Beta Reduction","text":"\\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} &amp; (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\end{array} \\] <p>What's new here is the term \\([t_2/x]\\), which is a meta term, which refers to a substitution. \\([t_2/x]t_1\\) denotes the application of the substitution \\([t_2/x]\\) to \\(t_1\\), Informally speaking it means we replace every occurrence of the formal argument \\(x\\) in \\(t_1\\) with \\(t_2\\).</p> <p>For instance, recall our earlier example:</p> \\[ \\begin{array}{rl} (\\lambda x.x)\\ (\\lambda y.y) &amp; \\longrightarrow_{\\scriptsize {\\tt (\\beta\\ reduction)}} \\\\ \\lbrack(\\lambda y.y)/x \\rbrack x &amp; \\longrightarrow _{\\scriptsize {\\tt (substitution)}} \\\\ \\lambda y.y \\end{array} \\] <p>It is common understanding in programming that there are scopes of variables. We can reuse the same name for different variables in different scopes without affecting the meanings of the program. Consider a variant of our running example:</p> \\[  (\\lambda x.x)\\ {\\tt (\\lambda x. x)} \\] <p>Here, we use a different font type for variables named \\(x\\) in different scopes. \\(x\\) is bound in the first lambda abstraction and \\({\\tt x}\\) is bound in the second lambda abstraction. It behaves the same as the original running example except for the name of the variable in the second lambda abstraction.</p> <p>To formally define the substitution operation used in the \\(\\beta\\) reduction rule, we need to compute the free variables, i.e. variables that are not bound.</p> \\[ \\begin{array}{rcl} fv(x) &amp; = &amp; \\{x\\}\\\\ fv(\\lambda x.t) &amp; = &amp; fv(t) - \\{x\\} \\\\ fv(t_1\\ t_2) &amp; = &amp; fv(t_1) \\cup fv(t_2) \\end{array} \\] <p>For instance.</p> \\[ \\begin{array}{rcl} fv(\\lambda x.x) &amp; = &amp; fv(x) - \\{x\\} \\\\                 &amp; = &amp; \\{ \\} \\end{array} \\] \\[ \\begin{array}{rcl} fv(\\lambda x.x\\ (\\lambda z.y\\ z)) &amp; = &amp; fv(x\\ (\\lambda z.y\\ z)) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup fv(\\lambda z.y\\ z)) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup (fv(y\\ z) - \\{z\\})) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup ((\\{y\\} \\cup \\{z\\}) - \\{z\\})) - \\{x\\} \\\\    &amp; = &amp; (\\{x\\} \\cup (\\{y, z\\} - \\{z\\})) - \\{x\\} \\\\    &amp; = &amp; \\{ y \\} \\end{array} \\] <p>One common error we often encounter is capturing the free variables.</p> <p>Consider:</p> \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] <p>Note:</p> \\[ fv((\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w)) =  \\{ {\\tt y}, w \\} \\] <p>Thus: $$ \\begin{array}{rl} (\\lambda x. \\lambda y.x y) ({\\tt y} w) &amp; \\longrightarrow_{\\scriptsize {\\tt (\\beta reduction)}} \\ \\lbrack({\\tt y} w)/x\\rbrack \\lambda y.x y &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\ \\lambda y. ({\\tt y} w) y \\end{array} $$</p> <p>Error! We captured the free variable \\({\\tt y}\\) in the lambda abstraction accidentally via substitution. Now the free variable \\({\\tt y}\\) is \"mixed up\" with the lambda bound variable \\(y\\) by mistake.</p>"},{"location":"fp_intro/#substitution-and-alpha-renaming","title":"Substitution and Alpha Renaming","text":"<p>In the following we consider all the possible cases for subsititution</p> \\[ \\begin{array}{rcll}  \\lbrack t_1 / x \\rbrack x &amp; = &amp; t_1 \\\\  \\lbrack t_1 / x \\rbrack y &amp; = &amp; y &amp; {\\tt if}\\  x \\neq y \\\\  \\lbrack t_1 / x \\rbrack (t_2\\ t_3) &amp; = &amp; \\lbrack t_1 / x \\rbrack t_2\\  \\lbrack t_1 / x \\rbrack t_3 &amp; \\\\  \\lbrack t_1 / x \\rbrack \\lambda y.t_2 &amp; = &amp; \\lambda y. \\lbrack t_1 / x  \\rbrack t_2 &amp; {\\tt if}\\  y\\neq x\\  {\\tt and}\\  y \\not \\in fv(t_1) \\end{array} \\] <p>In case  </p> \\[ y\\neq x\\  {\\tt and} \\ y \\not \\in fv(t_1) \\] <p>is not satified, we need to rename the lambda bound variables that are clashing. Recall:</p> \\[ (\\lambda x. \\lambda y.x\\ y)\\ ({\\tt y}\\ w) \\] <p>We rename the inner lambda bound variable \\(y\\) to \\(z\\):</p> \\[ (\\lambda x. \\lambda z.x\\ z)\\ ({\\tt y}\\ w) \\] <p>to avoid clashing, prior to applying the \\(\\beta\\) reduction. The renaming operation is also known as the \\(\\alpha\\) renaming.</p>"},{"location":"fp_intro/#evaluation-strategies","title":"Evaluation strategies","text":"<p>So far we have three rules (roughly)  \\(\\beta\\) reduction, substitution, and  \\(\\alpha\\) renaming.</p> <p>Given a lambda term, in order to evaluate it, we need to identify places that we can apply these rules.</p> <p>We call a (sub-)expression of shape \\((\\lambda x.t_1)\\ t_2\\) a redex.</p> <p>The task is to look for redexes in a lambda term and rewrite them by applying  \\(\\beta\\) reduction and substitution, and sometimes  \\(\\alpha\\) renaming to avoid capturing free variables.</p> <p>But in what order shall we apply these rules?</p> <p>There are two mostly known strategies</p> <ol> <li>Inner-most, leftmost - Applicative Order Reduction (AOR)</li> <li>Outer-most, leftmost - Normal Order Reduction (NOR)</li> </ol> <p>Consider \\((\\lambda x. ((\\lambda x. x)\\ x))\\ (\\lambda y.y)\\),</p> <ul> <li>AOR:</li> </ul> \\[ \\begin{array}{rll} (\\lambda x. (\\underline{(\\lambda x. x)\\ x}))\\ (\\lambda y.y)  &amp; \\longrightarrow_{\\tt (\\beta\\ reduction)} &amp;\\\\ \\underline{(\\lambda x.x)\\ (\\lambda y.y)}  &amp; \\longrightarrow_{\\tt (\\beta\\ reduction)} \\\\ \\lambda y.y \\end{array} \\] <ul> <li>NOR:</li> </ul> \\[ \\begin{array}{rl} \\underline{(\\lambda x. ((\\lambda x. x)\\ x))}\\ (\\lambda y.y)  &amp; \\longrightarrow_{\\tt(\\alpha)} \\\\ \\underline{(\\lambda z. [z/x]((\\lambda x.x)\\ x))}\\ (\\lambda y.y) &amp; \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ([z/x](\\lambda x.x)\\ [z/x]x))}\\ (\\lambda y.y) &amp; \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ([z/x](\\lambda x.x)\\ z))}\\ (\\lambda y.y) &amp; \\longrightarrow_{(\\alpha)} \\\\ \\underline{(\\lambda z. ([z/x](\\lambda u.[u/x]x)\\ z))}\\ (\\lambda y.y) &amp; \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ([z/x](\\lambda u.u)\\ z))}\\ (\\lambda y.y) &amp; \\longrightarrow_{\\tt (substitution)} \\\\ \\underline{(\\lambda z. ((\\lambda u. u)\\ z))\\ (\\lambda y.y)}  &amp; \\longrightarrow_{\\tt(\\beta)} \\\\   \\underline{(\\lambda u. u)\\ (\\lambda y.y)} &amp; \\longrightarrow_{\\tt (\\beta)}  \\\\ \\lambda y.y \\end{array} \\]"},{"location":"fp_intro/#interesting-notes","title":"Interesting Notes","text":"<ol> <li> <p>Some connection with real world languages:</p> <ul> <li>Call By Value semantics (CBV, found in C, C++, etc.) is like AOR except that we do not evaluate under lambda abstractions.</li> <li>Call By Name semantics (CBN, found in Haskell, etc.) is like NOR except that we do not evaluate under lambda abstractions.</li> </ul> </li> <li> <p>AOR or NOR, which one is better?</p> <ul> <li>By Church-Rosser Theorem, if a lambda term can be evaluated in two different ways and both ways terminate, both will yield the same result.</li> <li>Recall our earlier example.</li> <li>So how can it be non-terminating? Consider:</li> </ul> \\[ \\begin{array}{rl} (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x) &amp; \\longrightarrow \\\\ \\lbrack(\\lambda x.x\\ x)/x\\rbrack (x\\ x) &amp; \\longrightarrow \\\\ (\\lambda x.x\\ x)\\ (\\lambda x.x\\ x)  &amp; \\longrightarrow \\\\ ... \\end{array} \\] </li> <li> <p>NOR seems computationally more expensive, but is also more likely to terminate than AOR. Consider how       \\(((\\lambda x.\\lambda y.x)\\ x)\\  ((\\lambda x.x\\ x)\\ (\\lambda x.x\\ x))\\) terminates in NOR with \\(x\\), but diverges in AOR.</p> </li> <li>NOR can be used to evaluate terms that deals with infinite data.</li> </ol>"},{"location":"fp_intro/#let-binding","title":"Let Binding","text":"<p>Let-binding allows us to introduce local (immutable) variables.</p>"},{"location":"fp_intro/#approach-1-extending-the-syntax-and-evaluation-rules","title":"Approach 1 - extending the syntax and evaluation rules","text":"<p>We extend the syntax with let-binding:</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\end{array} \\] <p>and the evaluation rule:</p> \\[ \\begin{array}{rl} {\\tt (Let)} &amp; let\\ x=t_1\\ in\\ t_2 \\longrightarrow [t_1/x]t_2 \\\\ \\\\ \\end{array} \\] <p>and the substitution rule and the free variable function \\(fv()\\):</p> \\[ \\begin{array}{rcl} \\lbrack t_1 / x \\rbrack let\\ y = t_2\\ in\\ t_3 &amp; = &amp; let\\ y = \\lbrack t_1 / x \\rbrack t_2\\ in\\ \\lbrack t_1 / x \\rbrack t_3 &amp; {\\tt if}\\  y\\neq x\\  {\\tt and}\\  y \\not \\in fv(t_1) \\\\ \\end{array}  \\] \\[ \\begin{array}{rcl} fv(let\\ x=t_1\\ in\\ t_2) &amp; = &amp; (fv(t_1) - \\{x\\}) \\cup fv(t_2) \\\\ \\end{array} \\] <p>Note that the alpha renaming should be applied when name clash arises.</p>"},{"location":"fp_intro/#approach-2-desugaring","title":"Approach 2 - desugaring","text":"<p>In the alternative approach, we could use a pre-processing step to desugar the let-binding into an application. In compiler context, desugaring refers to the process of rewriting the source code from some high-level form to the core language.</p> <p>We can rewrite:</p> \\[ let\\ x=t_1\\ in\\ t_2 \\] <p>into:</p> \\[ (\\lambda x.t_2)\\ t_1 \\] <p>where \\(x \\not\\in fv(t_1)\\).</p> <p>What happen if \\(x \\in fv(t_1)\\)? It forms a recursive definition. We will look into recursion in a later section.</p>"},{"location":"fp_intro/#conditional-expression","title":"Conditional Expression","text":"<p>A language is pretty much useless without conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\). There are at least two different ways of incorporating conditional expression in our lambda term language.</p>"},{"location":"fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules","title":"Approach 1 - Extending the syntax and the evaluation rules","text":"<p>We could extend the grammar</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x=\\ t\\ in\\ t \\mid  if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\\\  {\\tt (Builtin\\ Operators)} &amp; op &amp; ::= &amp; + \\mid - \\mid * \\mid / \\mid\\ == \\\\  {\\tt (Builtin\\ Constants)} &amp; c &amp; ::= &amp; 0 \\mid 1 \\mid ... \\mid true \\mid false \\end{array} \\] <p>and the evaluation rules</p> \\[ \\begin{array}{rc} {\\tt (\\beta\\ reduction)} &amp; (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ \\\\ {\\tt (ifI)} &amp; \\begin{array}{c}                t_1 \\longrightarrow t_1'  \\\\                \\hline                if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\longrightarrow if\\ t_1'\\ then\\ t_2\\ else\\ t_3                \\end{array} \\\\ \\\\ {\\tt (ifT)} &amp;  if\\ true\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_2 \\\\ \\\\ {\\tt (ifF)} &amp;  if\\ false\\ then\\ t_2\\ else\\ t_3 \\longrightarrow t_3 \\\\ \\\\ {\\tt (OpI1)} &amp; \\begin{array}{c}                 t_1 \\longrightarrow t_1' \\\\                 \\hline                 t_1\\ op\\ t_2\\  \\longrightarrow t_1'\\ op\\ t_2                 \\end{array} \\\\ \\\\ {\\tt (OpI2)} &amp; \\begin{array}{c}                 t_2 \\longrightarrow t_2' \\\\                 \\hline                 c_1\\ op\\ t_2\\  \\longrightarrow c_1\\ op\\ t_2'                 \\end{array} \\\\ \\\\ {\\tt (OpC)} &amp;  \\begin{array}{c}                 invoke\\ low\\ level\\ call\\  op(c_1, c_2) = c_3 \\\\                 \\hline                   c_1\\ op\\ c_2\\  \\longrightarrow c_3                 \\end{array} \\\\ \\\\                 ... \\end{array} \\] <p>In the above we use a horizontal line to separate complex deduction rules that have some premise. The relations and statement written above the horizontal line are called the premises, and the relation the written below is called the conclusion. The conclusion holds if the premises are valid.</p> <ul> <li>The rule \\({\\tt (ifI)}\\) states that if we can evaluate  \\(t_1\\) to  \\(t_1'\\), then  \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) can be evaluated to  \\(if\\ t_1' \\ then\\ t_2\\ else\\ t_3\\). In otherwords, for us to reduce \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) to \\(if\\ t_1' \\ then\\ t_2\\ else\\ t_3\\), a pre-condition is to reduce \\(t_1\\) to \\(t_1'\\). </li> <li>The rule  \\({\\tt (ifT)}\\) states that if the conditional expression is \\(true\\), the entire term is evaluated to the then-branch.</li> <li>The rule  \\({\\tt (ifF)}\\) is similar.</li> <li>Rules  \\({\\tt (OpI1)}\\) and \\({\\tt (OpI2)}\\) are similar to rule \\({\\tt (IfI)}\\).</li> <li>The rule  \\({\\tt (OpC)}\\) invokes the built-in low level call to apply the binary operation to the two operands  \\(c_1\\) and  \\(c_2\\).  </li> </ul> <p>The substitution rules and free variable function \\(fv()\\) are also extended too</p> \\[ \\begin{array}{rcll}  \\lbrack t_1 / x \\rbrack c &amp; = &amp; c \\\\     \\lbrack t_1 / x \\rbrack t_2\\ op\\ t_3 &amp; = &amp; (\\lbrack t_1 / x \\rbrack t_2)\\ op\\ (\\lbrack t_1 / x \\rbrack t_3) \\\\    \\lbrack t_1 / x \\rbrack if\\ t_2\\ then\\ t_3\\ else\\ t_4 &amp; = &amp; if\\ \\lbrack t_1 / x \\rbrack t_2\\ then\\ \\lbrack t_1 / x \\rbrack t_3\\ else\\ \\lbrack t_1 / x \\rbrack t_4 \\\\  \\end{array} \\] \\[ \\begin{array}{rcl} fv(t_1\\ op\\ t_2) &amp; = &amp; fv(t_1) \\cup fv(t_2) \\\\  fv(if\\ t_1\\ then\\ t_2\\ else\\ t_3) &amp; = &amp; fv(t_1) \\cup fv(t_2) \\cup fv(t_3) \\\\ fv(c) &amp; = &amp; \\{\\} \\\\ \\end{array} \\] <p>Let's consider an example:</p> \\[ \\begin{array}{rl} (\\lambda x.if\\ x==0\\ then\\  0\\  else\\  10/x)\\ 2 &amp; \\longrightarrow_{\\scriptsize {\\tt \\beta}} \\\\ \\lbrack 2/x \\rbrack if\\ x==0\\ then\\  0\\  else\\  10/x &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 2==0\\ then\\  0\\  else\\  10/2 &amp; \\longrightarrow_{\\scriptsize {\\tt (IfI)}} \\\\ if\\ false\\ then\\ 0\\  else\\  10/2 &amp; \\longrightarrow_{\\scriptsize {\\tt (IfF)}} \\\\ 10/2 &amp; \\longrightarrow_{\\scriptsize {\\tt (OpC)}} \\\\ 5 \\end{array} \\]"},{"location":"fp_intro/#approach-2-church-encoding","title":"Approach 2 - Church Encoding","text":"<p>Instead of extending the syntax and evaluation rules, we could encode the conditional expression in terms of the basic lambda terms.</p> <p>Thanks to Church-encoding (discovered by Alonzo Church), we can encode boolean data and if-then-else using Lambda Calculus.</p> <p>Let's define:</p> <ul> <li>\\(true\\) as \\(\\lambda x.\\lambda y.x\\)</li> <li>\\(false\\) as \\(\\lambda x.\\lambda y.y\\)</li> <li>\\(ite\\) (read as if-then-else) as \\(\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3\\)</li> </ul> <p>We assume the function application is left associative, i.e. \\(e_1\\ e_2\\ e_3 \\equiv  (e_1\\ e_2)\\ e_3\\). For example,</p> \\[ \\begin{array}{rl} ite\\ true\\ w\\ z &amp; = \\\\ (\\lambda e_1. \\lambda e_2. \\lambda e_3. e_1\\ e_2\\ e_3)\\ true\\ w\\ z &amp; \\longrightarrow \\\\ true\\ w\\ z &amp; =  \\\\ (\\lambda x.\\lambda y.x)\\ w\\ z &amp; \\longrightarrow  \\\\ w \\end{array} \\]"},{"location":"fp_intro/#recursion","title":"Recursion","text":"<p>To make our language turing complete, we need to support loops. The way to perform loops in lambda calculus is via recursion.</p> <p>Similar to the conditional expression, there are at least two ways of introducing recursion to our language.</p>"},{"location":"fp_intro/#approach-1-extending-the-syntax-and-the-evaluation-rules_1","title":"Approach 1 - Extending the syntax and the evaluation rules","text":"<p>We extend the syntax with a mu-abstraction (\\(\\mu\\)):</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; ... \\mid \\mu f.t \\end{array} \\] <p>and the evaluation rules:</p> \\[ \\begin{array}{rl} {\\tt (\\beta\\ reduction)} &amp; (\\lambda x.t_1)\\ t_2 \\longrightarrow [t_2/x] t_1 \\\\ {\\tt (NOR)} &amp; \\begin{array}{c}                 t_1 \\longrightarrow t_1' \\\\                 \\hline                 t_1\\ t_2 \\longrightarrow t_1'\\ t_2                 \\end{array} \\\\ {\\tt (unfold)} &amp; \\mu f.t \\longrightarrow [(\\mu f.t)/f] t  \\\\ \\end{array} \\] <p>Note that we include the  \\({\\tt (NOR)}\\) rule into our evaluation rules to fix the evaluation strategy, and we only reduce the redexes that are not inside a lambda abstraction, otherwise the program does not terminate.</p> <p>We include the following cases for the free variable function \\(fv()\\) and the substitution</p> \\[ \\begin{array}{rcl} fv(\\mu f.t) &amp; = &amp; fv(t) - \\{f\\} \\end{array} \\] <p>and </p> \\[ \\begin{array}{rcl}  \\lbrack t_1 / x \\rbrack \\mu f.t_2 &amp; = &amp; \\mu f.\\lbrack t_1 / x \\rbrack t_2 &amp; {\\tt if}\\  f\\neq x\\  {\\tt and}\\  f \\not \\in fv(t_1) \\end{array} \\] <p>For instance:</p> \\[ \\begin{array}{rl} (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 &amp; \\longrightarrow_{\\scriptsize {\\tt(NOR)+(unfold)}} \\\\ (\\lbrack (\\mu f.\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))/f \\rbrack \\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*(f\\ (x-1)))\\ 3 &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution) + (\\alpha)}} \\\\ (\\lambda x.if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ y*(f\\ (y-1)))\\ (x-1)))\\ 3 &amp; \\longrightarrow_{\\scriptsize {\\tt (\\beta)}} \\\\ \\lbrack 3/x \\rbrack if\\ x==1\\ then\\ 1\\ else\\ x*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ y*(f\\ (y-1)))\\ (x-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (substitution)}} \\\\ if\\ 3==1\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ y*(f\\ (y-1)))\\ (3-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (ifI)+(OpC)}} \\\\ if\\ false\\ then\\ 1\\ else\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ y*(f\\ (y-1)))\\ (3-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (ifF)}} \\\\ 3*((\\mu f.\\lambda y.if\\ y==1\\ then\\ 1\\ else\\ y*(f\\ (y-1)))\\ (3-1)) &amp; \\longrightarrow_{\\scriptsize {\\tt (OpI2)}} \\\\ ... \\\\ 3*(2*1) \\end{array} \\] <p>The \\({\\tt (unfold)}\\) rule, (just like alpha-renaming), is not contributing to the progress in the evaluation directly. If we keep applying \\({\\tt (unfold)}\\) repeatedly without applying other rules, we are just expanding the lambda term infinitely without making progress. Hence we apply the \\({\\tt (unfold)}\\) rule only when there is no other applicable rule. (Just like alpha renaming, we only rename when we foresee a name clashing in substitution.)</p> <p>Another important point to note is that the set of rewriting rules we have gathered so far \\({\\tt (\\beta-reduction)}\\), \\({\\tt (NOR)}\\), \\({\\tt (unfold)}\\), \\({\\tt (IfT)}\\), \\({\\tt (IfF)}\\), \\({\\tt (IfI)}\\), \\({\\tt (OpC)}\\), \\({\\tt (OpI2)}\\) and \\({\\tt (OpI1)}\\) are syntax-directed, i.e. the LHS of the \\(\\longrightarrow\\) in the conclusion, which is AKA the head of the rule is unique if we try applying the rules in this specific order. A clear advantage of this is that we can view this deduction rule system as an algorithm, i.e. an implementation that resembles this specification exists. We will see this in the later part of this course. </p>"},{"location":"fp_intro/#approach-2-church-encoding_1","title":"Approach 2 - Church Encoding","text":"<p>Alternatively, recursion can be encoded using the fix-pointer combinator (AKA  \\(Y\\)-combinator). Let \\(Y\\) be</p> \\[ \\lambda f.((\\lambda y. (f\\ (y\\ y)))~(\\lambda x.(f\\ (x\\ x)))) \\] <p>We find that for any function \\(g\\), we have \\(Y\\ g = g\\ (Y\\ g)\\).</p> <p>We will work on the derivation during exercise.</p> <p>Let's try to implement the factorial function over natural numbers:</p> \\[ \\begin{array}{cc}    fac(n) = \\left [          \\begin{array}{ll}             1 &amp;  {if}~ n = 0 \\\\             n*fac(n-1) &amp; {otherwise}          \\end{array} \\right . \\end{array} \\] <p>Our goal is to look for a fixpoint function \\(Fac\\) such that \\(Y\\ Fac \\longrightarrow Fac\\ (Y\\ Fac)\\) and \\(Y\\ Fac\\) implements the above definition.</p> <p>Let \\(Fac\\) be</p> \\[ \\begin{array}{c}  \\lambda fac. \\lambda n. ite\\ (iszero\\ n)\\ one\\ (mul\\ n\\ (fac\\ (pred\\ n))) \\end{array} \\] <p>where \\(iszero\\) tests whether a number is 0 in Church Encoding. \\(mul\\) multiplies two numbers. \\(pred\\) takes a number and return its predecessor in natural number order. Then \\(Y\\ Fac\\) will be the implementation of the factorial function described above.</p>"},{"location":"fp_intro/#discussion-1","title":"Discussion 1","text":"<p>How to define the following?</p> <ul> <li>\\(one\\)</li> <li>\\(iszero\\)</li> <li>\\(mul\\)</li> <li>\\(pred\\)</li> </ul>"},{"location":"fp_intro/#discussion-2","title":"Discussion 2","text":"<p>The current evaluation strategy presented resembles the call-by-need semantics, in which the function arguments are not evaluated until they are needed. What modification will be required if we want to implement a call-by-value semantics (AKA. strict evaluation)?</p> <p>We will work on the two topics discussed above during the cohort class.</p>"},{"location":"fp_intro/#summary","title":"Summary","text":"<p>We have covered</p> <ul> <li>Syntax (lambda terms) and Semantics (\\(\\beta\\) reduction, substitution, \\(\\alpha\\) renaming).</li> <li>Evaluation strategies, their properties and connection to real world programming</li> <li>Extending lambda calculus to support conditional and loop<ul> <li>Via language extension (we will use)</li> <li>Via Church encoding (fun but not very pragmatic in our context)</li> </ul> </li> </ul>"},{"location":"fp_scala/","title":"50.054 - Instroduction to Scala","text":""},{"location":"fp_scala/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this class, you should be able to</p> <ul> <li>Develop simple implementation in Scala using List, Conditional, and Recursion</li> <li>Model problems and design solutions using Algebraic Datatype and Pattern Matching</li> <li>Compile and execute simple Scala programs</li> </ul>"},{"location":"fp_scala/#what-is-scala","title":"What is Scala?","text":"<p>Scala is a hybrid programming language which combines Object Oriented Paradigm and Functional Programming Paradigm. Scala has many backends, including JVM, node.js and native.</p> <p>Scala is widely used in the industry and the research communities. There many industry projects and open source projects were implemented mainly in Scala, e.g. Apache Spark, Kafka, Akka, Play! and etc. For more details in how Scala is used in the real-world business, you may refer to the following for further readings.</p> <ul> <li>Scala at Scale at Databricks</li> <li>Why Scala is seeing a renewed interest for developing enterprise software</li> <li>Who is using Scala, Akka and Play framework</li> <li>Type-safe Tensor</li> </ul>"},{"location":"fp_scala/#scala-hello-world","title":"Scala Hello World","text":"<p>Let's say we have a Scala file named <code>HelloWorld.scala</code></p> <pre><code>println(\"hello world\")\n</code></pre> <p>We can execute it via either</p> <pre><code>scala HelloWorld.scala\n</code></pre> <p>or to compile it then run</p> <pre><code>scalac HelloWorld.scala &amp;&amp; scala HelloWorld\n</code></pre> <p>In the cohort problems, we are going to rely on a Scala project manager called <code>sbt</code> to build, execute and test our codes.</p>"},{"location":"fp_scala/#scala-oop-vs-java-oop","title":"Scala OOP vs Java OOP","text":"<p>If you know Object Oriented Programming, you already know 70% of Scala.</p> <p>Consider the following Java code snippet</p> <pre><code>interface FlyBehavior {\n    void fly();\n}\n\nabstract class Bird {\n    private String species;\n    private FlyBehavior fb;\n    public Bird(String species, FlyBehavior fb) {\n        this.species = species;\n        this.fb = fb;\n    }\n    public String getSpecies() { return this.species; }\n    public void fly() { return this.fb.fly(); }\n}\n\nclass Duck extends Bird {\n    public Duck() {\n        super(\"Duck\", new FlyBehavior() {\n            @override\n            void fly() {\n                System.out.println(\"I can't fly\");\n            }\n        })\n    }\n}\n\nclass BlueJay extends Bird {\n    public BlueJay() {\n        super(\"BlueJay\", new FlyBehavior() {\n            @override\n            void fly() {\n                System.out.println(\"Swwooshh!\");\n            }\n        })\n    }\n}\n</code></pre> <p>We define an abstract class <code>Bird</code> which has two member attributes, <code>species</code> and <code>fb</code>. We adopt the Strategy design pattern to delegate the fly behavior of the bird through an interface <code>FlyBehavior</code>.</p> <p>Scala has the equivalence of language features as Java. The language has much concise syntax. In the following we implement the same logic in Scala.</p> <pre><code>trait FlyBehavior { \n    def fly()\n}\n\nabstract class Bird(species:String, fb:FlyBehavior) { \n    def getSpecies():String = this.species\n    def fly():Unit = this.fb.fly()\n}\n\nclass Duck extends Bird(\"Duck\", new FlyBehavior() {\n    override def fly() = println(\"I can't fly\")\n})\n\nclass BlueJay extends Bird(\"BlueJay\", new FlyBehavior() {\n    override def fly() = println(\"Swwooshh!\")\n})\n</code></pre> <p>In Scala, we prefer inline constructors. A <code>trait</code> is the Scala equivalent of Java's interface. Similar to Python, methods start with <code>def</code>. A method's return type comes after the method name declaration. Type annotations follow their  arguments instead of preceding them. Method bodies are defined after an equality sign. The <code>return</code> keyword is optional; the last expression will be returned as the result. The Java style of method body definition is also supported, i.e. the <code>getSpecies()</code> method can be defined as follows:</p> <pre><code>def getSpecies():String { return this.species }\n</code></pre> <p>Being a JVM language, Scala allows us to import and invoke Java libraries in Scala code.</p> <pre><code>import java.util.LinkedList\nval l = new java.util.LinkedList[String]()\n</code></pre> <p>Keyword <code>val</code> defines an immutable variable, and <code>var</code> defines a mutable variable.</p>"},{"location":"fp_scala/#functional-programming-in-scala-at-a-glance","title":"Functional Programming in Scala at a glance","text":"<p>In this module, we focus and utilise mostly the functional programming feature of Scala.</p> Lambda Calculus Scala Variable \\(x\\) <code>x</code> Constant \\(c\\) <code>1</code>, <code>2</code>, <code>true</code>, <code>false</code> Lambda abstraction \\(\\lambda x.t\\) <code>(x:T) =&gt; e</code> Function application \\(t_1\\ t_2\\) <code>e1(e2)</code> Conditional \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\) <code>if (e1) { e2 } else { e3 }</code> Let Binding \\(let\\ x = t_1\\ in\\ t_2\\) <code>val x = e1 ; e2</code> Recursion \\(let\\ f = (\\mu g.\\lambda x.g\\ x)\\ in\\ f\\ 1\\) <code>def f(x:Int):Int = f(x); f(1);</code> <p>where <code>T</code> denotes a type and <code>:T</code> denotes a type annotation. <code>e</code>, <code>e1</code>, <code>e2</code> and <code>e3</code> denote expressions.</p> <p>Similar to other mainstream languages, defining recursion in Scala is straight-forward, we just make reference to the recursive function name in its body.</p> <pre><code>def fac(x:Int):Int = { \n    if (x == 0) { 1 } else { x*fac(x-1) }\n}\n\nval result = fac(10)\n</code></pre>"},{"location":"fp_scala/#scala-strict-and-lazy-evaluation","title":"Scala Strict and Lazy Evaluation","text":"<p>Let <code>f</code> be a non-terminating function <pre><code>def f(x:Int):Int = f(x)\n</code></pre> The following shows that the function application in Scala is using strict evaluation. <pre><code>def g(x:Int):Int = 1\ng(f(1)) // it does not terminate\n</code></pre> On the other hand, the following code is terminating.  <pre><code>def h(x: =&gt; Int):Int = 1\nh(f(1)) // it terminates!\n</code></pre> The type annotation <code>: =&gt; Int</code> after <code>x</code> states that the argument <code>x</code> is passed in by name (lazy evaluation), not by value (strict evaluation).</p>"},{"location":"fp_scala/#list-data-type","title":"List Data type","text":"<p>We consider a commonly used builtin data type in Scala, the list data type. In Scala, the following define some list values.</p> <ol> <li><code>Nil</code> - an empty list.</li> <li><code>List()</code> - an empty list.</li> <li><code>List(1,2)</code> - an integer list contains two values.</li> <li><code>List(\"a\")</code> - an string list contains one value.</li> <li><code>1::List(2,3)</code> - prepends a value <code>1</code> to a list containing <code>2</code> and <code>3</code>.</li> <li><code>List(\"hello\") ++ List(\"world\")</code> - concatenating two string lists.</li> </ol> <p>To iterate through the items in a list, we can use a for-loop:</p> <pre><code>def sum(l:List[Int]):Int = {\n    var s = 0\n    for (i &lt;- l) {\n        s = s+i\n    }\n    s\n}\n</code></pre> <p>which is very similar to what we could implement in Java or Python.</p> <p>However, we are more interested in using the functional programming features in Scala:</p> <pre><code>def sum(l:List[Int]):Int = {\n    l match {\n        case Nil =&gt; 0\n        case (hd::tl) =&gt; hd + sum(tl)\n    }\n}\n</code></pre> <p>in which <code>l match {case Nil =&gt; 0; case (hd::tl) =&gt; hd+sum(tl) }</code> denotes a pattern-matching expression in Scala. It is similar to the switch statement found in other main stream languages, except that it has more perks.</p> <p>In this expression, we pattern match the input list <code>l</code> against two list patterns, namely:</p> <ul> <li><code>Nil</code> the empty list, and</li> <li><code>(hd::tl)</code> the non-empty list</li> </ul> <p>Note that here <code>Nil</code> and <code>hd::tl</code> are not list values, because they are appearing after a <code>case</code> keyword and on the left of a thick arrow <code>=&gt;</code>.</p> <p>Pattern cases are visited from top to bottom (or left to right). In this example, we first check whether the input list <code>l</code> is an empty list. If it is empty, the sum of an empty list must be <code>0</code>. </p> <p>If the input list <code>l</code> is not an empty list, it must have at least one element. The pattern <code>(hd::tl)</code> extracts the first element of the list and binds it to a local variable <code>hd</code> and the remainder (which is the sub list formed by taking away the first element from <code>l</code>) is bound to <code>hd</code>. We often call <code>hd</code> as the head of the list and <code>tl</code> as the tail. We would like to remind that <code>hd</code> is storing a single integer in this case, and <code>tl</code> is capturing a list of integers.</p> <p>One advantage of implementing the <code>sum</code> function in FP style is that it is much closer to its math specification.</p> \\[ \\begin{array}{rl} sum(l) = &amp; \\left [     \\begin{array}{ll}     0 &amp; {l\\ is\\ empty} \\\\     head(l)+sum(tail(l)) &amp; {otherwise}     \\end{array} \\right . \\end{array} \\] <p>Let's consider another example.</p> <pre><code>def reverse(l:List[Int]):List[Int] = l match {\n    case Nil =&gt; Nil\n    case (hd::tl) =&gt; reverse(tl) ++ List(hd)\n}\n</code></pre> <p>The function <code>reverse</code> takes a list of integers and generates a new list which is in the reverse order of the orginal one. We apply a similar strategy to break down the problem into two sub-problems via the <code>match</code> expression.</p> <ul> <li>When the input list <code>l</code> is an empty list, we return an empty list. The reverse of an empty list is an empty list</li> <li>When the input <code>l</code> is not empty, we make use of the pattern <code>(hd::tl)</code> to extract the head and the tail of the list</li> </ul> <p>We apply <code>reverse</code> recursively to the tail and then concatenate it with a list containing the head.</p> <p>You may notice that the same <code>reverse</code> function can be applied to lists of any element type, and not just integers, as long as all elements in a list share the same type. Therefore, we can rewrite the <code>reverse</code> function into a generic version as follows:</p> <pre><code>def reverse[A](l:List[A]):List[A] = l match {\n    case Nil =&gt; Nil\n    case (hd::tl) =&gt; reverse(tl) ++ List(hd)\n}\n</code></pre> <p>Note that the first <code>[A]</code> denotes a type argument, with which we specify that the element type of the list is <code>A</code> (any possible type). The type argument is resolved when we apply <code>reverse</code> to a actual argument. For instance in <code>reverse(List(1,2,3))</code> the Scala compiler will resolve <code>A=Int</code> and in <code>reverse(List(\"a\",\"b\"))</code> it will resolve <code>A=String</code>.</p>"},{"location":"fp_scala/#a-note-on-recursion","title":"A Note on Recursion","text":"<p>Note that recursive calls to <code>reverse</code> will incur additional memory space in the machine in form of additional function call frames on the call stack.</p> <p>A call stack frame has to created to \"save\" the state of function execution such as local variables. As nested recursive calls are being built up, the machine might run out of memory. This is also known as Stack Overflow Error.</p> <p>While simple recursions that make a few tens of or hundreds of nested calls won't harm a lot, we need to rethink when we note that a recursion is going to be executed for a large number of iterations. One way to address this issue is to rewrite non-tail recursion into tail-recursion.</p> <p>A tail-recursion is a recursive function in which the recursive call occurs at the last instruction. </p> <p>For instance, the <code>reverse()</code> function presented earlier is not. The following variant is a tail recursion</p> <pre><code>def reverse[A](l:List[A]):List[A] = {\n    def go(i:List[A], o:List[A]) : List[A] = i match {\n        case Nil =&gt; o\n        case (x::xs) =&gt; go(xs, x::o)\n    }\n    go(l,Nil)\n}\n</code></pre> <p>In the above definition, we rely on an inner function <code>go</code> which is a recursive function. In <code>go</code>, the recursion take places at the last instruction in the <code>(x::xs)</code> case. The trick is to pass around an accumulated output <code>o</code> in each recursive call.</p> <p>As compiler technology evolves, many modern FP language compilers are able to detect a subset of non-tail recursions and automatically transform them into the tail recursive version. </p> <p>However Scala does not automatically re-write a non-tail recursion into a tail recursion. Instead it offers a check:</p> <pre><code>import scala.annotation.tailrec\n\ndef reverse[A](l:List[A]):List[A] = {\n    @tailrec\n    def go(i:List[A], o:List[A]) : List[A] = i match {\n        case Nil =&gt; o\n        case (x::xs) =&gt; go(xs, x::o)\n    }\n    go(l,Nil)\n}\n</code></pre> <p>The annotation <code>tailrec</code> is to hint to the Scala compiler that <code>go</code> should be compiled in a way that no stack frame should be created. If the compiler fails to do that, it will complain. In the absence of the <code>tailrec</code> annotation, the compiler will still try to optimize the tail recursion. </p> <p>If we apply the <code>tailrec</code> annotation to a non-tail recursive function, Scala will complain.</p> <pre><code>@tailrec\ndef reverse[A](l:List[A]):List[A] = l match {\n    case Nil =&gt; Nil\n    case (hd::tl) =&gt; reverse(tl) ++ List(hd)\n}\n</code></pre> <p>The following error is reported: <pre><code>-- Error: ----------------------------------------------------------------------\n4 |    case (hd::tl) =&gt; reverse(tl) ++ List(hd)\n  |                     ^^^^^^^^^^^\n  |                 Cannot rewrite recursive call: it is not in tail position\n1 error found\n</code></pre></p>"},{"location":"fp_scala/#map-fold-and-filter","title":"Map, Fold and Filter","text":"<p>Consider the following function</p> <pre><code>def addToEach(x:Int, l:List[Int]):List[Int] = l match {\n    case Nil =&gt; Nil\n    case (y::ys) =&gt; {\n        val yx = y+x\n        yx::addToEach(x,ys)\n    }\n}\n</code></pre> <p>It takes two inputs, an integer <code>x</code> and an integer list <code>l</code>, and adds <code>x</code> to every element in <code>l</code> and put the results in the output list.</p> <p>For instance <code>addToEach(1, List(1,2,3))</code> yields <code>List(2,3,4)</code>.</p> <p>The above can rewritten by using a generic library method shipped with Scala.</p> <pre><code>def addToEach(x:Int, l:List[Int]):List[Int] = l.map(y=&gt;y+x)\n</code></pre> <p>The method <code>map</code> is a method of the list class that takes a function as input argument and applies it to all elements in the list object.</p> <p>Note that the above is same as</p> <pre><code>def addToEach(x:Int, l:List[Int]):List[Int] = {\n    def addX(y:Int):Int = y+x\n    l.map(addX)\n}\n</code></pre> <p>We can observe that the input list and the output list of the <code>map</code> method must be of the same type and have the same length.</p> <p>Recall in the <code>sum</code> function introduced in the earlier section. It takes a list of integers and \"collapses\" them into one number by summation. We can rewrite it using a fold function.</p> <pre><code>def sum(l:List[Int]):Int = l.foldLeft(0)((acc,x)=&gt; acc+x)\n</code></pre> <p>The <code>foldLeft</code> method takes a base accumulator, and a binary function as inputs, and aggregates the elements from the list using the binary function.  In particular, the binary aggreation function assumes the first argument is the accumulator.</p> <p>Besides <code>foldLeft</code>, there exists a <code>foldRight</code> method, in which the binary aggregation function expects the second argument is the accumulator.</p> <pre><code>def sum(l:List[Int]):Int = l.foldRight(0)((x,acc)=&gt; x+acc)\n</code></pre> <p>So what is the difference between <code>foldLeft</code> and <code>foldRight</code>?  What happen if you run the following? Can you explain the difference?</p> <pre><code>val l = List(\"a\",\"better\",\"world\", \"by\", \"design\")\nl.foldLeft(\"\")((acc,x) =&gt; (acc+\" \"+x)) \nl.foldRight(\"\")((x,acc) =&gt; (x+\" \"+acc))\n</code></pre> <p>Note that <code>+</code> is an overloaded operator. In the above it concatenates two string values.</p> <p>Intuitively, <code>l.foldLeft(\"\")((acc,x) =&gt; (acc+\" \"+x))</code> aggregates the list of words using the aggregation function by nesting the recursive calls to the left.</p> <pre><code>((((\"\"+\" \"+\"a\")+\" \"+\"better\")+\" \"+\"world\")+\" \"+\"by\")+\" \"+\"design\"\n</code></pre> <p>where <code>l.foldRight(\"\")((x,acc) =&gt; (x+\" \"+acc))</code> aggregates the list of words by nesting the recursive calls to the right.</p> <pre><code>\"a\"+\" \"+(\"better\"+\" \"+(\"world\"+\" \"+(\"by\"+\" \"+(\"design\"+\" \"+\"\"))))\n</code></pre> <p>The method <code>filter</code> takes a boolean test function and applies it to the elements in the list, keeping those whose test result is true and dropping those whose result is false.</p> <pre><code>val l = List(1,2,3,4)\ndef even(x:Int):Boolean = x%2==0\nl.filter(even)\n</code></pre> <p>returns <code>List(2,4)</code>.</p> <pre><code>val l = List('a','1','0','d')\nl.filter((c:Char) =&gt; c.isDigit)\n</code></pre> <p>returns <code>List('1','0')</code>.</p> <p>With <code>map</code>, <code>foldLeft</code> and <code>filter</code>, we can express the implementation of algorithms in a concise and elegant way. For instance, the following function implements the quicksort algorithm:</p> <pre><code>def qsort(l:List[Int]):List[Int] = l match {\n    case Nil =&gt; Nil\n    case List(x) =&gt; List(x)\n    case (p::rest) =&gt; {\n        val ltp = rest.filter( x =&gt; x &lt; p)\n        val gep = rest.filter( x =&gt; !(x &lt; p))\n        qsort(ltp) ++ List(p) ++ qsort(gep)\n    }\n}\n</code></pre> <p>which resembles the math specification</p> \\[ \\begin{array}{cc} qsort(l) = &amp; \\left[     \\begin{array}{ll}     l &amp; |l| &lt; 2 \\\\     qsort(\\{x|x \\in l \\wedge x &lt; head(l) \\}) \\uplus \\{head(l)\\} \\uplus qsort(\\{x|x\\in l \\wedge \\neg(x &lt; head(l)) \\}) &amp; otherwise     \\end{array} \\right . \\end{array} \\] <p>where \\(\\uplus\\) unions two bags and maintains the order.</p>"},{"location":"fp_scala/#flatmap-and-for-comprehension","title":"flatMap and for-comprehension","text":"<p>There is a variant of <code>map</code> method, consider</p> <pre><code>val l = (1 to 5).toList\nl.map( i =&gt; if (i%2 ==0) { List(i) } else { Nil })\n</code></pre> <p>would yield</p> <pre><code>List(List(), List(2), List(), List(4), List())\n</code></pre> <p>We would like to get rid of the nested lists and flatten the outer list. </p> <p>One possibility is to:</p> <pre><code>l.flatMap( i =&gt; if (i%2 ==0) { List(i) } else { Nil })\n</code></pre> <p>Like <code>map</code>, <code>flatMap</code> applies its parameter function  to every element in the list. Unlike <code>map</code>, <code>flatMap</code> expects the parameter function produces a list, thus it will join all the sub-lists into one list.</p> <p>With <code>map</code> and <code>flatMap</code>, we can define complex list transformation operations like the following:</p> <pre><code>def listProd[A,B](la:List[A], lb:List[B]):List[(A,B)] = \n    la.flatMap( a =&gt; lb.map(b =&gt; (a,b)))\n\nval l2 = List('a', 'b', 'c')\nlistProd(l, l2)\n</code></pre> <p>which produces:</p> <pre><code>List((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c), (4,a), (4,b), (4,c), (5,a), (5,b), (5,c))\n</code></pre> <p>Note that Scala supports list comprehension via the <code>for ... yield</code> construct. We could re-express <code>listProd</code> as follows:</p> <pre><code>def listProd2[A,B](la:List[A], lb:List[B]):List[(A,B)] = \n    for {\n        a &lt;- la\n        b &lt;- lb\n    } yield (a,b)\n</code></pre> <p>The Scala compiler desugars:</p> <pre><code>for { x1 &lt;- e1;  x2 &lt;- e2; ...; xn &lt;- en } yield e\n</code></pre> <p>into:</p> <pre><code>e1.flatMap( x1 =&gt; e2.flatMap(x2 =&gt;  ... en.map( xn =&gt; e) ... ))\n</code></pre> <p>The above syntactic sugar not only works for the list data type but any data type with <code>flatMap</code> and <code>map</code> defined (as we will see in the upcoming lessons).</p> <p>In its general form, we refer to it as for-comprehension. One extra note to take is that the for-comprehension should not be confused with the for-loop statement exists in the imperative style programming in Scala.</p> <pre><code>var sum = 0\nfor (i &lt;- 1 to 10)\n   {sum = sum + i}\nprintln(sum)\n</code></pre>"},{"location":"fp_scala/#algebraic-datatype","title":"Algebraic Datatype","text":"<p>Like many other languages, Scala supports user defined data type. From an earlier section, we have discussed how to use classes and traits in Scala to define data types, making using of the OOP concepts that we have learned.</p> <p>This style of defining data types using abstraction and encapsulation is also known as the abstract datatype.</p> <p>In this section, we consider an alternative, the Algebraic Datatype.</p> <p>Consider the following Extended BNF of a math expression.</p> <p>In computer science, extended Backus\u2013Naur form (EBNF) is a family of metasyntax notations, any of which can be used to express a context-free grammar. EBNF is used to make a formal description of a formal language such as a computer programming language. EBNF.</p> \\[ \\begin{array}{rccl} {\\tt (Math Exp)} &amp; e &amp; ::= &amp; e + e \\mid e - e \\mid  e * e \\mid e / e \\mid c \\\\ {\\tt (Constant)} &amp; c &amp; ::= &amp; ... \\mid -1 \\mid 0 \\mid 1 \\mid ... \\end{array} \\] <p>And we would like to implement a function <code>eval()</code> which evaluates a \\({\\tt (Math Exp)}\\) to a value.</p> <p>If we were to implement the above with OOP, we would probably use inheritance to extend subclasses of \\({\\tt (Math Exp)}\\), and use if-else statements with <code>instanceof</code> to check for a specific subclass instance. Alternatively, we can also rely on visitor pattern or delegation.</p> <p>It turns out that using Abstract Datatypes to model the above result in some engineering overhead.</p> <ul> <li>Firstly, encapsulation and abstract tend to hide the underlying structure of the given object (in this case, the \\({\\tt Math Exp})\\) terms)</li> <li>Secondly, using inheritance to model the sum of data types is not perfect (Note: the \"sum\" here refers to having a fixed set of alternatives of a datatype, not the summation for numerical values)</li> <li>For instance, there is no way to stop users of the library code from extending new instances of \\({\\tt (MathExp)}\\)</li> </ul> <p>The algebraic datatype is an answer to these issues. In essence, it is a type of data structure that consists of products and sums.</p> <p>In Scala 3, it is recommended to use <code>enum</code> for Algebraic datatypes.</p> <pre><code>enum MathExp:\n    case Plus(e1:MathExp, e2:MathExp)\n    case Minus(e1:MathExp, e2:MathExp)\n    case Mult(e1:MathExp, e2:MathExp)\n    case Div(e1:MathExp, e2:MathExp)\n    case Const(v:Int)\nend MathExp\n</code></pre> <p>In the above the <code>MathExp</code> (<code>enum</code>) datatype, there are exactly 5 alternatives. Let's take at look at one case, for instance <code>Plus(e1:MathExp, e2:MathExp)</code>, which states that a plus expression has two operands, both of which are of type <code>MathExp</code>.</p> <p>Note that the <code>end MathExp</code> is optional, as long as there is an extra line. Alternatively, we can use <code>{ }</code>.</p> <pre><code>enum MathExp {\n    case Plus(e1:MathExp, e2:MathExp)\n    case Minus(e1:MathExp, e2:MathExp)\n    case Mult(e1:MathExp, e2:MathExp)\n    case Div(e1:MathExp, e2:MathExp)\n    case Const(v:Int)\n}\n</code></pre> <p>We can represent the math expression <code>(1+2) * 3</code> as <code>MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3))</code>.  Note that we call <code>Plus(_,_)</code> , <code>Minus(_,_)</code>, <code>Mult(_,_)</code>, <code>Div(_,_)</code> and <code>Const(_)</code> \"data constructors\", as we use them to construct values of the <code>enum</code> algebraic datatype <code>MathExp</code>.</p> <p>Next let's implement an evaluation function based the specification:</p> \\[ eval(e) = \\left [ \\begin{array}{cl}                 eval(e_1) + eval(e_2) &amp; if\\ e = e_1+e_2 \\\\                 eval(e_1) - eval(e_2) &amp; if\\ e = e_1-e_2 \\\\                 eval(e_1) * eval(e_2) &amp; if\\ e = e_1*e_2 \\\\                 eval(e_1) / eval(e_2) &amp; if\\ e = e_1/e_2 \\\\                 c &amp; if\\ e = c                 \\end{array}         \\right. \\] <pre><code>def eval(e:MathExp):Int = e match {\n    case MathExp.Plus(e1, e2)  =&gt; eval(e1) + eval(e2)\n    case MathExp.Minus(e1, e2) =&gt; eval(e1) - eval(e2)\n    case MathExp.Mult(e1, e2)  =&gt; eval(e1) * eval(e2)\n    case MathExp.Div(e1, e2)   =&gt; eval(e1) / eval(e2)\n    case MathExp.Const(i)      =&gt; i\n}\n</code></pre> <p>In Scala, an <code>enum</code> algebraic datatype value can be accessed (deconstructured) via pattern matching.</p> <p>If we run:</p> <pre><code>eval(MathExp.Mult(MathExp.Plus(MathExp.Const(1), MathExp.Const(2)), MathExp.Const(3)))\n</code></pre> <p>we get <code>9</code> as result.</p> <p>Let's consider another example where we can implement some real-world data structures using the algebraic datatype.</p> <p>Suppose for experimental purposes, we would like to re-implement the list datatype in Scala (even though a builtin one already exists). For simplicity, let's consider a monomorphic version (no generic) version. </p> <p>We will look into the generic version in the next lesson</p> <p>In the following we consider the specification of the <code>MyList</code> data type in EBNF:</p> \\[ \\begin{array}{rccl} {\\tt (MyList)} &amp; l &amp; ::= &amp; Nil \\mid Cons(i,l) \\\\ {\\tt (Int)} &amp; i &amp; ::= &amp; 1 \\mid 2 \\mid   ... \\end{array} \\] <p>And we implement using <code>enum</code> in Scala:</p> <pre><code>enum MyList {\n    case Nil\n    case Cons(x:Int, xs:MyList)\n}\n</code></pre> <p>Next we implement the <code>map</code> function based on the following specification</p> \\[ map(f, l) = \\left [ \\begin{array}{ll}             Nil &amp; if\\ l = Nil\\\\             Cons(f(hd), map(f, tl)) &amp; if\\ l = Cons(hd, tl)             \\end{array} \\right . \\] <p>Then we could implement the map function</p> <pre><code>def mapML(f:Int=&gt;Int, l:MyList):MyList = l match {\n    case MyList.Nil =&gt; MyList.Nil\n    case MyList.Cons(hd, tl) =&gt; MyList.Cons(f(hd), mapML(f,tl))\n}\n</code></pre> <p>Running <code>mapML(x =&gt; x+1, MyList.Cons(1,MyList.Nil))</code> yields <code>MyList.Cons(2,MyList.Nil)</code>.</p> <p>But hang on a second! The <code>map</code> method from the Scala built-in list is a method of a list object, not a stand-alone function.</p> <p>In Scala 3, <code>enum</code> allows us to package the method inside <code>enum</code> values.</p> <pre><code>enum MyList {\n    case Nil\n    case Cons(x:Int, xs:MyList)\n    def mapML(f:Int=&gt;Int):MyList = this match {\n        case MyList.Nil =&gt; MyList.Nil\n        case MyList.Cons(hd, tl) =&gt; MyList.Cons(f(hd), tl.mapML(f))\n    }\n}\n</code></pre> <p>Running:</p> <pre><code>val l = MyList.Cons(1, MyList.Nil)\nl.mapML(x=&gt; x+1)\n</code></pre> <p>yields the same output as above.</p>"},{"location":"fp_scala/#summary","title":"Summary","text":"<p>In this lesson, we have discussed</p> <ul> <li>Scala's OOP vs Java's OOP</li> <li>Scala's FP vs Lambda Calculus</li> <li>How to use the <code>List</code> datatype to model and manipulate collections of multiple values.</li> <li>How to use Algebraic data type to define user customized data type to solve complex problems.</li> </ul>"},{"location":"fp_scala_poly/","title":"50.054 - Parametric Polymorphism and Adhoc Polymorphism","text":""},{"location":"fp_scala_poly/#learning-outcomes","title":"Learning Outcomes","text":"<p>By this end of this lesson, you should be able to </p> <ul> <li>develop parametrically polymorphic Scala code using Generic, Algebraic Datatype</li> <li>safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes </li> <li>develop generic programming style code using <code>Functor</code> type class.</li> <li>make use of <code>Option</code> and <code>Either</code> to handle and manipulate errors and exceptions. </li> </ul>"},{"location":"fp_scala_poly/#currying","title":"Currying","text":"<p>In functional programming, we could rewrite a function with multiple arguments into a function that takes the first argument and returns another function that takes the remaining arguments.</p> <p>For example,</p> <pre><code>def sum(x:Int, y:Int):Int = x + y\n</code></pre> <p>can be rewritten into </p> <pre><code>def sum_curry(x:Int)(y:Int):Int = x + y\n</code></pre> <p>These two functions are equivalent except that</p> <ol> <li>Their invocations are different, e.g.  <pre><code>sum(1,2)\nsum_curry(1)(2)\n</code></pre></li> <li>It is easier to reuse the curried version to define other function, e.g. <pre><code>def plus1(x:Int):Int = sum_curry(1)(x)\n</code></pre></li> </ol>"},{"location":"fp_scala_poly/#function-composition","title":"Function Composition","text":"<p>Every function and method in Scala is an object with a <code>.compose()</code> method. It works like the mathmethical composition.</p> <p>In math, let \\(g\\) and \\(f\\) be functions, then</p> \\[ (g \\circ f)(x) \\equiv g(f(x)) \\] <p>Let <code>g</code> and <code>f</code> be Scala functions (or methods), then</p> <p><pre><code>g.compose(f)\n</code></pre> is equivalent to  <pre><code>x =&gt; g(f(x))\n</code></pre></p> <p>For example</p> <pre><code>def f(x:Int):Int = 2 * x + 3\ndef g(x:Int):Int = x * x\n\nassert((g.compose(f))(2) == g(f(2)))\n</code></pre>"},{"location":"fp_scala_poly/#generics","title":"Generics","text":"<p>Generics is also known as type variables. It enables a language to support parametric polymoprhism. </p>"},{"location":"fp_scala_poly/#polymorphic-functions","title":"Polymorphic functions","text":"<p>Recall that the <code>reverse</code> function introduced in the last lesson <pre><code>def reverse(l:List[Int]):List[Int] = l match {\n    case Nil =&gt; Nil\n    case (hd::tl) =&gt; reverse(tl) ++ List(hd)\n}\n</code></pre></p> <p>We argue that the same implementation should work for all lists regardless of their elements' type. Thus, we would replace <code>Int</code> by a type variable <code>A</code>.</p> <pre><code>def reverse[A](l:List[A]):List[A] = l match {\n    case Nil =&gt; Nil\n    case (hd::tl) =&gt; reverse(tl) ++ List(hd)\n}\n</code></pre>"},{"location":"fp_scala_poly/#polymorphic-algebraic-datatype","title":"Polymorphic Algebraic Datatype","text":"<p>Recall that the following Algebraic Datatype from the last lesson. </p> <pre><code>enum MyList {\n    case Nil\n    case Cons(x:Int, xs:MyList)\n}\n\ndef mapML(l:MyList, f:Int =&gt; Int):MyList = l match {\n    case MyList.Nil =&gt; MyList.Nil\n    case MyList.Cons(hd, tl) =&gt; MyList.Cons(f(hd), mapML(tl, f))\n}\n</code></pre> <p>Same observation applies. <code>MyList</code> could have a generic element type <code>A</code> instead of <code>Int</code> and <code>mapML</code> should remains unchanged.</p> <pre><code>enum MyList[A] {\n    case Nil // type error\n    case Cons(x:A, xs:MyList[A])\n}\n\ndef mapML[A,B](l:MyList[A], f:A =&gt; B):MyList[B] = l match {\n    case MyList.Nil =&gt; MyList.Nil\n    case MyList.Cons(hd, tl) =&gt; MyList.Cons(f(hd), mapML( tl, f))\n}\n</code></pre> <p>The caveat here is that the Scala compiler would complain about the <code>Nil</code> case above </p> <pre><code>-- Error: ----------------------------------------------------------------------\n2 |    case Nil\n  |    ^^^^^^^^\n  |    cannot determine type argument for enum parent class MyList,\n  |    type parameter type A is invariant\n1 error found\n</code></pre> <p>To understand that error, we need to understand how Scala desugar the enum datatype.  The above <code>MyList</code> datatype is desugared as </p> <p><pre><code>enum MyList[A] {\n    case Nil extends MyList[Nothing] // type error\n    case Cons(x:A, xs:MyList[A]) extends MyList[A]\n}\n</code></pre> In which all sub cases within the enum type must be sub-class of the enum type.  However it is not trivial for <code>Nil</code>. It can't be declared as a subtype of <code>MyList[A]</code> since type variable <code>A</code> is not mentioned in its definition, unlike <code>Cons(x:A, xs:MyList[A])</code>. The best we can get is <code>MyList[Nothing]</code> where <code>Nothing</code> is the subtype of all other types in Scala. (As the dual, <code>Any</code> is the supertype of all other types in Scala). We are getting very close. Now we know that <code>Nil extends MyList[Nothing]</code>. If we can argue that <code>MyList[Nothing] extends MyList[A]</code> then we are all good. For <code>MyList[Nothing] extends MyList[A]</code> to hold,  <code>A</code> must be covariant type parameter.</p> <p>In type system with subtyping, </p> <ul> <li> <p>a type is covariant if it preserves the subtyping order when it is applied a type constructor. In the above situation, <code>MyList[_]</code> is a type constructor. The type parameter <code>A</code> is covarient because we note <code>Nothing &lt;: A</code> for all <code>A</code>, thus <code>MyList[Nothing] &lt;: MyList[A]</code>. </p> </li> <li> <p>a type is contravariant if it reverses the subtyping order when it is applied to a type constructor. For instance, given function type <code>A =&gt; Boolean</code>, the type parameter <code>A</code> is contravariant, because for <code>A &lt;: B</code>, we have <code>B =&gt; Boolean &lt;: A =&gt; Boolean</code>. (We can use functions of type <code>B =&gt; Boolean</code> in the context where a function <code>A =&gt; Boolean</code> is expected, but not the other way round.)</p> </li> <li> <p>a type is invariant if it does not preserve nor reverse the subtyping order when it is applied to a type constructor. </p> </li> </ul> <p>Hence to fix the above type error with the <code>MyList[A]</code> datatype, we declared that <code>A</code> is covariant, <code>+A</code>. </p> <p><pre><code>enum MyList[+A] {\n    case Nil // type error is fixed.\n    case Cons(x:A, xs:MyList[A])\n}\n\ndef mapML[A,B](l:MyList[A])(f:A =&gt; B):MyList[B] = l match {\n    case MyList.Nil =&gt; MyList.Nil\n    case MyList.Cons(hd, tl) =&gt; MyList.Cons(f(hd), mapML(tl)(f))\n}\n</code></pre> For easy of reasoning, we also rewrite <code>mapML</code> into currying style.</p> <p>Recall that we could make <code>mapML</code> function as a method of <code>MyList</code></p> <pre><code>enum MyList[+A] {\n    case Nil\n    case Cons(x:A, xs:MyList[A])\n    def mapML[B](f:A=&gt;B):MyList[B] = this match { \n        case MyList.Nil =&gt; MyList.Nil\n        case MyList.Cons(hd, tl) =&gt; MyList.Cons(f(hd), tl.mapML(f))\n    }\n}\n</code></pre> <ul> <li>Scala Variances</li> </ul>"},{"location":"fp_scala_poly/#type-class","title":"Type class","text":"<p>Suppose we would like to convert some Scala values to JSON strings.</p> <p>We could rely on overloading.</p> <pre><code>def toJS(v:Int):String = v.toString\ndef toJS(v:String):String = s\"'${v}'\"\ndef toJS(v:Boolean):String = v.toString\n</code></pre> <p>Given <code>v</code> is a Scala string value, <code>s\"some_prefix ${v} some_suffix\"</code> denotes a Scala string interpolation, which inserts <code>v</code>'s content into the \"place holder\" in the string <code>\"some_prefix ${v} some_suffix\"</code> where the  <code>${v}</code> is the place holder.</p> <p>However this becomes hard to manage as we consider complex datatype.</p> <pre><code>enum Contact {\n    case Email(e:String)\n    case Phone(ph:String)\n}\n\nimport Contact.*\ndef toJS(c:Contact):String = c match {\n    case Email(e) =&gt; s\"{'email': ${toJS(e)}}\" // compilation error\n    case Phone(ph) =&gt; s\"{'phone': ${toJS(ph)}}\" // compilation error\n}\n</code></pre> <p>When we try to define the <code>toJS</code> function for <code>Contact</code> datatype, we can't make use of the <code>toJS</code> function for string value because the compiler is confused that we are trying to make recursive calls. This is the first issue we faced.</p> <p>Let's pretend that the first issue has been addressed. There's still another issue.</p> <p>Consider</p> <pre><code>case class Person(name:String, contacts:List[Contact])\ncase class Team(members:List[Person])\n</code></pre> <p>A <code>case class</code> is like a normal class we have seen earlier except that we can apply pattern matching to its values.  Let's continue to overload <code>toJS</code> to handle <code>Person</code> and <code>Team</code>. </p> <pre><code>def toJS(p:Person):String = p match {\n    case Person(name, contacts) =&gt; s\"{'person':{ 'name':${toJS(name)},  'contacts':${toJS(contacts)}}}\"\n}\ndef toJS(cs:List[Contact]):String = {\n    val j = cs.map(c=&gt;toJS(c)).mkString(\",\")\n    s\"[${j}]\"\n}\n\ndef toJS(t:Team):String = t match {\n    case Team(members) =&gt; s\"{'team':{ 'members':${toJS(members)}}}\"\n}\n\ndef toJS(ps:List[Person]):String = {\n    val j = ps.map(p=&gt;toJS(p)).mkString(\",\")\n    s\"[${j}]\"\n}\n</code></pre> <p>The second issue is that the <code>toJS(cs:List[Contact])</code> and <code>toJS(ps:List[Person])</code> are the identical modulo the variable names. Can we combine two into one?</p> <p><pre><code>def toJS[A](vs:List[A]):String = {\n        val j = vs.map(v=&gt;toJS(v)).mkString(\",\") // compiler error\n    s\"[${j}]\"\n}\n</code></pre> However a compilation error occurs because the compiler is unable to resolve the <code>toJS[A](v:A)</code> used in the <code>.map()</code>.</p> <p>It seems that we need to give some extra information to the compiler so that it knows that when we use the above generic <code>toJS</code> we are referring to either <code>Person</code> or <code>Contact</code>, or whatever type that has a <code>toJS</code> defined.</p> <p>One solution to address the two above issues is to use type class. In Scala 3, a type class is defined by a polymoprhic trait and a set of type class instances. </p> <pre><code>trait JS[A] {\n    def toJS(v:A):String\n}\n\ngiven toJSInt:JS[Int] = new JS[Int]{ \n    def toJS(v:Int):String = v.toString\n}\n\ngiven toJSString:JS[String] = new JS[String] {\n    def toJS(v:String):String = s\"'${v}'\"\n}\n\ngiven toJSBoolean:JS[Boolean] = new JS[Boolean] {\n    def toJS(v:Boolean):String = v.toString\n}\n\ngiven toJSContact(using jsstr:JS[String]):JS[Contact] = new JS[Contact] {\n    import Contact.*\n    def toJS(c:Contact):String = c match {\n        case Email(e) =&gt; s\"{'email': ${jsstr.toJS(e)}}\" // compilation error is fixed\n        case Phone(ph) =&gt; s\"{'phone': ${jsstr.toJS(ph)}}\" // compilation error is fixed\n    }\n}\n\ngiven toJSPerson(using jsstr:JS[String], jsl:JS[List[Contact]]):JS[Person] = new JS[Person] {\n    def toJS(p:Person):String = p match {\n        case Person(name, contacts) =&gt; s\"{'person':{ 'name':${jsstr.toJS(name)},  'contacts':${jsl.toJS(contacts)}}}\"\n    }\n}\n\ngiven toJSTeam(using jsl:JS[List[Person]]):JS[Team] = new JS[Team] {\n    def toJS(t:Team):String = t match {\n        case Team(members) =&gt; s\"{'team':{ 'members':${jsl.toJS(members)}}}\"\n    }\n}\n\ngiven toJSList[A](using jsa:JS[A]):JS[List[A]] = new JS[List[A]] {\n    def toJS(as:List[A]):String = {\n        val j = as.map(a=&gt;jsa.toJS(a)).mkString(\",\")\n        s\"[${j}]\"\n    }\n}\n</code></pre> <p><code>given</code> defines a type class instance. An instance consists of a name and the context parameters (those with <code>using</code>) and instance type. In the body of the type class instance, we instantiate an anonymous object that extends type class with the specific type and provide the defintion. We can refer to the particular type class instance by the instance's name. For instance</p> <pre><code>import Contact.*\nval myTeam = Team( List(\n    Person(\"kenny\", List(Email(\"kenny_lu@sutd.edu.sg\"))), \n    Person(\"simon\", List(Email(\"simon_perrault@sutd.edu.sg\")))\n))\n</code></pre> <p><code>toJSTeam.toJS(myTeam)</code> yields</p> <pre><code>'team':{ 'members':['person':{ 'name':'kenny',  'contacts':['email': 'kenny_lu@sutd.edu.sg'] },'person':{ 'name':'simon',  'contacts':['email': 'simon_perrault@sutd.edu.sg'] }] }\n</code></pre> <p>We can also refer to the type class instance by the instace's type. For example, recall the last two instances. In the context of the <code>toJSTeam</code>, we refer to another instance of type <code>JS[List[Person]]</code>. Note that none of the defined instances has the required type. Scala is smart enought to synthesize it from the instances of <code>toJSList</code> and <code>toJSPerson</code>.  Given the required type class instance is <code>JS[List[Person]]</code>, the type class resolver finds the instance <code>toJSList</code> having type <code>JS[List[A]]</code>, and it unifies both and find that <code>A=Person</code>. In the context of the instance <code>toJSList</code>, <code>JS[A]</code> is demanded. We can refine the required instance's type as <code>JS[Person]</code>, which is <code>toJSPerson</code>.</p> <p>Note that when we call a function that requires a type class context, we do not need to provide the argument for the type class instance. </p> <pre><code>def printAsJSON[A](v:A)(using jsa:JS[A]):Unit = {\n    println(jsa.toJS(v))\n}\n\nprintAsJSON(myTeam)\n</code></pre> <p>Type class enables us to develop modular and resusable codes. It is related to a topic of Generic Programming. In computer programming, generic programming refers to the coding approach which an instance of code is written once and used for many different types/instances of values/objects.</p> <p>In the next few sections, we consider some common patterns in FP that are promoting generic programming.</p>"},{"location":"fp_scala_poly/#functor","title":"Functor","text":"<p>Recall that we have a <code>map</code> method for list datatype. </p> <pre><code>val l = List(1,2,3)\nl.map(x =&gt; x + 1)\n</code></pre> <p>Can we make <code>map</code> to work for other data type? For example</p> <pre><code>enum BTree[+A] {\n    case Empty\n    case Node(v:A, lft:BTree[A], rgt:BTree[A]) \n}\n</code></pre> <p>It turns out that extending <code>map</code> to different datatypes is similar to <code>toJS</code> function that we implemented earlier. We consider introducing a type class for this purpose.</p> <p><pre><code>trait Functor[T[_]] {\n    def map[A,B](t:T[A])(f:A =&gt; B):T[B]\n}\n</code></pre> In the above type class definition, <code>T[_]</code> denotes a polymorphic type that of kind <code>* =&gt; *</code>. A kind is a type of types. In the above, it means <code>Functor</code> takes any type constructors <code>T</code>. When <code>T</code> is instantiated, it could be <code>List[_]</code> or <code>BTree[_]</code> and etc. (C.f. In the type class <code>JS[A]</code>, the type argument has kind <code>*</code>.)</p> <pre><code>given listFunctor:Functor[List] = new Functor[List] {\n    def map[A,B](t:List[A])(f:A =&gt; B):List[B] = t.map(f)\n}\n\ngiven btreeFunctor:Functor[BTree] = new Functor[BTree] {\n    import BTree.*\n    def map[A,B](t:BTree[A])(f:A =&gt; B):BTree[B] = t match {\n        case Empty =&gt; Empty\n        case Node(v, lft, rgt) =&gt; Node(f(v), map(lft)(f), map(rgt)(f))\n    }\n}\n</code></pre> <p>Some example</p> <pre><code>val l = List(1,2,3)\nlistFunctor.map(l)((x:Int) =&gt; x + 1)\n\nval t = BTree.Node(2, BTree.Node(1, BTree.Empty, BTree.Empty), BTree.Node(3, BTree.Empty, BTree.Empty))\nbtreeFunctor.map(t)((x:Int) =&gt; x + 1)\n</code></pre>"},{"location":"fp_scala_poly/#functor-laws","title":"Functor Laws","text":"<p>All instances of functor must obey a set of mathematic laws for their computation to be predictable.</p> <p>Let <code>i</code> be a functor instance 1. Identity: <code>i =&gt; map(i)(x =&gt; x)</code> \\(\\equiv\\) <code>x =&gt; x</code>. When performing the mapping operation, if the values in the functor are mapped to themselves, the result will be an unmodified functor. 2. Composition Morphism: <code>i=&gt; map(i)(f.compose(g))</code> \\(\\equiv\\) <code>(i =&gt; map(i)(f)).compose(j =&gt; map(j)(g))</code>. If two sequential mapping operations are performed one after the other using two functions, the result should be the same as a single mapping operation with one function that is equivalent to applying the first function to the result of the second.</p>"},{"location":"fp_scala_poly/#foldable","title":"Foldable","text":"<p>Similarly we can define a <code>Foldable</code> type class for generic and overloaded <code>foldLeft</code>( and <code>foldRight</code>).</p> <pre><code>trait Foldable[T[_]]{\n    def foldLeft[A,B](t:T[B])(acc:A)(f:(A,B)=&gt;A):A\n}\n\ngiven listFoldable:Foldable[List] = new Foldable[List] {\n    def foldLeft[A,B](t:List[B])(acc:A)(f:(A,B)=&gt;A):A = t.foldLeft(acc)(f)\n}\n\ngiven btreeFoldable:Foldable[BTree] = new Foldable[BTree] {\n    import BTree.*\n    def foldLeft[A,B](t:BTree[B])(acc:A)(f:(A,B)=&gt;A):A = t match {\n        case Empty =&gt; acc\n        case Node(v, lft, rgt) =&gt; {\n            val acc1 = f(acc,v)\n            val acc2 = foldLeft(lft)(acc1)(f)\n            foldLeft(rgt)(acc2)(f)\n        }\n    }\n}\n\nlistFoldable.foldLeft(l)(0)((x:Int,y:Int) =&gt; x + y)\nbtreeFoldable.foldLeft(t)(0)((x:Int,y:Int) =&gt; x + y)\n</code></pre>"},{"location":"fp_scala_poly/#option-and-either","title":"Option and Either","text":"<p>Recall in the earlier lesson, we encountered the following example. </p> <pre><code>enum MathExp {\n    case Plus(e1:MathExp, e2:MathExp)\n    case Minus(e1:MathExp, e2:MathExp)\n    case Mult(e1:MathExp, e2:MathExp)\n    case Div(e1:MathExp, e2:MathExp)\n    case Const(v:Int)\n}\n\ndef eval(e:MathExp):Int = e match {\n    case MathExp.Plus(e1, e2)  =&gt; eval(e1) + eval(e2)\n    case MathExp.Minus(e1, e2) =&gt; eval(e1) - eval(e2)\n    case MathExp.Mult(e1, e2)  =&gt; eval(e1) * eval(e2)\n    case MathExp.Div(e1, e2)   =&gt; eval(e1) / eval(e2)\n    case MathExp.Const(i)      =&gt; i\n}\n</code></pre> <p>An error occurs when we try to evalue a <code>MathExp</code> which contains a division by zero sub-expression. Executing </p> <p><pre><code>import MathExp.*\neval(Div(Const(1), Minus(Const(2), Const(2))))\n</code></pre> yields</p> <pre><code>java.lang.ArithmeticException: / by zero\n  at rs$line$2$.eval(rs$line$2:5)\n  ... 41 elided\n</code></pre> <p>Like other main stream languages, we could use <code>try-catch</code> statement to handle the exception. </p> <pre><code>try {\n    import MathExp.*\n    eval(Div(Const(1), Minus(Const(2), Const(2))))\n}\ncatch {\n    case e:java.lang.ArithmeticException =&gt; println(\"handinging div by zero\")\n}\n</code></pre> <p>One downside of this approach is that at compile type it is hard to track the unhandled exceptions, (in particular with the presence of Java unchecked exceptions.)</p> <p>A more fine-grained approach is to use algebraic datatype to \"inform\" the compiler (and other programmers who use this function and datatypes).</p> <p>Consider the following builtin Scala datatype <code>Option</code></p> <pre><code>// no need to run this.\nenum Option[+A] {\n    case None\n    case Some(v:A)\n}\n</code></pre> <pre><code>def eval(e:MathExp):Option[Int] = e match {\n    case MathExp.Plus(e1, e2)  =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(v2) =&gt; Some(v1 + v2)            \n        }\n    }\n    case MathExp.Minus(e1, e2) =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(v2) =&gt; Some(v1 - v2)            \n        }\n    }\n    case MathExp.Mult(e1, e2)  =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(v2) =&gt; Some(v1 * v2)            \n        }\n    }\n    case MathExp.Div(e1, e2)   =&gt; eval(e1) match {\n        case None     =&gt; None\n        case Some(v1) =&gt; eval(e2) match {\n            case None     =&gt; None \n            case Some(0)  =&gt; None\n            case Some(v2) =&gt; Some(v1 / v2)            \n        }\n    }\n    case MathExp.Const(i)      =&gt; Some(i)\n}\n</code></pre> <p>When we execute <code>eval(Div(Const(1), Minus(Const(2), Const(2))))</code>,  we get <code>None</code> as the result instead of the exception. One advantage of this is that whoever is using <code>eval</code> function has to respect that its return type is <code>Option[Int]</code> instead of just <code>Int</code> therefore, a <code>match</code> must be applied before using the result to look out for potential <code>None</code> value.</p> <p>There are still two drawbacks. Firstly, the updated version of the <code>eval</code> function is much more verbose compared to the original unsafe version. We will address this issue in the next lesson. Secondly, we lose the chance of reporting where the division by zero has occured. Let's address the second issue.</p> <p>We could instead of using <code>Option</code>, use the <code>Either</code> datatype</p> <pre><code>// no need to run this, it's builtin\nenum Either[+A, +B] {\n    case Left(v:A)\n    case Right(v:B)\n}\n</code></pre> <pre><code>type ErrMsg = String\n\ndef eval(e: MathExp): Either[ErrMsg,Int] = e match {\n    case MathExp.Plus(e1, e2) =&gt;\n        eval(e1) match {\n            case Left(m) =&gt; Left(m)\n            case Right(v1) =&gt;\n                eval(e2) match {\n                    case Left(m) =&gt; Left(m)\n                    case Right(v2) =&gt; Right(v1 + v2)\n                }\n        }\n    case MathExp.Minus(e1, e2) =&gt;\n        eval(e1) match {\n            case Left(m) =&gt; Left(m)\n            case Right(v1) =&gt;\n                eval(e2) match {\n                    case Left(m) =&gt; Left(m)\n                    case Right(v2) =&gt; Right(v1 - v2)\n                }\n        }\n    case MathExp.Mult(e1, e2) =&gt;\n        eval(e1) match {\n            case Left(m) =&gt; Left(m)\n            case Right(v1) =&gt;\n                eval(e2) match {\n                    case Left(m) =&gt; Left(m)\n                    case Right(v2) =&gt; Right(v1 * v2)\n                }\n        }\n    case MathExp.Div(e1, e2) =&gt;\n        eval(e1) match {\n            case Left(m) =&gt; Left(m)\n            case Right(v1) =&gt;\n                eval(e2) match {\n                    case Left(m) =&gt; Left(m)\n                    case Right(0) =&gt;\n                        Left(s\"div by zero caused by ${e.toString}\")\n                    case Right(v2) =&gt; Right(v1 / v2)\n                }\n        }\n    case MathExp.Const(i) =&gt; Right(i)\n}\n</code></pre> <p>Executing <code>eval(Div(Const(1), Minus(Const(2), Const(2))))</code> will yield </p> <pre><code>Left(div by zero caused by Div(Const(1),Minus(Const(2),Const(2))))\n</code></pre>"},{"location":"fp_scala_poly/#summary","title":"Summary","text":"<p>In this lesson, we have discussed </p> <ul> <li>how to develop parametrically polymorphic Scala code using Generic, Algebraic Datatype</li> <li>how to safely mix parametric polymoprhism with adhoc polymoprhism (overloading) using type classes </li> <li>how to develop generic programming style code using <code>Functor</code> type class.</li> <li>how to make use of <code>Option</code> and <code>Either</code> to handle and manipulate errors and exceptions. </li> </ul>"},{"location":"fp_scala_poly/#appendix","title":"Appendix","text":""},{"location":"fp_scala_poly/#generalized-algebraic-data-type","title":"Generalized Algebraic Data Type","text":"<p>Generalized Algebraic Data Type is an extension to Algebraic Data Type, in which each case extends a more specific version of the top level algebraic data type. Consider the following example.</p> <p>Firstly, we need some type acrobatics to encode nature numbers on the level of type. </p> <p><pre><code>enum Zero {\n    case Zero\n}\nenum Succ[A] {\n    case Succ(v:A)\n}\n</code></pre> Next we define our GADT <code>SList[S,A]</code> which is a generic list of elements <code>A</code> and with size <code>S</code>. </p> <p><pre><code>enum SList[S,+A] {\n    case Nil extends SList[Zero,Nothing] // additional type constraint S = Zero\n    case Cons[N,A](hd:A, tl:SList[N,A]) extends SList[Succ[N],A]  // add'n type constraint S = Succ[N]\n}\n</code></pre> In the first subcase <code>Nil</code>, it is declared with the type of <code>SList[Zero, Nothing]</code> which indicates on type level that the list is empty. In the second case <code>Cons</code>, we define it to have the type <code>SList[Succ[N],A]</code> for some natural number <code>N</code>. This indicates on the type level that the list is non-empty. </p> <p>Having these information lifted to the type level allows us to define a type safe <code>head</code> function.</p> <pre><code>import SList.*\n\ndef head[A,N](sl:SList[Succ[N],A]):A = sl match {\n    case Cons(hd, tl) =&gt; hd\n}\n</code></pre> <p>Compiling <code>head(Nil)</code> yields a type error. </p> <p>Similarly we can define a size-aware function <code>snoc</code> which add an element at the tail of a list. </p> <pre><code>def snoc[A,N](v:A, sl:SList[N,A]):SList[Succ[N],A] = sl match {\n    case Nil =&gt; Cons(v,Nil)\n    // case Cons(hd, tl) =&gt; snoc(v, tl) will result in compilation error.\n    case Cons(hd, tl) =&gt; Cons(hd, snoc(v, tl))\n}\n</code></pre>"},{"location":"handout/","title":"50.054 Compiler Design and Program Analysis Course Handout","text":""},{"location":"handout/#this-page-will-be-updated-regularly-sync-up-often","title":"This page will be updated regularly. Sync up often.","text":""},{"location":"handout/#course-description","title":"Course Description","text":"<p>This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of program optimization and software security analysis. </p>"},{"location":"handout/#module-learning-outcomes","title":"Module Learning Outcomes","text":"<p>By the end of this module, students are able to</p> <ol> <li>Implement software solutions using functional programming language and applying design patterns</li> <li>Define the essential components in a program compilation pipeline</li> <li>Design a compiler for an imperative programming language</li> <li>Optimise the generated machine codes by applying program analysis techniques</li> <li>Detect bugs and security flaws in software by applying program analysis techniques</li> </ol>"},{"location":"handout/#measurable-outcomes","title":"Measurable Outcomes","text":"<ol> <li>Develop a parser for an imperative programming language with assignment, if-else and loop (AKA the source language) using Functional Programming</li> <li>Implement a type checker for the source language</li> <li>Develop a static analyser to eliminate dead codes</li> <li>Implement the register allocation algorithm in the target code generation module</li> <li>Develop a static analyser to identify potential security flaws in the source language</li> </ol>"},{"location":"handout/#topics","title":"Topics","text":"<ol> <li>Functional Programming : Expression, Function, Conditional</li> <li>Functional Programming : List, Algebraic data type and Pattern Matching</li> <li>Functional Programming : Type class</li> <li>Functional Programming : Generic and Functor</li> <li>Functional Programming : Applicative and Monad</li> <li>Syntax analysis: Lexing</li> <li>Syntax analysis: Parsing (LL, LR, SLR)</li> <li>Syntax analysis: Parser Combinator</li> <li>Intermediate Representation: Pseudo-Assembly</li> <li>Intermediate Representation: SSA</li> <li>Semantic analysis: Dynamic Semantics</li> <li>Semantic analysis: Type checking</li> <li>Semantic analysis: Type Inference</li> <li>Semantic analysis: Sign analysis</li> <li>Semantic analysis: Liveness analysis </li> <li>Code Gen: Instruction selection</li> <li>Code Gen: Register allocation</li> <li>Memory Management</li> </ol>"},{"location":"handout/#resource","title":"Resource","text":"<p>The main resources are lecture slides, tutorial sessions, and online documentations. There are no official textbooks. But the following are useful for reference and deeper understanding of some topics.</p> <ol> <li>Compilers: Principles, Techniques, and Tools by Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman</li> <li>Modern Compiler Implementation in ML by Andrew W. Appel</li> <li>Types and Programming Languages by Benjamin C. Pierce</li> <li>Static Program Analysis by Anders M\u00f8ller and Michael I. Schwartzbach</li> <li>Scala 3 Book <code>https://docs.scala-lang.org/scala3/book/introduction.html</code></li> </ol>"},{"location":"handout/#instructors","title":"Instructors","text":"<ul> <li>Kenny Lu (kenny_lu@sutd.edu.sg) <ul> <li>Office Hour: Monday 3:00-4:30pm (please send email to arrange)</li> </ul> </li> </ul>"},{"location":"handout/#communication","title":"Communication","text":"<p>If you have course/assignment/project related questions, please post it on the dedicated MS teams channel.</p>"},{"location":"handout/#assessment","title":"Assessment","text":"<ul> <li>Mid-term 10%</li> <li>Project 35%</li> <li>Homework 20%</li> <li>Final 30%</li> <li>Class Participation 5%</li> </ul>"},{"location":"handout/#things-you-need-to-prepare","title":"Things you need to prepare","text":"<ul> <li>If you are using Windows 10 or Windows 11, please install ubuntu subsystems<ul> <li>Win10</li> <li>Win11</li> </ul> </li> <li>If you are using Linux, it should be perfect.</li> <li>If you are using Mac, please install homebrew.</li> <li>Make sure JVM &gt;=11 is installed.</li> <li>Install Scala &gt;= 3</li> <li>https://www.scala-lang.org/download/</li> <li>IDE: It's your choice, but VSCode works fine.</li> <li>You should try test your setup by attempting Homework 1 on your own. </li> </ul>"},{"location":"handout/#schedule","title":"Schedule","text":"<p>Schedule</p>"},{"location":"handout/#project","title":"Project","text":"<p>Project Template</p> <p>The aim of the project is to apply the techniques and concepts taught in this module to develop a simple compiler for the SIMP language. </p> <p>You may work as a team (up to max 3 members). Please register your team here.  </p> <ul> <li>Lab 1 (10%, Deadline - 16 Nov 2025 23:59)</li> <li>Lab 2 (10%, Deadline - 30 Nov 2025 23:59)</li> <li>Lab 3 (15%, Deadline - 14 Dec 2025 23:59)</li> </ul>"},{"location":"handout/#submission-policy-and-plagiarism","title":"Submission Policy and Plagiarism","text":"<ol> <li>You will do the assignment/project on your own (own teams) and will not copy paste solutions from someone else.</li> <li>You will not post any solutions related to this course to a private/public repository that is accessible by the public/others.</li> <li>Students are allowed to have a private repository for their assignment which no one can access. </li> <li>For projects, students can only invite their partners as collaborators to a private repository.</li> <li>Failing to follow the Code of Honour will result in failing the course and/or being submitted to the University Disciplinary Committee. The consequences apply to both the person who shares their work and the person who copies the work.</li> </ol>"},{"location":"handout/#make-up-and-alternative-assessment","title":"Make Up and Alternative Assessment","text":"<p>Make ups for Final exam will be administered when there is an official Leave of Absence from OSA. There will be only one make up. There will be no make-up if students miss the make up test. </p>"},{"location":"introduction/","title":"50.054 - Introduction","text":""},{"location":"introduction/#module-description","title":"Module Description","text":"<p>This course aims to introduce a new programming paradigm to the learners, Functional programming and the suite of advanced language features and design patterns for software design and development. By building on top of these techniques, the course curates the process of designing a modern compiler, through syntax analysis, intermediate presentation construction, semantic analysis and code generation. More specifically the course focuses on the application of program analysis in areas of software verification, program optimization and software security analysis.</p>"},{"location":"introduction/#module-learning-objective","title":"Module Learning Objective","text":"<p>By the end of this module, you should be able to</p> <ul> <li>Comprehend and reason functional programming</li> <li>Develop functional program to solve real world problem</li> <li>Identify the major components in compiler development</li> <li>Explain and implement different techniques and algorithms used in compiler development</li> </ul>"},{"location":"introduction/#what-is-compilation","title":"What is compilation?","text":"<p>Program compilation is a process of mapping a source program into a target program. The source program is often generated by programmers or some higher level design processes and its structure ressemble the design, the specification or the algorithm. The target program is meant to be executed in the target platform and is generated based on the specific target environment requirement, e.g. hardware requirement, code size requirement and etc.</p> <p>A compiler is a software system that manages the program compilation process.</p>"},{"location":"introduction/#what-properties-a-good-compiler-should-posess","title":"What properties a good compiler should posess?","text":"<p>An ideal compiler should be:</p> <ol> <li>Correct. The produced target program should behave the same as the the source program.</li> <li>Reliable. Any errors that could arise in the program should be detected and reported before execution.</li> <li>Generating Efficient Code. The produced target program should be optimized and running efficiently in the target platform.</li> </ol> <p>Some optional properties,</p> <ol> <li>User friendly. The error report should be comprehensive.</li> <li>Intelligent. Helps to automate some of the repetitive tasks.</li> <li>...</li> </ol>"},{"location":"ir_pseudo_assembly/","title":"50.054 - Pseudo Assembly","text":""},{"location":"ir_pseudo_assembly/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you should be able to</p> <ol> <li>Describe the syntax of the source language SIMP.</li> <li>Describe the syntax of the intermediate representation language pseudo-assembly.</li> <li>Describe how pseudo-assembly program is executed.</li> <li>Apply Maximal Munch algorithms to generate a pseudo-assembly code from a given SIMP source code.</li> </ol>"},{"location":"ir_pseudo_assembly/#recap-the-compiler-pipeline","title":"Recap the compiler pipeline","text":"<p>Recall the compiler pipeline</p> <pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]</code></pre> <ul> <li>Lexing</li> <li>Input: Source file in String</li> <li>Output: A sequence of valid tokens according to the language specification (grammar)</li> <li>Parsing</li> <li>Input: Output from the Lexer</li> <li>Output: A parse tree representing parsed result according to the parse derivation</li> </ul> <p>And recall that a parse tree can be considered the first intermediate representation (IR). However the parse tree is to close to the source level which makes it hard to be used for code generation.  For now let's fast forward to consider another IR which is closer to the target code, we refer to it as pseudo assembly. In this unit, we skip the semantic analysis and consider a direct translation from the source language (SIMP) to the pseudo assembly.</p>"},{"location":"ir_pseudo_assembly/#the-simp-language","title":"The SIMP Language","text":"<p>We consider the syntax of SIMP as follows</p> \\[ \\begin{array}{rccl} (\\tt Statement) &amp; S &amp; ::= &amp; X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) &amp; E &amp; ::= &amp; E\\ OP\\ E \\mid X \\mid C  \\mid (E) \\\\ (\\tt Statements) &amp; \\overline{S} &amp; ::= &amp; S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) &amp; OP &amp; ::= &amp; + \\mid - \\mid * \\mid &lt;  \\mid == \\\\  (\\tt Constant) &amp; C &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\  (\\tt Variable) &amp; X &amp; ::= &amp; a \\mid b \\mid c \\mid d \\mid ... \\end{array} \\] <p>For simplicity, we ignore functions and procedures. We assume a special variable \\(input\\) serving as the input argument to the program. We write \\(\\overline{S}\\) to denote a sequence of statements. \\(return\\) statement takes a variable instead of an expression. \\(nop\\) stands a \"no-op\" statement, which implies no action preformed. The rest of the syntax is very similar to Java and C except that the type annotations are omitted. </p> <p>For example (Example SIMP1)</p> <pre><code>x = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n</code></pre>"},{"location":"ir_pseudo_assembly/#pseudo-assembly","title":"Pseudo Assembly","text":"<p>We consider the Pseudo Assembly language as follows.</p> \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) &amp; li  &amp; ::= &amp; l : i \\\\  (\\tt Instruction)   &amp; i   &amp; ::= &amp; d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\  (\\tt Labeled\\ Instructions)   &amp; lis   &amp; ::= &amp; li \\mid li\\ lis \\\\  (\\tt Operand)       &amp; d,s &amp; ::= &amp; r \\mid c \\mid t \\\\ (\\tt Temp\\ Var)      &amp; t   &amp; ::= &amp; x \\mid y \\mid ...  \\\\ (\\tt Label)         &amp; l   &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\ (\\tt Operator)      &amp; op  &amp; ::= &amp; + \\mid - \\mid &lt; \\mid == \\\\  (\\tt Constant)      &amp; c   &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\\\  (\\tt Register)      &amp; r &amp;   ::= &amp; r_{ret} \\mid r_1 \\mid r_2 \\mid ...   \\end{array} \\] <p>where \\(li\\), a labeled instruction, is a label \\(l\\) associated with an instruction \\(i\\). For simplicity, we use positive integers as labels.  An instruction is either a move operation (moving value from source operand \\(s\\) to destination operatnd \\(d\\)), a binary move operation, a return instruction, a conditional jump instruction and a jump instruction. Some non-syntactical restriction exists, e.g. a constant can't be used in a destination position. In Psuedo Assembly, we use <code>0</code> to denote <code>false</code> and <code>1</code>  to denote <code>true</code>. \\(r_{ret}\\) is a special register for the return statement.</p> <p>Example (PA1) <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t &lt;- c &lt; x \n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s\n10: ret\n</code></pre></p>"},{"location":"ir_pseudo_assembly/#informal-specification-of-pseudo-assembly","title":"Informal Specification of Pseudo Assembly","text":"<p>We assume that statements of a pseudo assembly program are stored in a list. There exists a mapping from labels to the corresponding instructions, When we execute a pseudo assembly program, we use a program counter to keep track of the current execution context (i.e. the current labeled instruction being considered) and use a set to keep track of the variable to value mapping. </p> <p>For example when we execute the above program with <code>input = 2</code></p> Program Counter Local  Memory Next Instr 1 {input: 2, x : 2} 2 2 {input: 2, x : 2, s : 0} 3 3 {input: 2, x : 2, s : 0, c : 0} 4 4 {input: 2, x : 2, s : 0, c : 0, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 0, t : 1} 6 6 {input: 2, x : 2, s : 0, c : 0, t : 1} 7 7 {input: 2, x : 2, s : 0, c : 1, t : 1} 8 8 {input: 2, x : 2, s : 0, c : 1, t : 1} 4 4 {input: 2, x : 2, s : 0, c : 1, t : 1} 5 5 {input: 2, x : 2, s : 0, c : 1, t : 1} 6 6 {input: 2, x : 2, s : 1, c : 1, t : 1} 7 7 {input: 2, x : 2, s : 1, c : 2, t : 1} 8 8 {input: 2, x : 2, s : 1, c : 2, t : 1} 4 4 {input: 2, x : 2, s : 1, c : 2, t : 0} 5 5 {input: 2, x : 2, s : 1, c : 2, t : 0} 9 9 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} 10 10 {input: 2, x : 2, s : 1, c : 2, t : 0, rret : 1} - <p>For the time being, we use a table to illusrate the execution of an PA program. Each entry in the table has 3 fields, the program counter, the current local memory (mapping from operands to values), and the next intruction. Move and binary operations updates the local memory. For non-jump instructions, the next instruction is the current instruction label incremeented by 1. For goto, the next instruction is the one associated with the destination label. For conditional jump, the next instruction is dependent on the source operand's value. We study the formal specification of the up-coming lessons.</p>"},{"location":"ir_pseudo_assembly/#maximal-munch-algorithm","title":"Maximal Munch Algorithm","text":"<p>To convert a SIMP program into the pseudo assembly, we could consider the Maximal Munch Algorithm which is described in terms of the set of deduction rules in the following. </p> \\[ \\begin{array}{rc} {\\tt (mAssign)} &amp; \\begin{array}{c}                 G_a(X)(E) \\vdash lis  \\\\                \\hline                G_s(X = E) \\vdash lis                \\end{array} \\\\  \\end{array}   \\] <p>In case we have an assignment statement \\(X = E\\), we call a helper function \\(G_a\\) to generate the Peudo Assembly (PA) labeled instructions.</p> \\[ \\begin{array}{rc} {\\tt (mReturn)} &amp; \\begin{array}{c}      G_a(r_{ret})(X) \\vdash lis \\ \\ l\\ {\\tt is\\ a\\ fresh\\ label} \\\\      \\hline      G_s(return\\ X) \\vdash lis + [ l: ret ]      \\end{array} \\end{array} \\] <p>In case we have a return statement \\(return\\ E\\), we make use of the same helper function \\(G_a\\) to generate the instructions of assigning \\(E\\) to the special register \\(r_{ret}\\). We then generate a new label \\(l\\), and append \\(l:ret\\) to the instructions.</p> \\[ \\begin{array}{rc} {\\tt (mSequence)} &amp; \\begin{array}{c}                 {\\tt for}\\ l \\in \\{1,n\\} ~~ G_s(S_l) \\vdash lis_l \\\\                \\hline                G_s(S_1;...;S_n) \\vdash lis_1 + ... +  lis_n                \\end{array}  \\end{array}   \\] <p>In case we have a sequence of statements, we apply \\(G_s\\) recurisvely to the individual statements in order, then we merge all the results by concatenation.</p> \\[ \\begin{array}{rl}      {\\tt (mIf)} &amp; \\begin{array}{c}                t\\ {\\tt is\\ a\\ fresh\\ var} \\\\                 G_a(t)(E) \\vdash lis_0 \\\\                l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                G_s(S_2) \\vdash lis_2 \\\\                 l_{EndThen}\\ {\\tt  is\\ a\\ fresh\\ label} \\\\                  l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\                 G_s(S_3) \\vdash lis_3 \\\\                 l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\                 lis_1 = [l_{IfCondJ}: ifn\\ t\\ goto\\ l_{Else} ] \\\\                 lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\                 lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\                 \\hline                  G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash lis_0 + lis_1 + lis_2' + lis_3'                                \\end{array} \\\\   \\end{array} \\] <p>In case we have a if-else statement, we  1. generate a fresh variable \\(t\\), and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 2. generate a new label \\(l_{IfCondJ}\\) (conditional jump). 3. call \\(G_s(S_2)\\) to generate the PA instructions for the then branch. 4. generate a new label \\(l_{EndThen}\\) which is associated with the \"end-of-then-branch\" goto instruction. 5. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{Else}\\).  6. call \\(G_s(S_3)\\) to generate the PA instructions for the else branch. 7. generate a new label \\(l_{EndElse}\\), which is associated with the \"end-of-else-branch\" goto instruction. (Note that we can assume the next instruction after this is the end of If, in case of nested if-else.) 8. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) </p> \\[ \\begin{array}{rl}      {\\tt (mWhile)} &amp; \\begin{array}{c}                     l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\                      t\\ {\\tt is\\ a\\ fresh\\ var} \\\\                          G_a(t)(E) \\vdash lis_0 \\\\                      l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                      G_s(S) \\vdash lis_2\\\\                      l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\                       l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\                      lis_1 = [l_{WhileCondJ}: ifn\\ t\\ goto\\ l_{EndWhile}] \\\\                     lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\                     \\hline                     G_s(while\\ E\\ \\{S\\}) \\vdash lis_0 + lis_1 + lis_2'                            \\end{array} \\\\   \\end{array} \\] <p>In case we have a while statement, we  1. peek into the label generator to find out what is the next upcoming label and refer it as \\(l_{While}\\), which can be used later as the reference for the backward jump.  2. generate a fresh variable \\(t\\), and call \\(G_a(t)(E)\\) to convert the conditional expression into PA instructions. 3. generate a new label \\(l_{WhileCondJ}\\) (conditional jump). 4. call \\(G_s(S)\\) to generate the PA instructions for the body. 5. generate a new label \\(l_{EndBody}\\) which is associated with the \"end-of-loop-body\" goto instruction. 6. peek into the label generator to find out what is the next upcoming label and refer to it as \\(l_{EndWhile}\\). (Note that we can assume the next instruction after this is the end of While, in case of nested while) 7. peek into the label generator to find out what the next upcoming label is and refer to it as \\(l_{EndIf}\\) </p> <p>The above summarizes all cases of \\(G_s(S)\\). We now consider the sub algorithm, \\(G_a(d)(E)\\), it takes a destination operand and SIMP expression \\(E\\) and return a set of labeled instructions. </p> \\[ {\\tt (mNOp)} ~~ G_s(nop) \\vdash []  \\] <p>The case of \\(nop\\) statement is stratight-forward.</p> \\[ \\begin{array}{rc} {\\tt (mConst)} &amp; \\begin{array}{c}                l\\ {\\tt  is\\ a\\ fresh\\ label}\\\\ c = conv(C) \\\\                \\hline                G_a(X)(C) \\vdash [l : X \\leftarrow c]                 \\end{array} \\\\  \\end{array} \\] <p>In the above rule, given a SIMP variable \\(X\\) and a constant \\(C\\) we generate a labeled instruction \\(X \\leftarrow c\\). where \\(c\\) is the PA constant converted from SIMP's counter-part through the \\(conv()\\) function. </p> \\[ \\begin{array}{rcl} conv(true) &amp; = &amp;  1\\\\ conv(false) &amp; = &amp; 0\\\\ conv(C) &amp; =&amp;  C \\end{array} \\] <p>For simplicity, we omitted the conversion from the SIMP variable to the IR temp variable. </p> \\[ \\begin{array}{rc} {\\tt (mVar)} &amp; \\begin{array}{c}                l\\ {\\tt  is\\ a\\ fresh\\ label} \\\\                \\hline                G_a(X)(Y) \\vdash [l : X \\leftarrow Y]                 \\end{array} \\\\  \\end{array}   \\] <p>In the above rule, we generate labeled instruction for the case of assigning a SIMP variable \\(Y\\) to another variable \\(X\\). The treat is similar to the case of \\({\\tt (Const)}\\).</p> \\[  \\begin{array}{rc} {\\tt (mParen)} &amp; \\begin{array}{c}                G_a(X)(E) \\vdash lis                \\\\ \\hline                G_a(X)((E)) \\vdash lis                \\end{array} \\end{array} \\] <p>In the rule \\({\\tt (mParen)}\\), we generate the IR labeled instructions by calling the generation algorithm recursively with the inner expression.</p> \\[ \\begin{array}{rc} {\\tt (mOp)} &amp; \\begin{array}{c}                t_1\\ {\\tt is\\ a\\ fresh\\ var} \\\\                G_a(t_1)(E_1) \\vdash lis_1 \\\\                t_2\\ {\\tt is\\ a\\ fresh\\ var} \\\\                G_a(t_2)(E_2) \\vdash lis_2 \\\\                l\\ {\\tt  is\\ a\\ fresh\\ label} \\\\                \\hline                G_a(X)(E_1 OP E_2) \\vdash lis_1 + lis_2 + [l : X \\leftarrow t_1 OP t_2]                 \\end{array} \\\\  \\end{array}   \\] <p>The above rule handles the case where the RHS of the SIMP assignment statement is a binary operation \\(E_1\\ OP\\ E_2\\), we generate two temp variables \\(t_1\\) and \\(t_2\\), and apply the generation function recursively to \\(E_1\\) and \\(E_2\\). Finally we concatenate the results \\(lis_1\\) and \\(lis_2\\) with the binary move operation \\(X \\leftarrow t_1\\ OP\\ t_2\\).</p> <p>For example, given the source in Example SIMP1, we apply the above algorithm and observe the following derivation.</p> <ol> <li>Firstly we apply \\({\\tt (mSequence)}\\) rule to individual statement,</li> </ol> <pre><code>Gs(x = input; s = 0; c = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;) \n---&gt;\nGs(x = input) ; Gs( s = 0) ; Gs(c = 0) ; Gs( while c &lt; x  { s = c + s; c = c + 1;}) ; Gs(return s);\n</code></pre> <p>The derivation for <code>Gs(x = input)</code> is trivial, we apply \\({\\tt (mAssign)}\\) rule.  <pre><code>Gs(x = input) \n---&gt; # using (mAssign) rule\nGa(x)(input)\n---&gt; # using (mVar) rule\n---&gt; [ 1: x &lt;- input ] \n</code></pre> Similarly we generate </p> <pre><code>Gs( s = 0)\n---&gt; # using (mAssign) rule\nGa(s)(0)\n---&gt; # using (mConst) rule\n---&gt; [ 2: s &lt;- 0 ] \n</code></pre> <p>and </p> <pre><code>Gs(c = 0)\n---&gt; # using (mAssign) rule\nGa(c)(0)\n---&gt; # using (mConst) rule\n---&gt; [ 3: c &lt;- 0 ] \n</code></pre> <ol> <li>Next we consider the while statement</li> </ol> <pre><code>Gs(\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\n)\n---&gt; # using (mWhile) rule\n  # the condition exp\n  t is a fresh var\n  Ga(t)(c&lt;x) ---&gt; # using (mOp) rule\n     t1 is a fresh var\n     Ga(t1)(x) ---&gt; [4: t1 &lt;- x]\n     t2 is a fresh var \n     Ga(t2)(c) ---&gt; [5: t2 &lt;- c]\n  ---&gt; [4: t1 &lt;- x, 5: t2 &lt;-c, 6: t &lt;- t1 &lt; t2 ]\n  # the conditional jump, we generate a new label 7 reserved for whilecondjump\n  # the while loop body\n  Gs[ s = c + s; c = c + 1]\n  ---&gt; # (mSequence), (mOp) and (mOp) rules\n  [ 8: t3 &lt;- c, 9: t4 &lt;- s, 10: s &lt;- t3 + t4,  11: t5 &lt;- c, 12: t6 &lt;- 1, 13: c &lt;- t5 + t6 ]\n  # end of the while loop\n  [ 14: goto 4 ]\n  # the conditional jump \n  ---&gt; [7: ifn t goto 15 ]\n---&gt;  # putting altogther\n[4: t1 &lt;- x, 5: t2 &lt;- c, 6:  t &lt;- t1 &lt; t2,   7: ifn t goto 15, \n 8: t3 &lt;- c, 9: t4 &lt;- s, 10: s &lt;- t3 + t4,  11: t5 &lt;- c, \n 12: t6 &lt;- 1, 13: c &lt;- t5 + t6, 14: goto 4] \n</code></pre> <ol> <li>Finally we convert the return statement <pre><code>Gs(return s)\n---&gt; # (mReturn) rule\n[15: r_ret &lt;- s, 16: ret]\n</code></pre></li> </ol> <p>Putting 1,2,3 together</p> <pre><code>1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t1 &lt;- x\n5: t2 &lt;- c\n6: t &lt;- t1 &lt; t2\n7: ifn t goto 15 \n8: t3 &lt;- c\n9: t4 &lt;- s\n10: s &lt;- t3 + t4\n11: t5 &lt;- c \n12: t6 &lt;- 1 \n13: c &lt;- t5 + t6\n14: goto 4\n15: rret &lt;- s\n16: ret\n</code></pre> <p>As we observe, we don't quite get the exact output as example PA1. The main reason is that we generate extra steps thanks to the \\({\\tt (mOp)}\\) rule, (in which each operand of the binary operator takes up a new instruction).</p>"},{"location":"ir_pseudo_assembly/#maximal-munch-algorithm-v2","title":"Maximal Munch Algorithm V2","text":"<p>Since the \\({\\tt (mOp)}\\) rule is the culprit of causing extra move steps generated in the IR.</p> <p>We consider a variant the Maximal Munch Algorithm. Instead of using \\(G_a(X)(E)\\) to generate  labeled instructions \\(lis\\) , we use a different sub system \\(G_e(E)\\) to generate a pair of results, \\(\\hat{e}, \\check{e}\\). where \\(\\check{e}\\) is a sequence of label instructions generated from \\(E\\) and \\(\\hat{e}\\) is the \"result\" operand storing the final result of \\(\\check{e}\\).</p> <p>The adjusted \\({\\tt (mConst)}\\), \\({\\tt (mVar)}\\) and \\({\\tt (mOp)}\\) rules are as follows,</p> \\[  \\begin{array}{rc} {\\tt (m2Const)} &amp; \\begin{array}{c}            G_e(C) \\vdash (conv(C), [])            \\end{array}  \\end{array}   \\] \\[ \\begin{array}{rc} {\\tt (m2Var)} &amp; \\begin{array}{c}            G_e(Y) \\vdash (Y, [])       \\end{array}  \\end{array}   \\] <p>The rules \\({\\tt (m2Const)}\\) and \\({\\tt (m2Var)}\\) are simple. We just return the constant (variable) as the \\(\\hat{e}\\) with an empty set of label instructions.</p> \\[ \\begin{array}{rc} {\\tt (m2Paren)} &amp; \\begin{array}{c}            G_e(E) \\vdash (\\hat{e}, \\check{e})           \\\\ \\hline           G_e((E)) \\vdash (\\hat{e}, \\check{e})      \\end{array}  \\end{array}   \\] <p>In the rule \\({\\tt (m2Paren)}\\), we generate the results by recursivelly applying the algorithm to the inner expression.</p> \\[ \\begin{array}{rc} {\\tt (m2Op)} &amp; \\begin{array}{c}            G_e(E_1) \\vdash (\\hat{e}_1, \\check{e}_1) \\\\            G_e(E_2) \\vdash (\\hat{e}_2, \\check{e}_2) \\\\            t \\ {\\tt is\\ a\\ fresh\\ variable.} \\\\            l \\ {\\tt is\\ a\\ fresh\\ label.} \\\\            \\hline           G_e(E_1 OP E_2) \\vdash (t, \\check{e}_1 + \\check{e}_2 + [l : t \\leftarrow \\hat{e}_1 OP \\hat{e}_2])            \\end{array} \\\\  \\end{array}   \\] <p>In the \\({\\tt (m2Op)}\\) rule, we call \\(G_e(\\cdot)\\) recursively to generate the  results for \\(E_1\\) and \\(E_2\\), namely \\((\\hat{e}_1, \\check{e}_1)\\) and \\((\\hat{e}_2, \\check{e}_2)\\).  We then use them to synthesis the final output. </p> <p>The \\(G_s(S)\\) rules are now calling \\(G_e(E)\\) instead of \\(G_a(X)(E)\\). </p> \\[ \\begin{array}{rc} {\\tt (m2Assign)} &amp; \\begin{array}{c}       G_e(E) \\vdash (\\hat{e}, \\check{e})  \\ \\       l\\ {\\tt is\\ a\\ fresh\\ label.} \\\\       \\hline      G_s(X = E) \\vdash \\check{e} + [ l : X \\leftarrow \\hat{e}]      \\end{array} \\\\  \\end{array}   \\] \\[ \\begin{array}{rc} {\\tt (m2Return)} &amp; \\begin{array}{c}      G_s(return\\ X) \\vdash \\check{e} + [ l_1 : r_{ret} \\leftarrow X,  l_2: ret ]      \\end{array}  \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2If)} &amp; \\begin{array}{c}           G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\            l_{IfCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\           G_s(S_2) \\vdash lis_2 \\\\            l_{EndThen}\\ {\\tt  is\\ a\\ fresh\\ label} \\\\             l_{Else}\\ {\\tt is\\ the\\ next\\ label (w/o\\ incr)} \\\\            G_s(S_3) \\vdash lis_3 \\\\            l_{EndElse}\\ {\\tt is\\ a\\ fresh\\ label} \\\\           l_{EndIf}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\            lis_1 = [l_{IfCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{Else} ] \\\\            lis_2' = lis_2 + [l_{EndThen}:goto\\ l_{EndIf}] \\\\            lis_3' = lis_3 + [l_{EndElse}:goto\\ l_{EndIf}] \\\\            \\hline             G_s(if\\ E\\ \\{S_2\\}\\ else\\ \\{S_3\\}) \\vdash \\check{e} + lis_1 + lis_2' + lis_3'                          \\end{array}  \\end{array} \\] \\[ \\begin{array}{rl} {\\tt (m2While)} &amp; \\begin{array}{c}           l_{While}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\            G_e(E) \\vdash (\\hat{e}, \\check{e}) \\\\            l_{WhileCondJ}\\ {\\tt is\\ a\\ fresh\\ label} \\\\            G_s(S) \\vdash lis_2\\\\            l_{EndBody}\\ {\\tt is\\ a\\ fresh\\ label} \\\\             l_{EndWhile}\\ {\\tt is\\ the\\ next\\ label\\ (w/o\\ incr)} \\\\            lis_1 = [l_{WhileCondJ}: ifn\\ \\hat{e}\\ goto\\ l_{EndWhile}] \\\\           lis_2' = lis_2 + [ l_{EndBody}: goto\\ l_{While} ] \\\\           \\hline           G_s(while\\ E\\ \\{S\\}) \\vdash  \\check{e} + lis_1 + lis_2'                      \\end{array}  \\end{array} \\] <p>By comparing this version with the first one, we note that we reduce the number of labels as well as the numbers of temp variables being created througout the conversion from SIMP to PA. </p> <p>For example, if we apply the optimized version of Maximal Munch to the example SIMP1, we should obtain example PA1 as result.</p>"},{"location":"liveness_analysis/","title":"50.054 - Liveness Analysis","text":""},{"location":"liveness_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Define the liveness analysis problem</li> <li>Apply lattice and fixed point algorithm to solve the liveness analysis problem</li> </ol>"},{"location":"liveness_analysis/#recall","title":"Recall","text":"<p><pre><code>// SIMP1\nx = input;\ny = 0;\ns = 0;\nwhile (y &lt; x) { \n    y = y + 1;\n    t = s;  // t is not used.\n    s = s + y;  \n}\nreturn s;\n</code></pre> In the above program the statement <code>t = s</code> is redundant as <code>t</code> is not used.</p> <p>It can be statically detected by a liveness analysis. </p>"},{"location":"liveness_analysis/#liveness-analysis","title":"Liveness Analysis","text":"<p>A variable is consideredd live at a program location \\(v\\) if it may be used in another program location \\(u\\) if we follow the execution order, i.e. in the control flow graph there exists a path from \\(v\\) to \\(u\\). Otherwise, the variable is considered not live or dead.  Note that from this analysis a variable is detected to be live, it is actually \"maybe-live\" since we are using a conservative approximation via lattice theory. On the hand, the negation, i.e. dead is definite.</p> <p>By applying this analysis to the above program, we can find out at the program locations where variables must be dead.</p>"},{"location":"liveness_analysis/#defining-the-lattice-for-livenesss-analysis","title":"Defining the Lattice for Livenesss Analysis","text":"<p>Recall from the previous lesson, we learned that if \\(A\\) be a set, then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice, where \\({\\cal P}(A)\\) the power set of \\(A\\).</p> <p>Applying this approach the liveness analysis, we consider the powerset  the set of all variables in the program.</p> <p>Let's recast the <code>SIMP1</code> program into pseudo assembly, let's label it as <code>PA1</code></p> <pre><code>1: x &lt;- input\n2: y &lt;- 0\n3: s &lt;- 0\n4: b &lt;- y &lt; x\n5: ifn b goto 10\n6: y &lt;- y + 1\n7: t &lt;- s\n8: s &lt;- s + y\n9: goto 4\n10: rret &lt;- s\n11: ret\n</code></pre> <p>In <code>PA1</code> we find the set of variables \\(V = \\{input, x, y, s, t, b\\}\\), if we construct a powerset lattice \\(({\\cal P(V)}, \\subseteq)\\), we see the following hasse diagram</p> <pre><code>graph TD;\n    N58[\"{b}\"] --- N64[\"{}\"] \n    N59[\"{t}\"] --- N64[\"{}\"] \n    N60[\"{s}\"] --- N64[\"{}\"] \n    N61[\"{y}\"] --- N64[\"{}\"] \n    N62[\"{x}\"] --- N64[\"{}\"] \n    N63[\"{input}\"] --- N64[\"{}\"] \n    N43[\"{t,b}\"] --- N58[\"{b}\"] \n    N44[\"{s,b}\"] --- N58[\"{b}\"] \n    N46[\"{y,b}\"] --- N58[\"{b}\"] \n    N49[\"{x,b}\"] --- N58[\"{b}\"] \n    N53[\"{input,b}\"] --- N58[\"{b}\"] \n    N43[\"{t,b}\"] --- N59[\"{t}\"] \n    N45[\"{s,t}\"] --- N59[\"{t}\"] \n    N47[\"{y,t}\"] --- N59[\"{t}\"] \n    N50[\"{x,t}\"] --- N59[\"{t}\"] \n    N54[\"{input,t}\"] --- N59[\"{t}\"] \n    N44[\"{s,b}\"] --- N60[\"{s}\"] \n    N45[\"{s,t}\"] --- N60[\"{s}\"] \n    N48[\"{y,s}\"] --- N60[\"{s}\"] \n    N51[\"{x,s}\"] --- N60[\"{s}\"] \n    N55[\"{input,s}\"] --- N60[\"{s}\"] \n    N46[\"{y,b}\"] --- N61[\"{y}\"] \n    N47[\"{y,t}\"] --- N61[\"{y}\"] \n    N48[\"{y,s}\"] --- N61[\"{y}\"] \n    N52[\"{x,y}\"] --- N61[\"{y}\"] \n    N56[\"{input,y}\"] --- N61[\"{y}\"] \n    N49[\"{x,b}\"] --- N62[\"{x}\"] \n    N50[\"{x,t}\"] --- N62[\"{x}\"] \n    N51[\"{x,s}\"] --- N62[\"{x}\"] \n    N52[\"{x,y}\"] --- N62[\"{x}\"] \n    N57[\"{input,x}\"] --- N62[\"{x}\"] \n    N53[\"{input,b}\"] --- N63[\"{input}\"] \n    N54[\"{input,t}\"] --- N63[\"{input}\"] \n    N55[\"{input,s}\"] --- N63[\"{input}\"] \n    N56[\"{input,y}\"] --- N63[\"{input}\"] \n    N57[\"{input,x}\"] --- N63[\"{input}\"] \n    N23[\"{s,t,b}\"] --- N43[\"{t,b}\"] \n    N24[\"{y,t,b}\"] --- N43[\"{t,b}\"] \n    N27[\"{x,t,b}\"] --- N43[\"{t,b}\"] \n    N33[\"{input,t,b}\"] --- N43[\"{t,b}\"] \n    N23[\"{s,t,b}\"] --- N44[\"{s,b}\"] \n    N25[\"{y,s,b}\"] --- N44[\"{s,b}\"] \n    N28[\"{x,s,b}\"] --- N44[\"{s,b}\"] \n    N34[\"{input,s,b}\"] --- N44[\"{s,b}\"] \n    N23[\"{s,t,b}\"] --- N45[\"{s,t}\"] \n    N26[\"{y,s,t}\"] --- N45[\"{s,t}\"] \n    N29[\"{x,s,t}\"] --- N45[\"{s,t}\"] \n    N35[\"{input,s,t}\"] --- N45[\"{s,t}\"] \n    N24[\"{y,t,b}\"] --- N46[\"{y,b}\"] \n    N25[\"{y,s,b}\"] --- N46[\"{y,b}\"] \n    N30[\"{x,y,b}\"] --- N46[\"{y,b}\"] \n    N36[\"{input,y,b}\"] --- N46[\"{y,b}\"] \n    N24[\"{y,t,b}\"] --- N47[\"{y,t}\"] \n    N26[\"{y,s,t}\"] --- N47[\"{y,t}\"] \n    N31[\"{x,y,t}\"] --- N47[\"{y,t}\"] \n    N37[\"{input,y,t}\"] --- N47[\"{y,t}\"] \n    N25[\"{y,s,b}\"] --- N48[\"{y,s}\"] \n    N26[\"{y,s,t}\"] --- N48[\"{y,s}\"] \n    N32[\"{x,y,s}\"] --- N48[\"{y,s}\"] \n    N38[\"{input,y,s}\"] --- N48[\"{y,s}\"] \n    N27[\"{x,t,b}\"] --- N49[\"{x,b}\"] \n    N28[\"{x,s,b}\"] --- N49[\"{x,b}\"] \n    N30[\"{x,y,b}\"] --- N49[\"{x,b}\"] \n    N39[\"{input,x,b}\"] --- N49[\"{x,b}\"] \n    N27[\"{x,t,b}\"] --- N50[\"{x,t}\"] \n    N29[\"{x,s,t}\"] --- N50[\"{x,t}\"] \n    N31[\"{x,y,t}\"] --- N50[\"{x,t}\"] \n    N40[\"{input,x,t}\"] --- N50[\"{x,t}\"] \n    N28[\"{x,s,b}\"] --- N51[\"{x,s}\"] \n    N29[\"{x,s,t}\"] --- N51[\"{x,s}\"] \n    N32[\"{x,y,s}\"] --- N51[\"{x,s}\"] \n    N41[\"{input,x,s}\"] --- N51[\"{x,s}\"] \n    N30[\"{x,y,b}\"] --- N52[\"{x,y}\"] \n    N31[\"{x,y,t}\"] --- N52[\"{x,y}\"] \n    N32[\"{x,y,s}\"] --- N52[\"{x,y}\"] \n    N42[\"{input,x,y}\"] --- N52[\"{x,y}\"] \n    N33[\"{input,t,b}\"] --- N53[\"{input,b}\"] \n    N34[\"{input,s,b}\"] --- N53[\"{input,b}\"] \n    N36[\"{input,y,b}\"] --- N53[\"{input,b}\"] \n    N39[\"{input,x,b}\"] --- N53[\"{input,b}\"] \n    N33[\"{input,t,b}\"] --- N54[\"{input,t}\"] \n    N35[\"{input,s,t}\"] --- N54[\"{input,t}\"] \n    N37[\"{input,y,t}\"] --- N54[\"{input,t}\"] \n    N40[\"{input,x,t}\"] --- N54[\"{input,t}\"] \n    N34[\"{input,s,b}\"] --- N55[\"{input,s}\"] \n    N35[\"{input,s,t}\"] --- N55[\"{input,s}\"] \n    N38[\"{input,y,s}\"] --- N55[\"{input,s}\"] \n    N41[\"{input,x,s}\"] --- N55[\"{input,s}\"] \n    N36[\"{input,y,b}\"] --- N56[\"{input,y}\"] \n    N37[\"{input,y,t}\"] --- N56[\"{input,y}\"] \n    N38[\"{input,y,s}\"] --- N56[\"{input,y}\"] \n    N42[\"{input,x,y}\"] --- N56[\"{input,y}\"] \n    N39[\"{input,x,b}\"] --- N57[\"{input,x}\"] \n    N40[\"{input,x,t}\"] --- N57[\"{input,x}\"] \n    N41[\"{input,x,s}\"] --- N57[\"{input,x}\"] \n    N42[\"{input,x,y}\"] --- N57[\"{input,x}\"] \n    N8[\"{y,s,t,b}\"] --- N23[\"{s,t,b}\"] \n    N9[\"{x,s,t,b}\"] --- N23[\"{s,t,b}\"] \n    N13[\"{input,s,t,b}\"] --- N23[\"{s,t,b}\"] \n    N8[\"{y,s,t,b}\"] --- N24[\"{y,t,b}\"] \n    N10[\"{x,y,t,b}\"] --- N24[\"{y,t,b}\"] \n    N14[\"{input,y,t,b}\"] --- N24[\"{y,t,b}\"] \n    N8[\"{y,s,t,b}\"] --- N25[\"{y,s,b}\"] \n    N11[\"{x,y,s,b}\"] --- N25[\"{y,s,b}\"] \n    N15[\"{input,y,s,b}\"] --- N25[\"{y,s,b}\"] \n    N8[\"{y,s,t,b}\"] --- N26[\"{y,s,t}\"] \n    N12[\"{x,y,s,t}\"] --- N26[\"{y,s,t}\"] \n    N16[\"{input,y,s,t}\"] --- N26[\"{y,s,t}\"] \n    N9[\"{x,s,t,b}\"] --- N27[\"{x,t,b}\"] \n    N10[\"{x,y,t,b}\"] --- N27[\"{x,t,b}\"] \n    N17[\"{input,x,t,b}\"] --- N27[\"{x,t,b}\"] \n    N9[\"{x,s,t,b}\"] --- N28[\"{x,s,b}\"] \n    N11[\"{x,y,s,b}\"] --- N28[\"{x,s,b}\"] \n    N18[\"{input,x,s,b}\"] --- N28[\"{x,s,b}\"] \n    N9[\"{x,s,t,b}\"] --- N29[\"{x,s,t}\"] \n    N12[\"{x,y,s,t}\"] --- N29[\"{x,s,t}\"] \n    N19[\"{input,x,s,t}\"] --- N29[\"{x,s,t}\"] \n    N10[\"{x,y,t,b}\"] --- N30[\"{x,y,b}\"] \n    N11[\"{x,y,s,b}\"] --- N30[\"{x,y,b}\"] \n    N20[\"{input,x,y,b}\"] --- N30[\"{x,y,b}\"] \n    N10[\"{x,y,t,b}\"] --- N31[\"{x,y,t}\"] \n    N12[\"{x,y,s,t}\"] --- N31[\"{x,y,t}\"] \n    N21[\"{input,x,y,t}\"] --- N31[\"{x,y,t}\"] \n    N11[\"{x,y,s,b}\"] --- N32[\"{x,y,s}\"] \n    N12[\"{x,y,s,t}\"] --- N32[\"{x,y,s}\"] \n    N22[\"{input,x,y,s}\"] --- N32[\"{x,y,s}\"] \n    N13[\"{input,s,t,b}\"] --- N33[\"{input,t,b}\"] \n    N14[\"{input,y,t,b}\"] --- N33[\"{input,t,b}\"] \n    N17[\"{input,x,t,b}\"] --- N33[\"{input,t,b}\"] \n    N13[\"{input,s,t,b}\"] --- N34[\"{input,s,b}\"] \n    N15[\"{input,y,s,b}\"] --- N34[\"{input,s,b}\"] \n    N18[\"{input,x,s,b}\"] --- N34[\"{input,s,b}\"] \n    N13[\"{input,s,t,b}\"] --- N35[\"{input,s,t}\"] \n    N16[\"{input,y,s,t}\"] --- N35[\"{input,s,t}\"] \n    N19[\"{input,x,s,t}\"] --- N35[\"{input,s,t}\"] \n    N14[\"{input,y,t,b}\"] --- N36[\"{input,y,b}\"] \n    N15[\"{input,y,s,b}\"] --- N36[\"{input,y,b}\"] \n    N20[\"{input,x,y,b}\"] --- N36[\"{input,y,b}\"] \n    N14[\"{input,y,t,b}\"] --- N37[\"{input,y,t}\"] \n    N16[\"{input,y,s,t}\"] --- N37[\"{input,y,t}\"] \n    N21[\"{input,x,y,t}\"] --- N37[\"{input,y,t}\"] \n    N15[\"{input,y,s,b}\"] --- N38[\"{input,y,s}\"] \n    N16[\"{input,y,s,t}\"] --- N38[\"{input,y,s}\"] \n    N22[\"{input,x,y,s}\"] --- N38[\"{input,y,s}\"] \n    N17[\"{input,x,t,b}\"] --- N39[\"{input,x,b}\"] \n    N18[\"{input,x,s,b}\"] --- N39[\"{input,x,b}\"] \n    N20[\"{input,x,y,b}\"] --- N39[\"{input,x,b}\"] \n    N17[\"{input,x,t,b}\"] --- N40[\"{input,x,t}\"] \n    N19[\"{input,x,s,t}\"] --- N40[\"{input,x,t}\"] \n    N21[\"{input,x,y,t}\"] --- N40[\"{input,x,t}\"] \n    N18[\"{input,x,s,b}\"] --- N41[\"{input,x,s}\"] \n    N19[\"{input,x,s,t}\"] --- N41[\"{input,x,s}\"] \n    N22[\"{input,x,y,s}\"] --- N41[\"{input,x,s}\"] \n    N20[\"{input,x,y,b}\"] --- N42[\"{input,x,y}\"] \n    N21[\"{input,x,y,t}\"] --- N42[\"{input,x,y}\"] \n    N22[\"{input,x,y,s}\"] --- N42[\"{input,x,y}\"] \n    N2[\"{x,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N8[\"{y,s,t,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N9[\"{x,s,t,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N9[\"{x,s,t,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N10[\"{x,y,t,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N10[\"{x,y,t,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N11[\"{x,y,s,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N11[\"{x,y,s,b}\"] \n    N2[\"{x,y,s,t,b}\"] --- N12[\"{x,y,s,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N12[\"{x,y,s,t}\"] \n    N3[\"{input,y,s,t,b}\"] --- N13[\"{input,s,t,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N13[\"{input,s,t,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N14[\"{input,y,t,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N14[\"{input,y,t,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N15[\"{input,y,s,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N15[\"{input,y,s,b}\"] \n    N3[\"{input,y,s,t,b}\"] --- N16[\"{input,y,s,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N16[\"{input,y,s,t}\"] \n    N4[\"{input,x,s,t,b}\"] --- N17[\"{input,x,t,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N17[\"{input,x,t,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N18[\"{input,x,s,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N18[\"{input,x,s,b}\"] \n    N4[\"{input,x,s,t,b}\"] --- N19[\"{input,x,s,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N19[\"{input,x,s,t}\"] \n    N5[\"{input,x,y,t,b}\"] --- N20[\"{input,x,y,b}\"] \n    N6[\"{input,x,y,s,b}\"] --- N20[\"{input,x,y,b}\"] \n    N5[\"{input,x,y,t,b}\"] --- N21[\"{input,x,y,t}\"] \n    N7[\"{input,x,y,s,t}\"] --- N21[\"{input,x,y,t}\"] \n    N6[\"{input,x,y,s,b}\"] --- N22[\"{input,x,y,s}\"] \n    N7[\"{input,x,y,s,t}\"] --- N22[\"{input,x,y,s}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N2[\"{x,y,s,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N3[\"{input,y,s,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N4[\"{input,x,s,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N5[\"{input,x,y,t,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N6[\"{input,x,y,s,b}\"] \n    N1[\"{input,x,y,s,t,b}\"] --- N7[\"{input,x,y,s,t}\"] </code></pre> <p>In the above lattice, the \\(\\top\\) is the full set of \\(V\\) and the \\(\\bot\\) is the empty set \\(\\{\\}\\). The order \\(\\subseteq\\) is the subset relation \\(\\sqsubseteq\\).</p>"},{"location":"liveness_analysis/#defining-the-monotone-constraint-for-liveness-analysis","title":"Defining the Monotone Constraint for Liveness Analysis","text":"<p>In Sign Analysis the state variable \\(s_i\\) denotes the mapping of the variables to the sign abstract values after the instruction \\(i\\) is executed.</p> <p>In Liveness Analysis, we define the state variable \\(s_i\\) as the set of variables may live before the execution of the instruction \\(i\\).</p> <p>In Sign Analysis the \\(join(s_i)\\) function is defined as the  least upper bound of all the states that are preceding \\(s_i\\) in the control flow.</p> <p>In Liveness Analysis, we define the \\(join(s_i)\\) function as follows</p> \\[ join(s_i) = \\bigsqcup succ(s_i) \\] <p>where \\(succ(s_i)\\) returns the set of successors of \\(s_i\\) according to the control flow graph.</p> <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l:ret\\), \\(s_l = \\{\\}\\)</li> <li>case \\(l: t \\leftarrow src\\), \\(s_l = join(s_l) - \\{ t \\} \\cup var(src)\\)</li> <li>case \\(l: t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) - \\{t\\} \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: r \\leftarrow src\\), \\(s_l = join(s_l) \\cup var(src)\\)</li> <li>case \\(l: r \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\cup var(src_1) \\cup var(src_2)\\)</li> <li>case \\(l: ifn\\ t\\ goto\\ l'\\), \\(s_l = join(s_l) \\cup \\{ t \\}\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>The helper function \\(var(src)\\) returns the set of variables (either empty or singleton) from operand \\(src\\).</p> \\[ \\begin{array}{rcl} var(r) &amp; = &amp; \\{ \\} \\\\  var(t) &amp; = &amp; \\{ t \\} \\\\  var(c) &amp; = &amp; \\{ \\} \\end{array} \\] <p>By applying the <code>PA</code> program above we have</p> <p><pre><code>s11 = {}\ns10 = join(s10) U {s}               = {s}\ns9  = join(s9)                      = s4\ns8  = (join(s8) - {s}) U {s, y}     = (s9 - {s}) U {s, y}\ns7  = (join(s7) - {t}) U {s}        = (s8 - {t}) U {s}\ns6  = (join(s6) - {y}) U {y}        = (s7 - {y}) U {y}\ns5  = join(s5) U {b}                = s6 U s10 U {b}\ns4  = (join(s4) - {b}) U {y, x}     = (s5 - {b}) U {y, x}\ns3  = join(s3) - {s}                = s4 - {s}\ns2  = join(s2) - {y}                = s3 - {y}\ns1  = (join(s1) - {x}) U {input}    = (s2 - {x}) U {input}\n</code></pre> For the ease of seeing the change of \"flowing\" direction, we order the state variables in descending order.</p> <p>By turning the above equation system to a monotonic function</p> \\[ \\begin{array}{rcl} f_1(s_{11}, s_{10}, s_9, s_8, s_7, s_6, s_5, s_4, s_3, s_2, s_1) &amp; = &amp; \\left (     \\begin{array}{c}      \\{\\}, \\\\       \\{s\\}, \\\\      s_4, \\\\      (s_9 -\\{s\\}) \\cup \\{s,y\\}, \\\\      (s_8 - \\{t\\}) \\cup \\{s\\}, \\\\      (s_7 - \\{y\\}) \\cup \\{y\\}, \\\\      s_6 \\cup s_{10} \\cup \\{b\\}, \\\\      (s_5 - \\{b\\}) \\cup \\{y, x\\}, \\\\      s_4 - \\{s\\}, \\\\      s_3 - \\{y\\}, \\\\      (s_2 - \\{x\\}) \\cup \\{ input \\}     \\end{array}      \\right ) \\end{array} \\] <p>Question, can you show that \\(f_1\\) is a monotonic function?</p> <p>By applying the naive fixed point algorithm (or its optimized version) with starting states <code>s1 = ... = s11 = {}</code>, we solve the above constraints and find</p> <pre><code>s11 = {}\ns10 = {s}\ns9  = {y,x,s}\ns8  = {y,x,s}\ns7  = {y,x,s}\ns6  = {y,x,s}\ns5  = {y,x,s,b}\ns4  = {y,x,s}\ns3  = {y, x}\ns2  = {x}\ns1  = {input}\n</code></pre> <p>From which we can identify at least two possible optimization opportunities.</p> <ol> <li><code>t</code> is must be dead throughout the entire program. Hence instruction <code>7</code> is redundant.</li> <li><code>input</code> only lives at instruction 1. If it is not holding any heap references, it can be freed. </li> <li><code>x,y,b</code> lives until instruction 9. If they are not holding any heap references, they can be freed.</li> </ol>"},{"location":"liveness_analysis/#forward-vs-backward-analysis","title":"Forward vs Backward Analysis","text":"<p>Given an analysis in which the monotone equations are defined by deriving the current state based on the predecessors's states, we call this analysis a forward analysis. </p> <p>Given an analysis in which the monotone equations are defined by deriving the current state based on the successor's states, we call this analysis a  backward analysis.</p> <p>For instance, the sign analysis is a forward analysis and the liveness analysis is a backward analysis.</p>"},{"location":"liveness_analysis/#may-analysis-vs-must-analysis","title":"May Analysis vs Must Analysis","text":"<p>Given an analysis that makes use of powerset lattice, it is a may analysis if it gives an over-approximation. For example, liveness analysis analyses the set of variables that may be \"live\" at a program point. </p> <p>Given an analysis that makes use of powerset lattice, it is a must analysis if it gives an under-approximation. For example, if we negate the result of a liveness analysis to analyse the set of variables that must be \"dead\" at a program point. In this analysis we can keep track of the set of variables must be dead and use \\(\\sqcap\\) (which is \\(\\cap\\)) instead of \\(\\sqcup\\) (which is \\(\\cup\\)).</p>"},{"location":"memory_management/","title":"50.054 - Memory Management","text":""},{"location":"memory_management/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Extend PA to support function call and array operations</li> <li>Extend the dynamic semantics to model the run-time memory operations</li> <li>Define activation records</li> <li>Extend SIMP to support function call and array operations</li> <li>Describe the challenges of memory management</li> <li>Apply linear type system to ensure memory saftey in SIMP.</li> </ol>"},{"location":"memory_management/#pseudo-assembly-extended","title":"Pseudo Assembly (Extended)","text":"<p>So far, we have been dealing with a toy language without function call nor complex data structure. We consider extending the Pseudo Assembly language syntax as follows</p> \\[ \\begin{array}{rccl} (\\tt Instruction)   &amp; i   &amp; ::= &amp; ... \\mid begin\\ f\\ d \\mid call\\ f\\ s \\mid d \\leftarrow alloc\\ s \\mid free\\ s  \\\\  &amp; &amp;  &amp; \\mid  d \\leftarrow ref\\ s \\mid deref\\ s\\ s\\\\  \\end{array} \\] <p>Besides the existing instructions, we include</p> <ul> <li>\\(begin\\ f\\ d\\) - denotes the start of a function name \\(f\\) (\\(f\\) is a variable) and the formal argument (operand) \\(d\\). </li> <li>\\(call\\ f\\ s\\) - denotes a function invocation of \\(f\\) with actual argument \\(s\\).</li> <li>\\(d \\leftarrow alloc\\ s\\) - denotes the memory allocation. It allocates \\(s\\) bytes of unoccupied memory and assigns the reference address to \\(d\\). </li> <li>\\(free\\ s\\) - deallocates the allocated memory at address stored in \\(s\\). </li> <li>\\(d \\leftarrow ref\\ s\\) - references the value at the memory address stored in \\(s\\) and copies it to \\(d\\). </li> <li>\\(deref\\ s_1\\ s_2\\) - dereferences the memory location stored in \\(s_1\\) and updates with the value of \\(s_2\\). </li> </ul>"},{"location":"memory_management/#operational-semantics-for-extended-pa","title":"Operational Semantics for Extended PA","text":"<p>We extend the operational semantics of Pseudo Assembly defined in here. Instead of mixing the temp variable-to-constant mappings and register-to-constant mappings in \\(L\\), we move the register-to-constant mappings in \\(R\\). </p> \\[ \\begin{array}{rccl} (\\tt PA\\ Stack\\ Frame) &amp; L &amp; \\subseteq &amp; (t \\times c) \\\\  (\\tt PA\\ Register\\ Environment) &amp; R &amp; \\subseteq &amp; (r \\times c) \\\\  (\\tt PA\\ Heap\\ Registry) &amp; G &amp; \\subseteq &amp; (loc \\times loc ) \\\\  (\\tt PA\\ Heap\\ Memory) &amp; H &amp; \\subseteq &amp; (loc \\times c) \\\\  (\\tt Heap\\ Address) &amp; loc &amp; ::= &amp; loc(1) \\mid ... \\mid loc(n) \\\\  (\\tt PA\\ Memory\\ Environment) &amp; M &amp; ::= &amp; (\\overline{L}, \\overline{l}, H, G, R)  \\end{array} \\] <ul> <li>\\(G\\) - denotes a set of address tuples. The first address in the tuple denotes the starting address of the allocated memory in the heap (inclusive) and the second one denotes the ending address (exclusive).</li> <li>\\(H\\) - denotes the mapping from addresses to constants. \\(G\\) and \\(H\\) together model the run-time heap memory</li> <li>\\(M\\) - a tuple of 5 items. A stack of stack frames \\(\\overline{L}\\), a stack of function invocation labels \\(\\overline{l}\\) (a sequence of labels marking the function calling instructions), the heap and the register environment. \\(\\overline{L}\\) and \\(\\overline{l}\\) should have the same size. Given an index \\(i\\), the \\(i\\)-th elements in \\(\\overline{L}\\) and \\(\\overline{l}\\) form the activation record.</li> </ul> <p>The small step operational semantic rules in shape \\(P \\vdash (L, li) \\longrightarrow (L, li)\\) introduced in our earlier class are modified to have shape of \\(P \\vdash (M, li) \\longrightarrow (M, li)\\).  </p> <p>We highlight the important rules</p> \\[ \\begin{array}{cc} {\\tt (pAlloc)} &amp;  \\begin{array}{c}         M = (L:\\overline{L}, \\overline{l}, H, G, R) \\ \\ \\ \\          findfree(G,L(s)) = loc \\\\          H' = H\\cup[(loc + i, 0) \\mid i \\in \\{0, L(s)-1\\}] \\ \\ \\ G' = G \\cup[(loc, loc+L(s))] \\\\          M' = (L\\oplus(d,loc):\\overline{L}, \\overline{l}, H', G',  R)         \\\\ \\hline         P \\vdash (M, l: d \\leftarrow alloc\\ s) \\longrightarrow (M', P(l+1))           \\end{array} \\\\ \\\\  {\\tt (pFree)} &amp; \\begin{array}{c}         M = (L:\\overline{L}, \\overline{l}, H, G, R) \\ \\ \\ \\ loc = L(s) \\\\          G'\\cup[(loc,loc')] = G \\ \\ \\ \\ H' = H - \\{ loc, ..., loc' \\} \\ \\ \\ \\         M' = (L:\\overline{L}, \\overline{l}, H', G', R) \\\\ \\hline         P \\vdash (M, l: free\\ s) \\longrightarrow (M', P(l+1))         \\end{array} \\\\ \\\\  {\\tt (pRef)} &amp; \\begin{array}{c}         M = (L:\\overline{L}, \\overline{l}, H, G, R) \\ \\ \\ \\ loc = L(s) \\\\          \\exists (loc_1, loc_2) \\in G. loc_1 \\leq loc &lt; loc_2 \\\\          v = H(loc) \\ \\ \\ M' = (L\\oplus(d,v):\\overline{L}, \\overline{l}, H, G, R)         \\\\ \\hline         P \\vdash ( M, l: d \\leftarrow ref\\ s) \\longrightarrow (M', P(l+1))         \\end{array} \\\\ \\\\  {\\tt (pDeref)} &amp; \\begin{array}{c}          M = (L:\\overline{L}, \\overline{l}, H,G, R) \\ \\ \\ \\ loc = L(s_1) \\\\         \\exists (loc_1, loc_2) \\in G. loc_1 \\leq loc &lt; loc_2 \\\\          H' = H \\oplus (loc, L(s_2)) \\ \\ \\ M' = (L:\\overline{L}, \\overline{l}, H', G, R)\\\\ \\hline         P \\vdash (M ,l: deref\\ s_1\\ s_2) \\longrightarrow (M', P(l+1))         \\end{array} \\end{array}  \\] <ul> <li>The rule \\((\\tt pAlloc)\\) defines the memory allocation routine. Given the asking size, \\(L(s)\\), we make use of the run-time built-in function \\(findfree()\\) to locate the starting address of the free memory region. We save the starting address in \\(d\\), and \"zero-out\" the allocated region addresses ranging from \\(loc\\) to \\(loc+L(s)\\). </li> <li>The rule \\((\\tt pFree)\\) defines the memory deallocation routine. Given the starting address of the memory to be freed \\(loc\\), we remove the pair \\((loc, loc')\\) from \\(G\\) and the keys in the range \\((loc, loc')\\)  from \\(H\\).</li> <li>The rule \\((\\tt pRef)\\) defines the memory reference operation. Given the referenced address \\(loc\\), we ensure the address is in \\(G\\) (which implies it is in \\(H\\)). We store the value \\(H(loc)\\)  into the stack frame and move onto the next instruction. </li> <li>The rule \\((\\tt pDeref)\\) defines the memory deference operation. Given the dereferenced address \\(loc\\), we ensure the address is in \\(G\\) (which implies it is in \\(H\\)). We update the value in \\(H\\) at location \\(loc\\) to \\(L(s_2)\\).</li> </ul> \\[ \\begin{array}{rc} {\\tt (pCall)} &amp;  \\begin{array}{c}         l': begin\\ f\\ d' \\in P \\ \\ \\ M = (L:\\overline{L}, \\overline{l}, H, G, R) \\\\          M' = (\\{(d', L(s))\\}:L:\\overline{L}, l:\\overline{l}, H, G, R)         \\\\ \\hline         P \\vdash (M, l: d \\leftarrow call\\ f\\ s) \\longrightarrow (M', P(l'+1))           \\end{array} \\\\ \\\\  {\\tt (pBegin)} &amp; \\begin{array}{c}         (l':ret) \\in P \\ \\ \\ \\forall (l'':ret) \\in P: l''&gt;= l \\implies l'' &gt;= l'         \\\\ \\hline         P \\vdash (M, l:begin\\ f\\ d) \\longrightarrow (M, P(l'+1))          \\end{array} \\\\ \\\\  {\\tt (pRet1)} &amp;  \\begin{array}{c}         M = (L':L:\\overline{L}, l':\\overline{l}, H, G, R)\\ \\ \\          l': d \\leftarrow call\\ f\\ s \\in P \\\\           R' = R - {r_{ret}} \\ \\ \\          M' = (L\\oplus(d,R(r_{ret})):\\overline{L}, \\overline{l}, H, G, R')         \\\\ \\hline         P \\vdash (M, l:ret) \\longrightarrow (M', P(l'+1))           \\end{array} \\\\ \\\\  {\\tt (pRet2)} &amp;  \\begin{array}{c}         M = (\\overline{L}, [], H, G, R)\\ \\ \\          \\\\ \\hline         P \\vdash (M, l:ret) \\longrightarrow exit(R(r_{ret}))         \\end{array}  \\end{array} \\] <ul> <li>The rule \\((\\tt pCall)\\) handles the function call instruction. In this case, we search for the begin statement of the callee. We push the new stack frame into the stack with the binding of the input argument. We push the caller's label into the labels stack. The executation context is shifted to the function body instruction. </li> <li>The rule \\((\\tt pBegin)\\) processes the begin instruction. Since it is the defining the function, we skip the function body and move to the instruction that follows the return instruction. </li> <li>The rule \\((\\tt pRet1)\\) manages the termination of a function call. We pop the stack frame and the top label \\(l'\\) from the stack. We search for the caller instruction by the label \\(l'\\). We update the caller's stack frame with the returned value of the function call. </li> <li>The rule \\({\\tt pRet2}\\) defines the termination of the entire program.</li> </ul> <p>We omit the rest of rules as we need to change the \\(L\\) to \\(M = (L:\\overline{L}, \\overline{l}, H, G, R)\\).</p> <p>For example given a PA program </p> <pre><code>// PA1\n1: begin plus1 x\n2: y    &lt;- x + 1\n3: rret &lt;- y\n4: ret\n5: z    &lt;- call plus1 0 \n6: rret &lt;- z\n7: ret\n</code></pre> <p>We have the following derivation</p> <pre><code>P |- ([[]], [], [], [], []), 1: begin plus1 x  ---&gt; # (pBegin)\nP |- ([[]], [], [], [], []), 5: z &lt;- call plus1 0 ---&gt; # (pCall) \nP |- ([[(x,0)],[]],[5], [], [], []), 2: y &lt;- x + 1 ---&gt; # (pOp)\nP |- ([[(x,0),(y,1)],[]],[5], [], [], []), 3: rret &lt;- y ---&gt; # (pTempVar)\nP |- ([[(x,0),(y,1)],[]],[5], [], [], [(rret,1)], 4: ret  ---&gt; # (pRet1)\nP |- ([[(z,1)]],[], [], [], []), 6: rret &lt;- z ---&gt; # (pTempVar) \nP |- ([[(z,1)]],[], [], [], [(rret,1)]) 7: ret ---&gt; # (pRet2)\nP |- exit()\n</code></pre>"},{"location":"memory_management/#optional-content-call-stack-design-in-target-platform","title":"(Optional Content) Call Stack Design in Target Platform","text":"<p>The call stack in the target platform is often implemented as a sequence of memory locations. The bottom of the stack has the lowest address of the entire stack and the top of the stack has the highest address (at a particular point in time.)</p> frame for <code>main()</code> frame for <code>plus1(x)</code> ... <p>If we zoom into the frame for <code>plus1(x)</code> </p> address content fp-4 param x fp caller label/return address fp+4 tempvar y <p>the frame pointer <code>fp</code> marks the memory address where ther caller's label/address is stored. If we subtract the parameter size offset from <code>fp</code>, say <code>fp-4</code>, we can access the paramter <code>x</code> and if we add the variable size offset to <code>fp</code>, we access the temp variables, in this case we can statically determine the size of the call frame, as 3 * 4 bytes. As a convention, the <code>begin</code> instruction in the target code is associated with the frame size required by this function.  When we make a function call in the target code, we have to push the parameter into the call stack one by one. </p> <p>i.e. the instruction <code>5</code> in the above example <code>PA1</code> should be broken into </p> <pre><code>5.1 param 0\n5.2 call plus1\n5.3 popframe 12\n</code></pre> <ul> <li>At 5.1, we push the actual argument as the parameter <code>x</code>. </li> <li>At 5.2, we call the function and shift the program counter to the starting label/address of the function body.</li> <li>When the function terminates, we jump back to 5.2, then at 5.3, we pop the stack frame based on its size.</li> </ul> <p>In case a function has multiple parameters, the parameters are pushed from  right to left. </p> <p>For example, if we have a <code>min(x,y)</code> function which has no local variable and we call <code>min(-10,9)</code>, we generate the following target code</p> <pre><code>1: param 9\n2: param -10\n3: call min\n4: popframe 12\n</code></pre>"},{"location":"memory_management/#another-example-with-heap-memory-access","title":"Another example with heap memory access","text":"<p>The following PA program is an example of using the memory from the heap. </p> <pre><code>// PA2 \n1:  begin range x \n2:  s &lt;- 4 * x\n3:  a &lt;- alloc s\n4:  i &lt;- 0\n5:  t &lt;- i &lt; x \n6:  ifn t goto 11\n7:  ai &lt;- a + i\n8:  deref ai i \n9:  i &lt;- i + 1\n10: goto 5\n11: rret &lt;- a\n12: ret\n13: r &lt;- call range 3\n14: y &lt;- ref r 2\n15: free r\n16: rret &lt;- y\n17: ret\n</code></pre> <p>Instructions 1-12 define the <code>range(x)</code> function, which initializes an array with the given size <code>x</code> and the values are from <code>0</code> to <code>x</code>. Lines 13-14 invoke the function and access the 3<sup>rd</sup> element. Line 15 frees the memory.</p>"},{"location":"memory_management/#cohort-exercise","title":"Cohort Exercise","text":"<p>As an exercise, can you work out the derivation of \"running\" the above program <code>PA2</code> using the operational semantics?</p>"},{"location":"memory_management/#simp-extended-with-function-and-array","title":"SIMP (extended with function and array)","text":"<p>We consider syntax of the SIMP language extended with function and array.</p> \\[ \\begin{array}{rccl} (\\tt Statement) &amp; S &amp; ::= &amp; ... \\mid X[E] = E  \\mid free\\ X \\\\  (\\tt Expression) &amp; E &amp; ::= &amp; ... \\mid f(E) \\mid T[E] \\mid X[E] \\\\ (\\tt Function\\ Declaration) &amp; D &amp; ::= &amp; func\\ f\\ (x:T)\\ T\\ \\{\\overline{S}\\} \\\\  (\\tt Constant) &amp; C &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\  (\\tt Type) &amp; T &amp; ::= &amp; ... \\mid [T] \\mid T \\rightarrow T \\mid unit \\\\  (\\tt Value) &amp; V &amp; ::= &amp; C \\mid D \\mid (loc, loc) \\mid unit \\\\ (\\tt Program) &amp; P &amp; ::= &amp; \\overline{D};\\overline{S} \\end{array} \\] <p>New statement syntax includes:</p> <ul> <li>\\(X[E] = E\\) - denotes an array element assignment statement</li> <li>\\(free\\ X\\) - dellocate the array referenced by variable \\(X\\). </li> </ul> <p>New expression syntax includes</p> <ul> <li>\\(f(E)\\) - function application, where \\(f\\) is a function name (a special variable)</li> <li>\\(T[E]\\) - array initialization, where \\(T\\) is the element type of the array and \\(E\\) denotes the size of the array.</li> <li>\\(X[E]\\) - array element dereference, where \\(X\\) is an array and \\(E\\) is the element index. </li> </ul> <p>Function declaration syntax includes</p> <ul> <li>\\(func\\ f\\ (x:T)\\ T\\ \\{\\overline{S}\\}\\) - \\(f\\) is the function name; \\((x:T)\\) is the formal parameter and its type. The second \\(T\\) is the return type. \\(\\overline{S}\\) is the body.</li> </ul> <p>New type sytanx includes</p> <ul> <li>\\([T]\\) - array type</li> <li>\\(T \\rightarrow T\\) - function type</li> <li>\\(unit\\) - the type has only one value \\(unit\\) (its role is similar to the \\(void\\) type in Java and C)</li> </ul> <p>Value syntax includes</p> <ul> <li>\\(C\\) - constant</li> <li>\\(D\\) - declaration</li> <li>\\((loc, loc)\\) - memory segment, the first item is the starting address (inclusive) and the second item is the ending address (exclusive)</li> <li>\\(unit\\) - the only value for the \\(unit\\) type.</li> </ul> <p>A SIMP program is a sequence of function declarations followed by a sequence of statements. </p> <p>For example, the following SIMP program is equivalent to Pseudo Assembly program <code>PA1</code> introduced earlier.</p> <pre><code>// SIMP1 \nfunc plus1 (x:int) int {\n    y = x + 1;\n    return y;\n}\nz = plus1(0);\nreturn z;\n</code></pre> <p>The following SIMP program is equivalent to Pseudo Assembly program <code>PA2</code></p> <pre><code>// SIMP2 array enumeration\nfunc range(x:int) [int] {\n    a = int[x];\n    i = 0;\n    while i &lt; x {\n        a[i] = i;\n        i = i + 1;\n    }\n    return a;\n}\nr = range(3); \ny = r[2];\nfree r;\nreturn y;\n</code></pre>"},{"location":"memory_management/#operational-semantic-of-extended-simp","title":"Operational Semantic of extended SIMP","text":"<p>For brevity, we consider the big step operational semantics of extended SIMP by extending the rules.</p> \\[ \\begin{array}{rccl} (\\tt SIMP\\ Var\\ Environment) &amp; \\Delta &amp; \\subseteq &amp; (X \\times V) \\\\  (\\tt SIMP\\ Obj\\ Environment) &amp; \\rho &amp; \\subseteq &amp; (loc \\times V) \\end{array} \\] <p>First and foremost, in the extended SIMP, values include function declarations, memory tuples and unit, besides constants. Hence the \\(\\Delta\\) environment maps variables to values.  Besides the variable environment \\(\\Delta\\), we define the object (heap) environment \\(\\rho\\) as a mapping from memory location to values.</p>"},{"location":"memory_management/#big-step-operational-semantics-for-extended-simp-expression","title":"Big Step Operational Semantics for extended SIMP expression","text":"<p>The rules of shape \\(\\Delta \\vdash E \\Downarrow C\\) have to be adapted to the shape of \\(\\overline{\\Delta} \\vdash (\\rho, E) \\Downarrow (\\rho', V)\\), where \\(\\overline{\\Delta}\\) is the stack of variable environments.</p> <p>For example, the \\((\\tt bVar)\\) rule is updated as follows</p> \\[ {\\tt (bVar)} ~~~~ \\Delta:\\overline{\\Delta} \\vdash (\\rho, X) \\Downarrow (\\rho,\\Delta(X)) \\] <p>The rest of the rules can be easily adapted to the new scheme. Now we consider cases for the new syntax.</p> \\[ {\\tt (bApp)} ~~~~ \\begin{array}{c}         \\overline{\\Delta} \\vdash (\\rho, E_2) \\Downarrow (\\rho_2,V_2) \\\\          (f, func\\ f\\ (x:T_1)T_2 \\{\\overline{S}\\}) \\in \\overline{\\Delta} \\\\          \\Delta =  \\{(x,V_2)\\} \\ \\ \\ (\\Delta:\\overline{\\Delta}, \\rho_2, \\overline{S}) \\Downarrow (\\Delta':\\overline{\\Delta'}, \\rho_3, return\\ y) \\ \\ \\ (y, V)\\in \\Delta'         \\\\ \\hline         \\overline{\\Delta} \\vdash (\\rho, f(E_2)) \\Downarrow (\\rho_3, V)         \\end{array} \\] <p>In case of a function application, we first evaluate the function argument into a value. We search for the function definition from the variable environment \\(\\overline{\\Delta}\\) (in the order from the top frame to the bottom frame). A new variable environment (frame) \\(\\Delta\\) is created to store the binding between the function's formal argument and the actual argument. We then call the statement evaluation rules (to be discussed shortly) to run the body the of the function.  Finally, we retrieve the returned value from the call.</p> \\[ {\\tt (bArrInst)} ~~~~ \\begin{array}{c}         \\overline{\\Delta} \\vdash (\\rho, E_2) \\Downarrow (\\rho_1,V_2) \\\\          \\forall x \\in [m, m+V_2). loc(x) \\not\\in dom(\\rho) \\\\          V = default(T) \\ \\ \\         \\rho' = \\rho \\cup \\{ (loc(x), V) \\mid x \\in [m, m+V_2) \\}          \\\\ \\hline         \\overline{\\Delta} \\vdash (\\rho, T[E_2]) \\Downarrow (\\rho', (loc(m), loc(m+V_2)))         \\end{array} \\] <p>In case of an array instantiation, we first evaluate the size argument into a value (must be an integer constant). We find a sequence of unsused memory locations \\(loc(m)\\) to \\(loc(m+V_2)\\) and initialize the value to the default value. </p> \\[ {\\tt (bArrRef)} ~~~~ \\begin{array}{c}         (X, (loc(m_1), loc(m_2))) \\in \\Delta \\\\         \\Delta:\\overline{\\Delta} \\vdash (\\rho, E_2) \\Downarrow (\\rho_2, V_2) \\ \\ \\ \\         m_1 + V_2 &lt; m_2         \\\\ \\hline         \\Delta:\\overline{\\Delta} \\vdash (\\rho, X[E_2]) \\Downarrow (\\rho_2, \\rho_2(loc(m_1 + V_2))         \\end{array} \\] <p>In case of an array reference, we lookup the memory location boundaries of \\(X\\), then we evaluate \\(E_2\\) into a constant (integer) \\(V_2\\). If the index is within the boundary, we lookup  the value associated with the address. </p>"},{"location":"memory_management/#big-step-operational-semantics-for-extended-simp-statement","title":"Big Step Operational Semantics for extended SIMP statement","text":"<p>Similarly, to support the change of SIMP statement,  we adapt the big step oeprational semantics rule of shape \\((\\Delta, S) \\Downarrow \\Delta\\) to shape \\((\\overline{\\Delta}, \\rho, S) \\Downarrow (\\overline{\\Delta'}, \\rho', S')\\)</p> <p>For example the assignment statement rule is updated as follows</p> \\[ \\begin{array}{rc} {\\tt (bAssign)} &amp; \\begin{array}{c}     \\Delta:\\overline{\\Delta} \\vdash (\\rho,E) \\Downarrow (\\rho',V)     \\\\ \\hline     (\\Delta:\\overline{\\Delta}, \\rho, X = E) \\Downarrow (\\Delta \\oplus (X, V):\\overline{\\Delta}, \\rho', nop)     \\end{array} \\end{array} \\] <p>We focus on the new rules and omit the rest of the rules</p> \\[ \\begin{array}{rc} {\\tt (bArrDeref)} &amp; \\begin{array}{c}     \\Delta:\\overline{\\Delta} \\vdash (\\rho,E_1) \\Downarrow (\\rho_1,V_1) \\ \\ \\ \\      \\Delta:\\overline{\\Delta} \\vdash (\\rho_1,E_2) \\Downarrow (\\rho_2,V_2) \\\\      (X, (loc(m_1), loc(m_2))) \\in \\Delta \\ \\ \\ m_1 + V_1 &lt; m_2 \\\\      \\rho_3 = \\rho_2 \\oplus (loc(m_1 + V_1), V_2)     \\\\ \\hline     (\\Delta:\\overline{\\Delta}, \\rho, X[E_1] = E_2) \\Downarrow (\\Delta:\\overline{\\Delta}, \\rho_3, nop)     \\end{array} \\end{array} \\] <p>In case of array deference, we first compute the index argument into an integer constant \\(V_1\\). We lookup the memory range \\((loc(m_1), loc(m_2))\\) of \\(X\\) and ensure that the \\(m_1 + V_1\\) is within range. Evaluating \\(E_2\\) yields the value to be assigned to the memory location \\(loc(m_1 + V_1)\\). Finally we return the updated object memory environment.</p> \\[ \\begin{array}{rc} {\\tt (bFree)} &amp; \\begin{array}{c}     (X, (loc(m_1), loc(m_2))) \\in \\Delta \\ \\ \\ \\ \\      \\rho' \\cup \\{ (loc(x),V) \\mid x \\in [m_1, m_2)\\} = \\rho     \\\\ \\hline     (\\Delta:\\overline{\\Delta}, \\rho, free\\ X) \\Downarrow (\\Delta - ((X, (loc(m_1), loc(m_2)))):\\overline{\\Delta}, \\rho', nop)     \\end{array} \\end{array} \\] <p>In case of free statement, we ensure that the argument is a variable that holding some reference to the object envrionment.  We remove the memory assignment from \\(\\rho\\) and remove \\(X\\) from \\(\\Delta\\).  (In some system, \\(X\\) is not removed from \\(\\Delta\\), which causes the \"double-freeing\" error.)</p>"},{"location":"memory_management/#big-step-operational-semantics-for-extended-simp-program","title":"Big Step Operational Semantics for extended SIMP Program","text":"\\[ \\begin{array}{rc} {\\tt (bProg)} &amp; \\begin{array}{c}         \\Delta' = \\Delta \\oplus(f, func\\ f\\ (X:T_1)T_2\\{\\overline{S'}\\}) \\\\         (\\Delta':\\overline{\\Delta}, \\rho, \\overline{D};\\overline{S}) \\Downarrow (\\overline{\\Delta''}, \\rho', return\\ X)         \\\\ \\hline         (\\Delta:\\overline{\\Delta}, \\rho, func\\ f\\ (X:T_1)T_2\\{\\overline{S}\\}; \\overline{D};\\overline{S}) \\Downarrow (\\overline{\\Delta''}, \\rho', return\\ X)         \\end{array} \\\\ \\\\  {\\tt (bSeq)} &amp; \\begin{array}{c}         (\\overline{\\Delta}, \\rho, S) \\Downarrow (\\overline{\\Delta}', \\rho', nop) \\\\         (\\overline{\\Delta'}, \\rho', \\overline{S}) \\Downarrow (\\overline{\\Delta''}, \\rho'', return\\ X)         \\\\ \\hline         \\overline{\\Delta}, \\rho, S;\\overline{S}) \\Downarrow (\\overline{\\Delta''}, \\rho'', return\\ X)         \\end{array} \\end{array} \\] <p>The above two rules define the execution of a SIMP program and statement sequences. The rule \\((\\tt bProg)\\) records the variable \\(f\\) to function definition binding in \\(\\Delta'\\) and we use \\(\\Delta':\\overline{\\Delta}\\) to evaluate the rest of the evaluation.  The rule \\((\\tt bSeq)\\) evaluates the first statement until it becomes \\(nop\\) and moves on the the rest of the statement.</p> <p>For example, running the <code>SIMP1</code> program yields the following</p> <pre><code>[{plus1: func plus1 (...)}] |- ({} 0) \u21d3 ({},0) (bConst)\n\n(plus1: func plus1 (...)) in {plus1: func plus1 (...)}  [sub tree 1]\n--------------------------------------------------------------(bApp)\n[{plus1: func plus1 (...)}] |- ({}, plus1(0)) \u21d3 ({},1) \n--------------------------------------------------------------------(bAssign)    \n[{plus1: func plus1 (...)}], {}, z = plus1(0); \u21d3 [{plus1: func plus1 (...), z:1}], {}, nop;    \n-------------------------------------------------------------------------(bSeq)\n[{plus1, func plus1 (...)}], {}, z = plus1(0); return z; \u21d3 [{plus1: func plus1 (...), z:1}], {}, return z\n---------------------------------------------------------------------------- (bProg)\n[],{}, \nfunc plus1 (x:int) int {\n    y = x + 1;\n    return y;\n}\nz = plus1(0);\nreturn z; \u21d3 [{plus1: func plus1 (...), z:1}], {}, return z\n</code></pre> <p>where sub derivation<code>[sub tree 1]</code> is as follows</p> <pre><code>[{x:0}, {plus1: func plus1 (...)}] |- ({}, x) \u21d3 ({}, 0) (bVar)\n\n[{x:0}, {plus1: func plus1 (...)}] |- ({}, 1) \u21d3 ({}, 1) (bConst)\n------------------------------------------------------------ (bOp)\n[{x:0}, {plus1: func plus1 (...)}] |- ({}, x + 1) \u21d3 ({}, 1)\n-------------------------------------------------------------------------(bAssign)\n[{x:0}, {plus1: func plus1 (...)}], {}, y = x + 1; \u21d3 [{x:0,y:1}, {plus1: func plus1 (...)}], nop\n---------------------------------------------------------------------------------------------(bSeq)\n[{x:0}, {plus1: func plus1 (...)}], {}, y = x + 1; return y; \n\u21d3 [{x:0,y:1}, {plus1: func plus1 (...)}], {}, return y;\n</code></pre>"},{"location":"memory_management/#cohort-exercise_1","title":"Cohort Exercise","text":"<p>As an exercise, can you work out the derivation of \"running\" the program <code>SIMP2</code> using the big step operational semantics?</p>"},{"location":"memory_management/#simp-to-pa-conversion-extended","title":"SIMP to PA conversion (Extended)","text":"<p>We consider the update to the maximal munch algorithm. </p> \\[  \\begin{array}{rc} {\\tt (m2App)} &amp; \\begin{array}{c}            G_e(E_2) \\vdash (\\hat{e_2}, \\check{e_2} ) \\\\            t \\ {\\tt is\\ a\\ fresh\\ variable} \\\\           l \\ {\\tt is\\ a\\ fresh\\ label}            \\\\ \\hline           G_e(f(E_2)) \\vdash (t, \\check{e_2} + [l: t\\leftarrow call\\ f\\ \\hat{e}])            \\end{array}  \\end{array}   \\] <p>The \\({\\tt (m2App)}\\) rule converts a function application expression to PA instructions.</p> \\[  \\begin{array}{rc} {\\tt (m2ArrRef)} &amp; \\begin{array}{c}            G_e(E_2) \\vdash (\\hat{e_2}, \\check{e_2} ) \\\\            t, t' \\ {\\tt are\\ fresh\\ variables} \\\\           l, l' \\ {\\tt are\\ fresh\\ labels}            \\\\ \\hline           G_e(X[E_2]) \\vdash (t', \\check{e_2} + [l: t\\leftarrow X + \\hat{e_2}, l': t' \\leftarrow deref\\ t])            \\end{array}  \\end{array}   \\] <p>The \\({\\tt (m2ArrRef)}\\) rule converts an array reference expression to PA instructions.</p> \\[  \\begin{array}{rc} {\\tt (m2ArrInst)} &amp; \\begin{array}{c}            G_e(E_2) \\vdash (\\hat{e_2}, \\check{e_2} ) \\ \\ \\ c \\ {\\tt is\\ the size\\ of\\ } T\\\\            t, t' \\ {\\tt are\\ fresh\\ variables} \\\\           l, l' \\ {\\tt are\\ fresh\\ labels}            \\\\ \\hline           G_e(T[E_2]) \\vdash (t', \\check{e_2} + [l: t\\leftarrow c * \\hat{e_2}, l': t' \\leftarrow alloc\\ t])            \\end{array}  \\end{array}   \\] <p>The \\({\\tt (m2ArrInst)}\\) rule converts an array instanstiation expression to PA instructions. Note that we assume that we can assess the size of \\(T\\) in bytes.</p> \\[  \\begin{array}{rc} {\\tt (m2ArrDeref)} &amp; \\begin{array}{c}            G_e(E_1) \\vdash (\\hat{e_1}, \\check{e_2}) \\ \\ \\ \\ G_e(E_2) \\vdash (\\hat{e_2}, \\check{e_2}) \\\\           l, l'  {\\tt\\ are\\ fresh\\ labels}  \\ \\ \\ t \\ {\\tt is\\ a fresh\\ variable} \\\\ \\hline           G_s(X[E_1] = E_2) \\vdash \\check{e_1} + \\check{e_2} + [l: t \\leftarrow X + \\hat{e_1}, l': deref\\ t\\ \\hat{e_2}]           \\end{array}  \\end{array}   \\] <p>The \\({\\tt (m2ArrDeref)}\\) rule converts an array derference assignment statement to PA instructions. </p> \\[  \\begin{array}{rc} {\\tt (m2FuncDecl)} &amp; \\begin{array}{c}            l  {\\tt\\ is\\ a fresh\\ label} \\ \\ \\ G_s(\\overline{S}) \\vdash lis           \\\\ \\hline           G_d(func\\ f(X:T_1)T_2 \\{\\overline{S}\\}) \\vdash [l:begin\\ f\\ x] + lis            \\end{array}  \\end{array}   \\] <p>The \\({\\tt (m2FuncDecl)}\\) rule converts a function declaration into PA instructions.</p> <p>Applying the above algorithm to <code>SIMP1</code> yields <code>PA1</code> and applying to <code>SIMP2</code> produces <code>PA2</code>. </p>"},{"location":"memory_management/#extend-simp-type-checking","title":"Extend SIMP Type checking","text":"<p>We consider extending the static semantic (type checking) of SIMP to support function and array.</p>"},{"location":"memory_management/#type-checking-simp-expression-extended","title":"Type Checking SIMP Expression (Extended)","text":"<p>Let's consider the type checking rules for the new SIMP expressions. Overall the type rule shape remains unchanged. Recall</p> \\[ \\begin{array}{rc} {\\tt (tArrRef)} &amp; \\begin{array}{c}            (X, [T]) \\in \\Gamma \\ \\ \\ \\ \\Gamma \\vdash E_2 : int           \\\\ \\hline           \\Gamma \\vdash X[E_2] : T            \\end{array}  \\end{array}   \\] <p>In the rule \\((\\tt tArrRef)\\) we type check memory reference expression. We validate \\(X\\)'s type is an array type and \\(E_2\\)'s type must be an \\(int\\) type.</p> \\[ \\begin{array}{rc} {\\tt (tArrInst)} &amp; \\begin{array}{c}            \\Gamma \\vdash E_2 : int           \\\\ \\hline           \\Gamma \\vdash T[E_2] : [T]            \\end{array}  \\end{array}   \\] <p>The rule \\((\\tt tArrInst)\\) defines the type checking for array instantiation. The entire expression is of type \\([T]\\) if the size argument \\(E_2\\) is of type \\(int\\).</p> \\[ \\begin{array}{rc} {\\tt (tApp)} &amp; \\begin{array}{c}            \\Gamma \\vdash f : T_1 \\rightarrow T_2 \\ \\ \\ \\ \\Gamma \\vdash E_2 : T_1           \\\\ \\hline           \\Gamma \\vdash f(E_2) : T_2           \\end{array}  \\end{array}   \\] <p>The rule \\((\\tt tApp)\\) defines the type checking for function application. The entire expression is of type  \\(T_2\\) if \\(f\\) has type \\(T_1 \\rightarrow T_2\\) and \\(E_2\\) has type \\(T_1\\).</p> \\[ \\begin{array}{rc} {\\tt (tUnit)} &amp; \\begin{array}{c}            \\Gamma \\vdash unit : unit           \\end{array}  \\end{array} \\] <p>The rule \\((\\tt tUnit)\\) defines the type checking for unit value.</p>"},{"location":"memory_management/#type-checking-simp-statement-extended","title":"Type Checking SIMP Statement (Extended)","text":"<p>For the extended SIMP statement type checking, we need to adjust the typing rules of shape \\(\\Gamma \\vdash S\\) to \\(\\Gamma \\vdash S : T\\). </p> <p>We adjust the typing rules for the standard statements as follows.</p> \\[ \\begin{array}{rc} {\\tt (tAssign)} &amp; \\begin{array}{c}            \\Gamma \\vdash E : T \\ \\ \\ \\ \\Gamma \\vdash X : T           \\\\ \\hline           \\Gamma \\vdash X = E : unit           \\end{array} \\\\ \\\\  {\\tt (tNop)} &amp; \\begin{array}{c}            \\Gamma \\vdash nop : unit           \\end{array} \\\\ \\\\  {\\tt (tReturn)} &amp; \\begin{array}{c}            \\Gamma \\vdash X : T           \\\\ \\hline           \\Gamma \\vdash return\\ X: T           \\end{array} \\\\ \\\\  {\\tt (tSeq)} &amp; \\begin{array}{c}            \\Gamma \\vdash S : T \\ \\ \\ \\ \\Gamma \\vdash \\overline{S}:T'            \\\\ \\hline           \\Gamma \\vdash S;\\overline{S}: T'           \\end{array} \\\\ \\\\  {\\tt (tIf)} &amp; \\begin{array}{c}            \\Gamma \\vdash E : bool \\ \\ \\ \\ \\Gamma \\vdash \\overline{S_1}:T \\ \\ \\ \\  \\Gamma \\vdash \\overline{S_2}:T           \\\\ \\hline           \\Gamma \\vdash if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\}: T           \\end{array} \\\\ \\\\  {\\tt (tWhile)} &amp; \\begin{array}{c}            \\Gamma \\vdash E : bool \\ \\ \\ \\ \\Gamma \\vdash \\overline{S}:T            \\\\ \\hline           \\Gamma \\vdash while\\ E\\ \\{\\overline{S}\\} : T           \\end{array} \\\\ \\\\  \\end{array}  \\] <p>The assignment statement and nop statement are of type \\(unit\\). The return statement  has type \\(T\\) if the return variable \\(X\\) has type \\(T\\). The head and the tail of a sequence of statements are typed indepdently. The two alternatives of an if statement should share the smae time. (In fact, we can be more specific to state that the type of if and while are \\(unit\\), though it is unnecessarily here.)</p> <p>We turn into the typing for new syntax.</p> \\[ \\begin{array}{rc} {\\tt (tFree)} &amp; \\begin{array}{c}            \\Gamma \\vdash X : [T]           \\\\ \\hline           \\Gamma \\vdash free\\ X: unit           \\end{array} \\end{array} \\] <p>The free statement has type \\(unit\\) provided the variable \\(X\\) is having type \\([T]\\).</p> \\[ \\begin{array}{rc} {\\tt (tArrDeref)} &amp; \\begin{array}{c}            \\Gamma \\vdash E_1 : int \\ \\ \\ \\ \\Gamma \\vdash X:[T] \\ \\ \\ \\Gamma \\vdash E_2:T           \\\\ \\hline           \\Gamma \\vdash X[E_1] = E_2: unit           \\end{array}  \\end{array}   \\] <p>The array dereference statement has type \\(unit\\) if the index expression \\(E_1\\) has type \\(int\\), \\(X\\) has type \\([T]\\) and the right hand side \\(E_2\\) has type \\(T\\).</p>"},{"location":"memory_management/#type-checking-simp-declaration-extended","title":"Type Checking SIMP Declaration (Extended)","text":"\\[ \\begin{array}{rc} {\\tt (tFuncDecl)} &amp; \\begin{array}{c}            \\Gamma \\oplus(f:T_1 \\rightarrow T_2)\\oplus(X:T_1) \\vdash \\overline{S}:T_2           \\\\ \\hline           \\Gamma \\vdash func\\ f(X:T_1)T_2 \\{ \\overline{S} \\}: T_1 \\rightarrow T_2           \\end{array} \\\\ \\\\  {\\tt (tProg)} &amp; \\begin{array}{c}            {\\tt for\\ } i \\in [1,n] \\ \\ \\           \\Gamma_i \\vdash D_i : T_i \\ \\ \\           \\Gamma \\vdash \\overline{S}:T           \\\\ \\hline           \\Gamma \\vdash D_1;...;D_n;\\overline{S}: T           \\end{array} \\ \\end{array}  \\] <p>In rule \\((\\tt tFuncDecl)\\), we type check the function declaration by extending the type environment with the type of \\(f\\) and the formal argument \\(X\\) and type check the body. In rule \\((\\tt tProg)\\), we type check the function declarations independently from the main program statement \\(\\overline{S}\\).</p>"},{"location":"memory_management/#example","title":"Example","text":"<p>We find the type checking derivation of the program <code>SIMP1</code></p> <p>Let <code>\u03931= {(y,int)}</code> and <code>\u0393={(plus1,int-&gt;int),(z,int)}</code> <pre><code>\u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- 1 :int(tConst)\n\n(x:int) \u2208 \u03931\u2295(plus1,int-&gt;int)\u2295(x,int)\n-------------------------------------(tVar)\n\u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- x :int        (y:int)\u2208\u03931\n-------------------------------------(tOp) -----------------------------------(tVar)\n\u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- x + 1       \u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |-y:int \n-----------------------------------------------------------------------------(tAssign)\n\u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- y = x + 1:unit        [sub tree 2]                             \n----------------------------------------------------------------------------------------- (tSeq)\n\u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- y = x + 1; return y;:int\n------------------------------------------------------------- (tFuncDecl)\n\u03931 |- func plus1 (x:int) int {\n    y = x + 1;\n    return y;                            [sub tree 3]\n} : int -&gt; int         \n---------------------------------------------------------------------------- (tProg)\n\u0393 |- func plus1 (x:int) int {\n    y = x + 1;\n    return y;\n}\nz = plus1(0);\nreturn z; :int \n</code></pre></p> <p>where [sub tree 2] is </p> <pre><code>y:int \u2208 \u03931\n---------------------------------------(tVar)\n \u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- y:int \n----------------------------------------------------------- (tReturn)\n\u03931\u2295(plus1,int-&gt;int)\u2295(x,int) |- return y;:int\n</code></pre> <p>[sub tree 3] is</p> <pre><code>\u0393 |- 0:int (tConst)\n\n(plus1,int-&gt;int) \u2208 \u0393\n----------------------(tVar)\n\u0393 |- plus1:int -&gt; int             (z,int) \u2208 \u0393\n--------------------------(tApp)  -------(tVar)\n\u0393 |- plus1(0):int                 \u0393 |- z:int       (z,int) \u2208 \u0393\n----------------------------------------(tAssign)  -------(tVar)\n\u0393 |- z=plus1(0);:unit                              \u0393 |- z:int\n----------------------------------------------------------- (tReturn)\n\u0393 |- z=plus1(0);return z;:int\n</code></pre>"},{"location":"memory_management/#cohort-exercise_2","title":"Cohort Exercise","text":"<p>As an exercise, apply the type checking rule to <code>SIMP2</code>.</p>"},{"location":"memory_management/#cohort-exercise_3","title":"Cohort Exercise","text":"<p>As an exercise, develop a type inference algorithm for the extended SIMP.</p>"},{"location":"memory_management/#run-time-errors-caused-by-illegal-memory-operations","title":"Run-time errors caused by illegal memory operations","text":"<p>There are several issues arising with the memory management.</p>"},{"location":"memory_management/#double-freeing","title":"Double-freeing","text":"<p>As motivated earlier, in some system, the operational semantics of the <code>free x</code> statement does not remove the variable <code>x</code> in the stack frame, which causes a double free error. For example</p> <pre><code>// SIMP3 \nfunc f(x:int) int {\n    t = int[x];\n    free t;\n    free t; // run-time error, as t's reference is no longer valid.\n    return x; \n}\ny = f(1);\nreturn y;\n</code></pre>"},{"location":"memory_management/#missing-free","title":"Missing free","text":"<p>On the other hand, missing free statement after array initialization might cause memory leak. </p> <pre><code>// SIMP4\nfunc f(x:int) int {\n    t = int[x];\n    return 1; // unfreed memory unless garbage-collected.\n}\ny = f(1);\nreturn y;\n</code></pre>"},{"location":"memory_management/#array-out-of-bound","title":"Array out of bound","text":"<p>Another common error is array out of bound access. </p> <pre><code>// SIMP5\nfunc f(x:int) int {\n    t = int[x];\n    t[1] = 0; // array out of bound might arise\n    free t;\n    return 0; \n}\ny = f(1);\nreturn y;\n</code></pre> <p>Note that all these three examples are well-typed in the current type checking system. The array out of bound error can be detected via dependent type system (recall GADT example in some earlier class). The other two kinds of errors can be flagged out using Linear Type system.</p>"},{"location":"memory_management/#linear-type-system","title":"Linear Type System","text":"<p>Linear Type was inspired by the linear logic proposed by Jean-Yves Girard. </p> <p>Linear Type System is a popular static semantic design choice to ensure memory safety. It has strong influences in languages such as Cyclone and Rust. </p> <p>The basic principal is that </p> <ol> <li>each variable has only one entry in a type environment \\(\\Gamma\\) (same as the normal type system)</li> <li>after a variable's type assignment from some type environment \\(\\Gamma\\) is used in some proof derivation, it will be removed from \\(\\Gamma\\). </li> </ol>"},{"location":"memory_management/#type-checking-expression-using-linear-type-system","title":"Type Checking Expression using Linear Type System","text":"<p>The linear typing rules for SIMP expression are in the form of \\(\\Gamma \\vdash E : T, \\Gamma'\\) which reads as \"we type check \\(E\\) to have type \\(T\\) under \\(\\Gamma\\), after that \\(\\Gamma\\) becomes \\(\\Gamma'\\).</p> \\[ \\begin{array}{rc} {\\tt (ltVar)} &amp; \\begin{array}{c}            \\Gamma'\\oplus(X,T) = \\Gamma           \\\\ \\hline           \\Gamma \\vdash X : T, \\Gamma'            \\end{array}  \\end{array} \\] <p>The above rule type checks the variable \\(X\\) to have type \\(T\\), this is valid if we can find \\((X,T)\\) in the type environment \\(\\Gamma\\), and we remove that entry from \\(\\Gamma\\) to produce \\(\\Gamma'\\).</p> \\[ \\begin{array}{rc} {\\tt (ltArrInst)} &amp; \\begin{array}{c}            \\Gamma \\vdash E:int, \\Gamma'           \\\\ \\hline           \\Gamma \\vdash T[E] : [T], \\Gamma'            \\end{array}  \\end{array} \\] <p>The array instantion expression is treated as before except that the post-checking type environment is taken into consideration. </p> \\[ \\begin{array}{rc} {\\tt (ltApp)} &amp; \\begin{array}{c}            \\Gamma \\vdash E_2:T_1, \\Gamma' \\ \\ \\ \\ (f, T_1\\rightarrow T_2) \\in \\Gamma'           \\\\ \\hline           \\Gamma \\vdash f(E_2) : T_2, \\Gamma'            \\end{array}   \\end{array} \\] <p>When type checking the function application expression, we linearly type-check the argument \\(E_2\\) to have type \\(T_1\\) and we check the function \\(f\\) is having the function type \\(T_1 \\rightarrow T_2)\\) in the update type environment. (Note that unlike other system, we do not remove the type assignment for functions after \"use\", so that a function can be reused.)</p> \\[ \\begin{array}{rc} {\\tt (ltArrRef)} &amp; \\begin{array}{c}            \\Gamma \\vdash E_2:int, \\Gamma' \\ \\ \\ \\ (X, [T]) \\in \\Gamma'           \\\\ \\hline           \\Gamma \\vdash X[E_2] : T, \\Gamma'            \\end{array}   \\end{array} \\] <p>Similarly, when type checking the array reference expression, we linearly type check the index expression against type \\(int\\), and we check the array variable \\(X\\) is in the type environment \\(\\Gamma'\\) without removing it. </p> <p>We omit the linearly typing rules for the rest of expressions as they contain no surprise. </p>"},{"location":"memory_management/#cohort-exercise_4","title":"Cohort Exercise","text":"<p>Work out the linear typing rules for \\(C\\), \\(unit\\), \\((E)\\) and \\(E\\ op\\ E\\).</p>"},{"location":"memory_management/#type-checking-statement-using-linear-type-system","title":"Type Checking Statement using Linear Type System","text":"<p>The linear typing rules for SIMP statements are in the form of \\(\\Gamma \\vdash S : T, \\Gamma'\\) which reads as \"we type check \\(S\\) to have type \\(T\\) under \\(\\Gamma\\), after that \\(\\Gamma\\) becomes \\(\\Gamma'\\).</p> \\[ \\begin{array}{rc} {\\tt (ltAssign)} &amp; \\begin{array}{c}            \\Gamma \\vdash E:T, \\Gamma'            \\\\ \\hline           \\Gamma \\vdash X = E : unit, \\Gamma' \\oplus(X,T)            \\end{array}  \\end{array} \\] <p>In rule \\((\\tt ltAssign)\\) we type check the assignment statement, we first type check the RHS expression, and we \"transfer\" the ownership of type \\(T\\) to \\(X\\) in the resulting environment. For example, in an assignment statement, \\(X = Y\\), \\(Y\\)'s type is transferred to \\(X\\), and \\(Y\\)'s type assignment is no longer accessible after this statement. Hence the following programming is not typeable in the linear type system. </p> <pre><code>// SIMP6\nx = 1;\ny = x;\nz = x;\n</code></pre> \\[ \\begin{array}{rc} {\\tt (ltNop)} &amp;            \\Gamma \\vdash nop : unit, \\Gamma  \\end{array} \\] <p>Typing \\(Nop\\) does not change the type environment.  </p> \\[ \\begin{array}{rc} {\\tt (ltArrDeref)} &amp; \\begin{array}{c}            \\Gamma \\vdash E_2:T, \\Gamma_1 \\ \\ \\ \\Gamma_1 \\vdash E_1:int, \\Gamma_2 \\\\ (X,[T]) \\in \\Gamma_2            \\\\ \\hline           \\Gamma \\vdash X[E_1] = E_2 : unit, \\Gamma_2           \\end{array}  \\end{array} \\] <p>In case of array dereference statement, we type check the RHS expression. With the updated type environemnt \\(Gamma_1\\) we type check the index expression \\(E_1\\) having type \\(int\\). Finally we make sure that \\(X\\) is an array type in \\(\\Gamma_2\\) without removing it.</p> \\[ \\begin{array}{rc} {\\tt (ltFree)} &amp; \\begin{array}{c}            \\Gamma \\vdash X:[T], \\Gamma'            \\\\ \\hline           \\Gamma \\vdash free\\ X: unit, \\Gamma'           \\end{array}  \\end{array} \\] <p>When type checking the free statement, \\(X\\)'s type must be an array type. (Note that \\((X,[T]))\\) will be removed.)</p> \\[ \\begin{array}{rc} {\\tt (ltReturn)} &amp; \\begin{array}{c}            \\Gamma \\vdash X:T, \\Gamma'            \\\\ \\hline           \\Gamma \\vdash return\\ X: T, \\Gamma'           \\end{array}  \\end{array} \\] <p>Return statement carries the type of the variable being returned. After that, the type assignment of \\(X\\) will be removed. </p> \\[ \\begin{array}{rc} {\\tt (ltIf)} &amp; \\begin{array}{c}            \\Gamma \\vdash E_1:bool, \\Gamma_1 \\\\            \\Gamma_1 \\vdash \\overline{S_1} : T, \\Gamma_2 \\\\             \\Gamma_1 \\vdash \\overline{S_2} : T, \\Gamma_2            \\\\ \\hline           \\Gamma \\vdash if\\ E_1\\ \\{\\overline{S_1}\\}\\ else\\ \\{\\overline{S_2}\\} : T, \\Gamma_2           \\end{array}  \\end{array} \\] <p>In the rule \\((\\tt ltIf)\\), we first type check the condition expression \\(E_1\\) against \\(bool\\). We then type check the then and else statements under the same type \\(T\\) and resulting type environment \\(\\Gamma_2\\). </p> \\[ \\begin{array}{rc} {\\tt (ltWhile)} &amp; \\begin{array}{c}            \\Gamma \\vdash E:bool, \\Gamma_1 \\\\            \\Gamma_1 \\vdash \\overline{S} : T, \\Gamma_1             \\\\ \\hline           \\Gamma \\vdash while\\ E\\ \\{\\overline{S}\\} : T, \\Gamma_1           \\end{array}  \\end{array} \\] <p>The typing rule \\((\\tt ltWhile)\\) is similar to \\((\\tt ltIf)\\), except that the type environments \"before\" and \"after\" the while body should be unchanged to ensure linearity.</p> \\[ \\begin{array}{rc} {\\tt (ltSeq)} &amp; \\begin{array}{c}            \\Gamma \\vdash S:T, \\Gamma_1 \\\\            \\Gamma_1 \\vdash \\overline{S} : T', \\Gamma_2             \\\\ \\hline           \\Gamma \\vdash S;\\overline{S}:T', \\Gamma_2           \\end{array}  \\end{array} \\] <p>In the rule \\((\\tt ltSeq)\\), we type check a sequence of statements by propogating the updated type environments from top to bottom (left to right).</p>"},{"location":"memory_management/#type-checking-simp-declaration-using-linear-type-system","title":"Type Checking SIMP Declaration using Linear Type System","text":"\\[ \\begin{array}{rc} {\\tt (ltFuncDecl)} &amp; \\begin{array}{c}            \\Gamma \\oplus(f:T_1 \\rightarrow T_2)\\oplus(X:T_1) \\vdash \\overline{S}:T_2, \\Gamma\\oplus(f:T_1 \\rightarrow T_2)           \\\\ \\hline           \\Gamma \\vdash func\\ f(X:T_1)T_2 \\{ \\overline{S} \\}: T_1 \\rightarrow T_2, \\Gamma           \\end{array} \\\\ \\\\  {\\tt (ltProg)} &amp; \\begin{array}{c}            {\\tt for\\ } i \\in [1,n] \\ \\ \\           \\{\\} \\vdash D_i : T_i, \\Gamma_i \\ \\ \\           \\Gamma \\vdash \\overline{S}:T,\\Gamma'           \\\\ \\hline           \\Gamma \\vdash D_1;...;D_n;\\overline{S}: T,\\Gamma'           \\end{array}  \\end{array}  \\] <p>When type checking a function declaration, we extend the type environemnt with the function's type assignment and its argument type assignment, then type check the body. The additional requirement is that the resulting environment must be exactly the same as \\(\\Gamma\\oplus(f:T_1 \\rightarrow T_2)\\) to maintain linearity. </p> <p>In \\((\\tt ltPRog)\\), we type check the function declaration idependently with an empty type environment then type check the main statement sequence left to right. </p>"},{"location":"memory_management/#rejecting-simp3-via-linear-type-system","title":"Rejecting <code>SIMP3</code> via linear type system","text":"<p>Let's apply the linear type checking rules type check the function <code>f</code> from  our earlier example <code>SIMP3</code>.</p> <pre><code>{(f,int-&gt;int),(x,int)} |- x : int (ltVar), {(f,int-&gt;int)} (ltVar)\n--------------------------------------------------------(ltAssign)\n{(f,int-&gt;int),(x,int)} |- t = int[x] :unit, {(f,int-&gt;int),(t,[int])} [sub tree 5]\n-----------------------------------------------------------------(ltSeq)\n{(f,int-&gt;int),(x,int)} |- t = int[x]; free t; free t; return x; : ???, ???\n----------------------------------------------------- (ltFuncDecl)\n{} |- func f(x:int) int {\n    t = int[x];\n    free t;\n    free t; \n    return x; \n} : ???, ???\n</code></pre> <p>[sub tree 5] is as follows</p> <pre><code>{(f,int-&gt;int),(t,[int])} |- t : [int], {(f,int-&gt;int)} (ltVar) \n----------------------------------------------------------- (ltFree)\n{(f,int-&gt;int),(t,[int])} |- free t : unit, {(f,int-&gt;int)}   [sub tree 6]\n---------------------------------------------------------- (ltSeq)\n{(f,int-&gt;int),(t,[int])} |- free t; free t; return x; : ???, ???\n</code></pre> <p>[sub tree 6] is as follows</p> <pre><code>we get stuck here. t's type has been \"consumed\" \n---------------------------- (ltFree)\n{(f,int-&gt;int)} |- free t : ???, ???\n-------------------------------------------- (ltSeq)\n{(f,int-&gt;int)} |- free t; return x; : ???, ???\n</code></pre> <p>Since the type checking fails, <code>SIMP3</code> will be rejected by the linear type system. </p>"},{"location":"memory_management/#rejecting-simp4-via-linear-type-system","title":"Rejecting <code>SIMP4</code> via linear type system","text":"<p>Let's try to type check <code>SIMP4</code>.</p> <pre><code>{(f,int-&gt;int),(x,int)} |- x : int (ltVar), {(f,int-&gt;int)} (ltVar)\n--------------------------------------------------------(ltAssign)\n{(f,int-&gt;int),(x,int)} |- t = int[x] :unit, {(f,int-&gt;int),(t,[int])} [sub tree 7]\n-----------------------------------------------------------------(ltSeq)\n{(f,int-&gt;int),(x,int)} |- t = int[x]; return 1;:int, {(f,int-&gt;int),(t,[int])} we get stuck  \n----------------------------------------------------- (ltFuncDecl)\n{} |- func f(x:int) int {\n    t = int[x];\n    return 1;\n} : int, ??? \n</code></pre> <p>where [sub tree 7] is as follows</p> <pre><code>{(f,int-&gt;int),(t,[int])} |- 1 : int, {(f,int-&gt;int),(t,[int])}\n----------------------------------------------------- (ltReturn)\n{(f,int-&gt;int),(t,[int])} |- return 1; : int, {(f,int-&gt;int),(t,[int])}\n</code></pre> <p>The above program fails to type check as the result type environment of the \\((\\tt ltFuncDelc)\\) rule is not matching with the input type environment. </p>"},{"location":"memory_management/#make-linear-type-system-practical","title":"Make linear type system practical","text":"<p>The linear type checking is a proof-of-concept that we could use it to detect run-time errors related to memory management. </p> <p>The current system is still naive. </p> <ol> <li>We probably need to apply the linearity restriction to heap object such as array but not to primitive values such as <code>int</code> and <code>bool</code>. For example, the following program will not type check</li> </ol> <pre><code>func square (x:int):int {\n    y = x * x;\n    return y;\n}\n</code></pre> <ol> <li> <p>Typing rules will become complex if we consider nested arrays. </p> </li> <li> <p>We need a type inference algorithm, which should reject <code>SIMP3</code>. But for <code>SIMP4</code>, its type constraints should identify the missing <code>free</code> statement and let the compiler insert the statement on behalf of the programmers. </p> </li> </ol> <p>We leave these as future work. </p>"},{"location":"name_analysis/","title":"50.054 - Name Analysis","text":""},{"location":"name_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Articulate the purpose of name analysis.</li> <li>Describe the properties of the static single assignment forms.</li> <li>Implement the static single assignment construction and deconstruction algorithms.</li> </ol>"},{"location":"name_analysis/#what-is-name-analysis","title":"What is Name Analysis","text":"<p>Given a source program (or AST), the compiler needs to check for each identifier defined (i.e. name).</p> <ol> <li>Is it a variable name or a function name? This is not an issue for SIMP language as we don't deal with function at the momement.</li> <li>Is the variable name of type int or bool? This has been addressed via the type inference and type checking in the previous unit.</li> <li>What is the scope of the variable?</li> <li>Has the variable been declared before used?</li> <li>Where is the defined variable used?</li> </ol>"},{"location":"name_analysis/#variable-scope","title":"Variable Scope","text":"<p>Consider the following Python program, </p> <p><pre><code>x = -1\n\ndef f():\n    x = 1\n    return g()\n\ndef g():\n    print(x)\n\nf()\n</code></pre> When the program is executed, we observe <code>-1</code> being printed. The variable <code>x=1</code> in <code>f()</code> does not modify the <code>x=-1</code> in the outer scope. Hence when <code>g()</code> is called, the variable <code>x</code> being printed is from the global scope <code>x=-1</code>. This is known as static scoping. </p>"},{"location":"name_analysis/#static-variable-scoping","title":"Static Variable Scoping","text":"<p>For a programming language with static variable scoping, the relation between a variable's definition and its reference is defined by its syntactic structure, (also known as lexical structure). For instance the earlier example shows that Python is using static variable scoping, because the Python program has the following syntactic structure (e.g. Syntax Tree).</p> <pre><code>graph TD;\n    Main --&gt; x1[\"x=-1\"];\n    Main --&gt; f;\n    f --&gt; x2[x=1];\n    Main --&gt; g;\n    g --&gt; usex1[\"print(x)\"];</code></pre> <p>Thus the <code>print(x)</code> of <code>g</code> uses the <code>x</code> defined in its parent node.</p>"},{"location":"name_analysis/#dynamic-variable-scoping","title":"Dynamic Variable Scoping","text":"<p>For a programming language with dynamic scoping, the relation between a variable's definition and its reference is defined by the dynamic call stack. </p> <p><pre><code>$x = -1;\n\nsub f {\n    local $x = 1;\n    return g();\n}\n\nsub g {\n    print $x;\n}\nf()\n</code></pre> In the above, it is the same program coded in <code>perl</code>. Except that in perl, variables with <code>local</code> are defined using dynamic scoping. As a result, <code>1</code> is printed when the program is executed. When a program with dynamic variable scoping is executed, its variable reference follows the </p> <pre><code>graph TD;\n    Main --&gt; x1[\"x=-1\"];\n    Main --&gt; f;\n    f --&gt; x2[x=1];\n    f --&gt; g;\n    g --&gt; usex1[\"print(x)\"];    </code></pre> <p>As illustrated by the dynamic call graph above, the variable <code>x</code> in <code>print(x)</code> refers to <code>g</code>'s caller, i.e. <code>f</code>, which is <code>1</code>.</p>"},{"location":"name_analysis/#more-on-static-variable-scoping","title":"More On Static Variable Scoping","text":"<p>Static Variable Scoping is dominating the program language market now. Most of the main stream languages uses static variable scoping thanks to its ease of reasoning, e.g. C, C++, Python, Java and etc. Among these languages, there are also some minor variant of static variable scoping implementation.</p> <p>Consider the following Python program.</p> <p><pre><code>def main(argv):\n    x = 1\n    if len(argv) == 0:\n        x = 2\n    else:\n        y = 1\n    print(y)\n</code></pre> when the input <code>argv</code> is a non-empty list, the function <code>main</code> prints <code>1</code> as results. However when <code>argv</code> is an empty list, a run-time error arises.</p> <p>Consider the \"nearly-the-same\" program in Java.</p> <pre><code>class Main {\n    public static int main(String[] argv) {\n        int x = 1;\n        if (argv.length &gt; 0){\n            x = 2;\n        } else {\n            int y = 1;\n        }\n        System.out.println(y.toString());\n        return 1;\n    }\n}\n</code></pre> <p>Java returns a compilation error, complaining variable <code>y</code> being use in the <code>System.out.println</code> function can't be resolved.</p> <p>The difference here is that in Python, all variables declared in a function share the same scope. While in Java, variable's scope is further divided based on the control flow statement such as if-else, while and etc. In the above example, the variable <code>y</code>'s scope is only within the else branch but not outside.</p> <p>In SIMP, we assume the same variable scoping implementation as Python, i.e. all variables declared in a function shared the same scope, and since the SIMP language we have so far does not support function call, we further simplify the problem that all variables are sharing same scope. </p> <p>However how might we detect the run-time error similar to what we've observed from the last Python example? </p> <p>Let's recast the example in SIMP, let's call it <code>SIMP_ERR1</code> <pre><code>// SIMP_ERR1\nx = 1;\nif input == 0 {\n    x = 2;\n} else {\n    y = 1;\n}\nreturn y;\n</code></pre></p> <p>The above program will cause an error when <code>input == 0</code>. It is typeable based on the type inference algorithm we studied in the previous class.  Let's consider its pseudo assembly version. The Maximal Munch algorithm v2 produces the following given the SIMP program.</p> <p><pre><code>// PA_ERR1\n1:  x &lt;- 1\n2:  t &lt;- input == 0\n3:  ifn t goto 6\n4:  x &lt;- 2\n5:  goto 7\n6:  y &lt;- 1\n7:  rret &lt;- y\n8:  ret\n</code></pre> Same error arises when <code>input == 0</code>. </p>"},{"location":"name_analysis/#static-single-assignment-form","title":"Static Single Assignment form","text":"<p>Static Single Assignment (SSA) form is an intermediate representation  widely used in compiler design and program verification. </p> <p>In a static single assignment form, </p> <ul> <li>Each variable is only allowed to be assigned once syntactically, i.e. it only appears in the LHS of the assignment once. </li> <li>\\(\\phi\\)-assignments are placed at the end of branching statements to merge different (re)-definition of the same variable (from the source program). </li> </ul> <p>SSA form construction is one of the effective ways to analysis</p> <ol> <li>the scope of variables</li> <li>the use-def relationship of variables</li> </ol>"},{"location":"name_analysis/#unstructured-ssa-form","title":"Unstructured SSA Form","text":"<p>Suppose we extend the pseudo assembly with \\(\\phi\\)-assignment statements, </p> \\[ \\begin{array}{rccl} (\\tt Labeled\\ Instruction) &amp; li  &amp; ::= &amp; l : \\overline{\\phi}\\ i \\\\  (\\tt Instruction)   &amp; i   &amp; ::= &amp; d \\leftarrow s \\mid d \\leftarrow s\\ op\\ s \\mid ret \\mid ifn\\ s\\ goto\\ l \\mid goto\\ l \\\\  (\\tt PhiAssignment) &amp; \\phi &amp; ::= &amp; d \\leftarrow phi(\\overline{l:s}) \\\\  (\\tt Labeled\\ Instructions)   &amp; lis   &amp; ::= &amp; li \\mid li\\ lis \\\\  (\\tt Operand)       &amp; d,s &amp; ::= &amp; r \\mid c \\mid t \\\\ (\\tt Temp\\ Var)      &amp; t   &amp; ::= &amp; x \\mid y \\mid ...  \\\\ (\\tt Label)         &amp; l   &amp; ::= &amp; 1 \\mid 2 \\mid ... \\\\ (\\tt Operator)      &amp; op  &amp; ::= &amp; + \\mid - \\mid &lt; \\mid == \\mid ... \\\\  (\\tt Constant)      &amp; c   &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\\\  (\\tt Register)      &amp; r &amp;   ::= &amp; r_{ret} \\mid r_1 \\mid r_2 \\mid ...   \\end{array} \\] <p>The syntax is largely unchanged, except that for each labeled instruction, there exists a sequence of phi assignments \\(\\overline{\\phi}\\). (which could be empty) before the actual instruction \\(i\\). When \\(\\overline{\\phi}\\) is empty, we omit it from the syntax.</p> <p>we are able convert any \"well-defined\" pseudo assembly program into an SSA form. Since we build the SSA forms from some unstructured language program (i.e. no nested control flow statements), we call them unstructured SSA forms.</p> <p>Suppose we have the following pseudo assembly program </p> <pre><code>// PA1\n1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: t &lt;- c &lt; x \n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s\n10: ret\n</code></pre> <p>Note that variables <code>s</code> and <code>c</code> are re-assigned in the loop. </p> <p>The SSA form of the above is </p> <pre><code>// SSA_PA1\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n   t0 &lt;- c1 &lt; x0\n5: ifn t0 goto 9\n6: s2 &lt;- c1 + s1\n7: c2 &lt;- c1 + 1\n8: goto 4\n9: rret &lt;- s1 \n10: ret\n</code></pre> <p>In the above example, we inserted a set of phi assigments to label 4. Every variable/register is strictly assigned once. We need to introduce a new \"variant\" of the same source variable whenever re-assignment is needed. More specifically, in instruction with label 4, we use two phi assignments to merge the multiple definitions of the same source variable.  </p> <p>There are two possible preceding instructions that lead us to the following instruction <pre><code>4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n</code></pre> namely, 3 and 9. When the preceding instruction is 3, the above phi assignments will assign <code>s0</code> to <code>s1</code> and <code>c0</code> to <code>c1</code>. Otherwise, <code>s2</code> is assigned to <code>s1</code> and <code>c2</code> is assigned to <code>c1</code>.</p> <p>To cater for the phi assignment, we extend the small step operational semantics from</p> \\[ P \\vdash (L, li) \\longrightarrow (L', li') \\] <p>to </p> \\[ P \\vdash (L, li, p) \\longrightarrow (L', li', p') \\] <p>The third component \\(p\\) in the program context is a label from the preceding instruction based on the execution. </p> \\[ {\\tt (pConst)} ~~~ P \\vdash (L, l:  d \\leftarrow c, p) \\longrightarrow (L \\oplus (d,c), P(l+1), l) \\] \\[ {\\tt (pRegister)} ~~~P \\vdash (L, l: d \\leftarrow r, p) \\longrightarrow (L \\oplus (d,L(r)), P(l+1), l) \\] \\[ {\\tt (pTempVar)} ~~~P \\vdash (L, l: d \\leftarrow t, p ) \\longrightarrow (L \\oplus (d,L(t)), P(l+1), l) \\] \\[ {\\tt (pGoto)} ~~ P \\vdash (L, l:goto\\ l', p) \\longrightarrow (L, P(l'), l) \\] \\[ \\begin{array}{rc} {\\tt (pOp)} &amp;  \\begin{array}{c}         c_1 = L(s_1) ~~~ c_2 = L(s_2) ~~~ c_3 = c_1\\ op\\ c_2         \\\\ \\hline         P \\vdash (L, l: d \\leftarrow s_1\\ op\\ s_2, p) \\longrightarrow (L \\oplus (d,c_3), P(l+1), l)           \\end{array} \\\\ {\\tt (pIfn0)} &amp; \\begin{array}{c}      L(s) = 0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l'), l)      \\end{array} \\\\ {\\tt (pIfnNot0)} &amp; \\begin{array}{c}      L(s) \\neq  0      \\\\ \\hline      P \\vdash (L, l: ifn\\ s\\ goto\\ l', p) \\longrightarrow (L, P(l+1), l)      \\end{array} \\end{array} \\] <p>All the existing rules are required some minor changes to accomodate the third component in the program context.  The adjustments are common, i.e. propogating the label of the current labeled instruction from the LHS to the RHS as the proceding label. Note that the above handle the cases in which the labeled instruction has no phi assignments.  In the presence of phi-assignments,  we need the following rules to guide the execution.</p> \\[ \\begin{array}{rc} {\\tt (pPhi1)} &amp;  \\begin{array}{c}         (L, l: []\\ i, p) \\longrightarrow (L, l: i, p)         \\end{array} \\\\ \\\\ {\\tt (pPhi2)} &amp;  \\begin{array}{c}         l_i = p\\ \\ \\ c_i = L(s_i) \\\\  j \\in [1,i-1]: l_j \\neq p          \\\\ \\hline         (L, l: d \\leftarrow phi(l_1:s_1,..,l_n:s_n); \\overline{\\phi}\\ i , p) \\longrightarrow (L\\oplus(d,c_i), l: \\overline{\\phi}\\ i, p)           \\end{array} \\end{array} \\] <p>The execution of the labeled instruction with phi assignments is defined by the \\((\\tt pPhi1)\\) and \\((\\tt pPhi2)\\) rules. </p> <ul> <li>The \\((\\tt pPhi1)\\) rule handles the base case where \\(\\overline{\\phi}\\) is an empty sequence, it proceeds to execute the following instruction \\(i\\) by using one of the earlier rules. </li> <li>The \\((\\tt pPhi2)\\) rule is applied when the sequence of phi-assignments is not empty. <ol> <li>We process the first one phi-assignment. By scanning the set of labels in the \\(phi()\\)'s operands from left to right, we identify the first matching label \\(l_i\\) and lookup the value of the associated variable/register \\(s_i\\), i.e. \\(c_i\\).</li> <li>Add the new entry \\((d,c_i)\\) to the local environment \\(L\\). </li> <li>Proceed by recursively processing the rest of the phi assignments with the updated \\(L \\oplus (d,c_i)\\).</li> </ol> </li> </ul> <p>Given \\(input = 1\\), excuting <code>SSA_PA1</code> yields the following derivation</p> <pre><code>P |- {(input,1)}, 1: x0 &lt;- input, undef ---&gt; # (pTempVar)\nP |- {(input,1), (x0,1)}, 2: s0 &lt;- 0, 1 ---&gt; # (pConst)\nP |- {(input,1), (x0,1), (s0,0)}, 3: c0 &lt;- 0, 2 ---&gt; # (pConst)\nP |- {(input,1), (x0,1), (s0,0), (c0,0)}, 4: s1 &lt;- phi(3:s0, 9:s2); c1 &lt;- phi(3:c0, 9:c2) t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0)}, 4: c1 &lt;- phi(3:c0, 9:c2) t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: [] t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pPhi1)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0) }, 4: t0 &lt;- c1 &lt; x0, 3 ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 5: ifn t0 goto 9, 4 ---&gt; # (pIfnNot0)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1)}, 6: s2 &lt;- c1 + s1, 5  ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0)}, 7: c2 &lt;- c1 + 1, 6  ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 8: goto 4, 7  ---&gt; # (pGoto)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: s1 &lt;- phi(3:s0, 8:s2); c1 &lt;- phi(3:c0, 8:c2) t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,0), (t0, 1), (s2, 0), (c2, 1)}, 4: c1 &lt;- phi(3:c0, 8:c2) t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pPhi2)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: [] t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pPhi1)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 1), (s2, 0), (c2, 1)}, 4: t0 &lt;- c1 &lt; x0, 8 ---&gt; # (pOp)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 5: ifn t0 goto 9, 4 ---&gt; # (pIfn0)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1)}, 9: rret &lt;- s1, 5 ---&gt; # (pTempVar)\nP |- {(input,1), (x0,1), (s0,0), (c0,0), (s1,0), (c1,1), (t0, 0), (s2, 0), (c2, 1), (rret, 0)}, 10: ret, 9 \n</code></pre>"},{"location":"name_analysis/#minimality","title":"Minimality","text":"<p>One may argue that instead of generating <code>SSA_PA1</code>, one might generate the following static single assignment</p> <pre><code>// SSA_PA2\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n   t0 &lt;- c1 &lt; x0\n5: ifn t0 goto 9\n6: s2 &lt;- c1 + s1\n7: c2 &lt;- c1 + 1\n8: goto 4\n9: s3 &lt;- phi(5:s1)\n   rret &lt;- s3\n10: ret\n</code></pre> <p>which will yield the same output. However we argue that <code>SSA_PA1</code> is preferred as it has the minimal number of phi assignments.</p>"},{"location":"name_analysis/#ssa-construction-algorithm","title":"SSA Construction Algorithm","text":"<p>The defacto SSA construction algorithm that produces minimal SSA forms was developed by Cytron et al.</p> <p>https://doi.org/10.1145/115372.115320</p> <p>The main idea is to take the original program and identify the \"right\" locations to insert phi assignments so that the result is a minimal SSA form.</p>"},{"location":"name_analysis/#control-flow-graph","title":"Control flow graph","text":"<p>We can model a Pseudo Assembly program using a graph, namely the contorl flow graph.</p> <p>For example, <code>PA1</code> can be represented as the following Control flow graph <code>Graph1_PA1</code></p> <pre><code>graph TD;\n    B1(\"1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\")--&gt;B2;\n    B2--&gt;B3;\n    B2(\"4: t &lt;- c &lt; x \n5: ifn t goto 9\")--&gt;B4(\"9: rret &lt;- s\n10: ret\");\n    B3(\"6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\")--&gt;B2;</code></pre> <p>For the ease of reasoning (though unnecessary), without losing the graph properties, we would consider an isomoprhic version of the above graph where the vertices with multiple instructions are further divided until each vertex contains only one instruction, let's call it <code>Graph2_PA1</code></p> <pre><code>graph TD;\n    V1(\"1: x &lt;- input\") --&gt; V2;\n    V2(\"2: s &lt;- 0\") --&gt; V3;\n    V3(\"3: c &lt;- 0\") --&gt; V4;\n    V4(\"4: t &lt;- c &lt; x\") --&gt; V5;\n    V5(\"5: ifn t goto 9\") --&gt; V9;\n    V9(\"9: rret &lt;- s\") --&gt; V10(\"10: ret\")\n    V5(\"5: ifn t goto 9\") --&gt; V6;\n    V6(\"6: s &lt;- c + s\") --&gt; V7;\n    V7(\"7: c &lt;- c + 1\") --&gt; V8;\n    V8(\"8: goto 4\") --&gt; V4;</code></pre> <p>Now we refer to the vertex in a control flow graph by the label.</p> <p>The technical trick is to apply some graph operation to identify the \"right\" locations for phi assignments from the CFG.</p>"},{"location":"name_analysis/#identifying-the-right-locations","title":"Identifying the \"right\" locations","text":""},{"location":"name_analysis/#definition-1-graph","title":"Definition 1 - Graph","text":"<p>Let \\(G\\) be a graph, \\(G = (V, E)\\), where \\(V\\) denotes the set of vertices and \\(E\\) denote a set of edges. Let \\(v_1 \\in V\\) and \\(v_2 \\in V\\), \\((v_1,v_2) \\in E\\) implies that exists an edge going from \\(v_1\\) to \\(v_2\\). </p> <p>Occassionally, we also refer to a vertex as a node in the graph.  For convenience, we also write </p> <ul> <li>\\(v \\in G\\) as the short-hand for \\(v \\in V \\wedge G = (V,E)\\) and </li> <li>\\((v_1, v_2) \\in G\\) as the short-hand for \\((v_1, v_2) \\in E \\wedge G = (V,E)\\).</li> </ul>"},{"location":"name_analysis/#definition-2-path","title":"Definition 2 - Path","text":"<p>Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\). We say a path from \\(v_1\\) to \\(v_2\\), written as \\(path(v_1,v_2)\\), exists iff </p> <ol> <li>\\(v_1 = v_2\\) or</li> <li>the set of edges \\(\\{(v_1, u_1), (u_1,u_2), ..., (u_n,v_2)\\} \\subseteq E\\) where \\(E\\) is the set of edges in \\(G\\).</li> </ol> <p>For convenience, some times we write \\(v_1,u_1,...,u_n,v_2\\) to denote a particular path from \\(v_1\\) to \\(v_2\\).</p>"},{"location":"name_analysis/#definition-3-connectedness","title":"Definition 3 - Connectedness","text":"<p>Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\). We say \\(v_1\\) and \\(v_2\\) are connected, written \\(connect(v_1,v_2)\\), iff </p> <ol> <li>\\(path(v_1, v_2)\\) or \\(path(v_2, v_1)\\) exists, or </li> <li>there exists \\(v_3\\) in \\(G\\) such that \\(connect(v_1,v_3)\\) and \\(connect(v_3, v_2)\\).</li> </ol>"},{"location":"name_analysis/#definition-4-source-and-sink","title":"Definition 4 - Source and Sink","text":"<p>Let \\(v\\) be a vertex in a graph \\(G\\), we say \\(v\\) is a source vertex if there exists no entry \\((v',v) \\in E\\) where \\(E\\) is the set of edges in \\(G\\).</p> <p>Let \\(v\\) be a vertex in a graph \\(G\\), we say \\(v\\) is a sink vertex if there exists no entry \\((v, v') \\in E\\) where \\(E\\) is the set of edges in \\(G\\).</p>"},{"location":"name_analysis/#assumption","title":"Assumption","text":"<p>Since we are dealing with SIMP program's CFGs, we assume that the set of graphs we are considering are </p> <ol> <li>Connected, i.e. for any \\(v_1, v_2\\) in \\(G\\), we have \\(connect(v_1,v_2)\\) </li> <li>Has only one source vertex, which means there is only one entry point to the program.</li> <li>Has only one sink vertex, which means there is only one return statement.</li> </ol>"},{"location":"name_analysis/#definition-5-dominance-relation","title":"Definition 5 - Dominance Relation","text":"<p>Let \\(v_1\\) and \\(v_2\\) be two vertices in a graph \\(G\\). We say \\(v_1\\) dominates \\(v_2\\), written as \\(v_1 \\preceq v_2\\), iff for all path \\(v_0,...,v_2\\) where \\(v_0\\) is the source vertex, we find a prefix sequence \\(v_0,...,v_1\\) in \\(v_0,...,v_2\\). </p> <p>In other words, \\(v_1 \\preceq v_2\\) means whenever we execute the program from the start to location \\(v_2\\), we definitely pass through location \\(v_1\\). </p> <p>For instance, in the earlier control flow graph for <code>Graph2_PA1</code>, </p> <ul> <li>the vertex <code>1</code> dominates all vertices. </li> <li>the vertex <code>4</code> dominates itself, the vertices <code>5,6,7,8,9,10</code>.</li> </ul>"},{"location":"name_analysis/#lemma-1-dominance-is-transitive","title":"Lemma 1 - Dominance is transitive","text":"<p>\\(v_1 \\preceq v_2\\) and \\(v_2 \\preceq v_3\\) implies that \\(v_1 \\preceq v_3\\).</p>"},{"location":"name_analysis/#lemma-2-dominance-is-reflexive","title":"Lemma 2 - Dominance is reflexive","text":"<p>For any vertex \\(v\\), we have \\(v \\preceq v\\).</p>"},{"location":"name_analysis/#definition-6-strict-dominance","title":"Definition 6 - Strict Dominance","text":"<p>We say \\(v_1\\) stricly domainates \\(v_2\\), written \\(v_1 \\prec v_2\\) iff \\(v_1 \\preceq v_2\\) and \\(v_1 \\neq v_2\\).</p>"},{"location":"name_analysis/#definition-7-immediate-dominator","title":"Definition 7 - Immediate Dominator","text":"<p>We say \\(v_1\\) is the immediate dominator of \\(v_2\\), written \\(v_1 = idom(v_2)\\) iff \\(v_1 \\prec v_2\\) and not exists \\(v_3\\) such that \\(v_1 \\prec v_3\\) and \\(v_3 \\prec v_2\\).</p> <p>Note that \\(idom()\\) is a function, i.e. the immediate dominator of a vertex must be unique if it exists.</p>"},{"location":"name_analysis/#dominator-tree","title":"Dominator Tree","text":"<p>Given the \\(idom()\\) function, we can construct a dominator tree from a control flow graph \\(G\\).</p> <ul> <li>Each vertex \\(v \\in G\\) forms a node in the dominator tree.</li> <li>For vertices \\(v_1, v_2 \\in G\\), \\(v_2\\) is a child of \\(v_1\\) if \\(v_1 = idom(v_2)\\).</li> </ul> <p>For example, from the CFG <code>Graph2_PA1</code>, we construct a dominator tree <code>Tree2_PA1</code>, as follows, </p> <pre><code>graph TD;\n    1 --&gt; 2;\n    2 --&gt; 3;\n    3 --&gt; 4;\n    4 --&gt; 5;\n    5 --&gt; 6;\n    6 --&gt; 7;\n    7 --&gt; 8;\n    5 --&gt; 9;\n    9 --&gt; 10;</code></pre> <p>Let \\(T\\) be a dominator tree, we write \\(child(v,T)\\) to denote the set of children of \\(v\\) in \\(T\\).</p>"},{"location":"name_analysis/#definition-8-dominance-frontier","title":"Definition 8 - Dominance Frontier","text":"<p>Let \\(v\\) be vertex in a graph \\(G\\), we define the dominance frontier of \\(v\\) as $$ df(v, G) = { v_2 \\mid (v_1,v_2) \\in G \\wedge v \\preceq v_1 \\wedge \\neg(v \\prec v_2) } $$</p> <p>In other words, the dominance frontier of a vertex \\(v\\) is the set of vertices that are not dominated by \\(v\\) but their predecesors are (dominated by \\(v\\)). </p> <p>For instance, in our running example, the dominance frontier of  vertex <code>6</code> is the set containing vertex <code>4</code> This is because </p> <ul> <li>vertex <code>8</code> is one of the predecesors of the vertex <code>4</code> and </li> <li>vertex <code>8</code> is dominated by vertex <code>6</code>, but not the vertex <code>4</code> is not domainated by vertex <code>6</code>.</li> </ul> <p>Question: what is the dominance frontier of vertex <code>5</code>?</p>"},{"location":"name_analysis/#computing-dominance-frontier","title":"Computing Dominance Frontier","text":"<p>The naive algorithm of computing dominance frontier of all ther vertices in a CFG takes \\(O(n^2)\\) where \\(n\\) is the number of vertices. </p> <p>Cytron et al proposed a more efficient algorithm to compute the dominance frontiers of all the vertices in a CFG. </p>"},{"location":"name_analysis/#re-definining-dominance-frontier","title":"Re-definining Dominance Frontier","text":"<p>The main idea is to give a recursive definition to Dominance Frontier by making use of the dominator tree. </p> <p>Let \\(G\\) be a CFG, and \\(T\\) be the dominator tree of \\(G\\). We define </p> \\[ df(v, G) = df_{local}(v, G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u, G)  ~~~(E1) \\] <p>where </p> \\[ df_{local}(v, G) = \\{ w \\mid (v,w) \\in G \\wedge \\neg(v \\prec w)\\} ~~~(E2) \\] <p>and </p> \\[ df_{up}(v, G) = \\{ w \\mid w \\in df(v,G) \\wedge \\neg (idom(v) \\prec w)\\}~~~(E3) \\] <ul> <li>\\((E1)\\) says that the dominance frontier of a vertex \\(v\\) is the union of the local contribution \\(df_{local}(v,G)\\) and the (dominator tree) descendants' upward contribution \\(\\bigcup_{u \\in child(v,T)} df_{up}(u, G)\\)</li> <li>\\((E2)\\) defines the local dominance frontier of a vertex \\(v\\) by finding successors \\(w\\) of \\(v\\) (i.e. there is an edge from \\(v\\) to \\(w\\)) that are not dominated by \\(v\\).</li> <li>\\((E3)\\) defines the upward contributed frontier of a vertex \\(v\\), by finding vertices \\(w\\) in \\(v\\)'s dominance frontier, such that \\(w\\) is not dominated by \\(v\\)'s immediate dominator (i.e. \\(v\\)'s parent in the dominator tree).</li> </ul> <p>Cytron et al shows that \\((E1)\\)  defines the same result as Definition 6.</p>"},{"location":"name_analysis/#dominance-frontier-algorithm","title":"Dominance frontier algorithm","text":"<p>As we can observe from the recursive definition, it is more efficient to compute the dominance frontiers by traversing the dominator tree \"bottom-up\", as we can reuse the dominance frontier of the child nodes (vertices) to compute the upward contribution of the parent node (vertex).</p> <p>The algorithm is structured as follows</p> <ol> <li>For each vertex \\(v\\) by traversing the dominator tree bottom up:<ol> <li>compute \\(df_{local}(v,G)\\)</li> <li>compute \\(\\bigcup_{u \\in child(v,T)}df_{up}(u, G)\\), which can be looked up from the a memoization table.</li> <li>save \\(df(v,G) = df_{local}(v,G) \\cup \\bigcup_{u \\in child(v,T)} df_{up}(u,G)\\) in the memoization table.</li> </ol> </li> </ol> <p>For instance, we make use of <code>Graph2_PA1</code> and <code>Tree2_PA1</code> to construct the following memoization table <code>Table2_PA1</code></p> vertex/node successors children idom \\(df_{local}\\) \\(df_{up}\\) df 10 {} {} 9 {} {} {} 9 {10} {10} 5 {} {} {} 8 {4} {} 7 {4} {4} {4} 7 {8} {8} 6 {} {4} {4} 6 {7} {7} 5 {} {4} {4} 5 {6,9} {6,9} 4 {} {4} {4} 4 {5} {5} 3 {} {} {4} 3 {4} {4} 2 {} {} {} 2 {3} {3} 1 {} {} {} 1 {2} {2} {} {} {} <p>From the above table, we conclude that variables that are updated in vertices <code>5,6,7,8</code> should be merged via phi-assignments at the entry point of vertex <code>4</code>.</p> <p>As highlighted in Cytron's paper, \\(df_{local}(x,G)\\) can be defined efficiently as \\(\\{ y \\mid  (x,y)\\in G \\wedge idom(y) \\neq x \\}\\)</p> <p>Furthermore, \\(df_{up}(u,x,G)\\) can be defined efficiently as \\(\\{y \\mid y \\in df(u) \\wedge idom(y) \\neq x \\}\\)</p> <p>Note that in Cytron's paper, they include two special vertices, <code>entry</code> the entry vertex, and <code>exit</code> as the exit, and <code>entry</code> dominates everything, and <code>exit</code> is only dominated by <code>entry</code>. The purpose is to handle langugage allowing multiple return statements.</p>"},{"location":"name_analysis/#definition-9-iterative-dominance-frontier","title":"Definition 9 - Iterative Dominance Frontier","text":"<p>As pointed out by Cytron's work, if a variable \\(x\\) is updated in a program location (vertex) \\(v\\), a phi-assignment for this variable must be inserted in the dominance frontier of \\(v\\). However inserting a phi assignment at the dominance frontier of \\(v\\) introduces a new location of modifying the variable \\(x\\). This leads to some \"cascading effect\" in computing the phi-assignment locations. </p> <p>We extend the dominance frontier to handle a set of vertices.</p> <p>Let \\(S\\) denote a set of vertices of a graph \\(G\\). We define</p> \\[ df(S, G) = \\bigcup_{v\\in S} df(v, G) \\] <p>We define the iterative dominance frontier recursively as follows</p> \\[ \\begin{array}{l} df_1(S, G) = df(S, G) \\\\ df_n(S, G) = df(S \\cup df_{n-1}(S,G), G) \\end{array} \\] <p>It can be proven that there exists \\(k \\geq 1\\) where \\(df_{k}(S,G) = df_{k+1}(S,G)\\), i.e. the set is bounded. We use \\(df^+(S,G)\\) to denote the upper bound.</p> <p>It follows that if a variable \\(x\\) is modified in locations \\(S\\), then the set of phi-assignments to be inserted for \\(x\\) is \\(df^+(S,G)\\).</p>"},{"location":"name_analysis/#ssa-construction-algorithm_1","title":"SSA construction algorithm","text":"<p>Given the control flow graph \\(G\\), the dominator tree \\(T\\), and the dominance frontier table \\(DFT\\), the SSA construction algorithm consists of two steps.</p> <ol> <li>insert phi assignments to the original program \\(P\\).</li> <li>rename variables to ensure the single assignment property.</li> </ol>"},{"location":"name_analysis/#inserting-phi-assignments","title":"Inserting Phi assignments","text":"<p>Before inserting the phi assignments to \\(P\\), we need some intermediate data structure. </p> <ol> <li>A dictionary \\(E\\) that maps program labels (vertices in CFG) to a set of variables.  \\((l, S) \\in E\\)  implies that variables in \\(S\\) having phi-assignment to be inserted at the vertex label \\(l\\). \\(E\\) can be constructed from the \\(DFT\\) table using the \\(df^+(\\cdot,\\cdot)\\) operation.</li> </ol> <p>Input: the original program <code>P</code>, can be viewed as a list of labeled instructions. Output: the modified program <code>Q</code>. can be viewed as a list of labeled instructions.</p> <p>The phi-assignment insertion process can be described as follows,</p> <ol> <li><code>Q = List()</code></li> <li>for each <code>l:i</code> in <code>P</code><ol> <li>match <code>E.get(l)</code> with <ol> <li>case <code>None</code><ol> <li>add <code>l:i</code> to <code>Q</code></li> </ol> </li> <li>case <code>Some(xs)</code><ol> <li><code>phis = xs.map( x =&gt; x &lt;- phi( k:x | (k in pred(l,G)))</code></li> <li>if <code>phis</code> has more than 1 operand, add <code>l:phis i</code> to <code>Q</code></li> </ol> </li> </ol> </li> </ol> </li> </ol> <p>\\(pred(v, G)\\) retrieves the set of predecessors of vertex (label) in graph \\(G\\). </p> <p>For example, given <code>PA1</code>,</p> <ul> <li>variable \\(x\\) is modified at <code>1</code></li> <li>variable \\(s\\) is modified at <code>2,6</code></li> <li>variable \\(c\\) is modified at <code>3,7</code></li> <li>variable \\(t\\) is modified at <code>4</code></li> </ul> <p>We construct \\(E\\) by consulting the dominance frontier table <code>Table2_PA1</code>.</p> <p><pre><code>E = Map(\n    4 -&gt; Set(\"s\",\"c\", \"t\")\n)\n</code></pre> which says that in node/vertex <code>4</code>, we should insert the phi-assignments for variable <code>s</code> and <code>c</code>.</p> <p>Now we apply the above algorithm to <code>PA1</code> which generates</p> <pre><code>// PRE_SSA_PA1\n1: x &lt;- input\n2: s &lt;- 0\n3: c &lt;- 0\n4: s &lt;- phi(3:s, 8:s)\n   c &lt;- phi(3:c, 8:c)\n   t &lt;- c &lt; x\n5: ifn t goto 9\n6: s &lt;- c + s\n7: c &lt;- c + 1\n8: goto 4\n9: rret &lt;- s \n10: ret\n</code></pre> <p>Note that when we try to insert the phi assignment for <code>t</code> at <code>4</code>, we realize that there is only one operand. This is because <code>t</code> is not defined before label <code>4</code>. In this case we remove the phi assignment for <code>t</code>.</p>"},{"location":"name_analysis/#renaming-variables","title":"Renaming Variables","text":"<p>Given an intermediate output like <code>PRE_SSA_PA1</code>, we need to rename the variable so that there is only one assignment for each variable.</p> <p>Inputs: </p> <ul> <li>a dictionary of stacks <code>K</code> where the keys are the variable names in the original PA program. e.g. <code>K(x)</code> returns the stack for variable <code>x</code>. </li> <li> <p>the input program in with phi assignment but oweing the variable renaming,  e.g. <code>PRE_SSA_PA1</code>. We view the program as a dictionary mapping labels to labeled instructions.</p> </li> <li> <p>For each variable <code>x</code> in the program, initialize <code>K(x) = Stack()</code>.</p> </li> <li>Let label <code>l</code> be the root of the dominator tree \\(T\\).</li> <li>Let <code>vars</code> be an empty list</li> <li>Match <code>P(l)</code> with<ol> <li>case <code>l: phis r &lt;- s</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s' = ren(K, s)</code></li> <li>set <code>Q(l)</code> to <code>l: phis' r &lt;- s'</code></li> </ol> </li> <li>case <code>l: phis r &lt;- s1 op s2</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s1' = ren(K, s1)</code></li> <li><code>s2' = ren(K, s2)</code></li> <li>set <code>Q(l)</code> to  <code>l: phis' r &lt;- s1' op s2'</code></li> </ol> </li> <li>case <code>l: phis x &lt;- s</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s' = ren(K, s)</code></li> <li><code>i = next(K,x)</code></li> <li>append <code>x</code> to <code>vars</code></li> <li>set <code>Q(l)</code> to <code>l: phis' x_i &lt;- s'</code></li> </ol> </li> <li>case <code>l: phis x &lt;- s1 op s2</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>s1' = ren(K, s1)</code></li> <li><code>s2' = ren(K, s2)</code></li> <li><code>i = next(K,x)</code></li> <li>append <code>x</code> to <code>vars</code></li> <li>set <code>Q(l)</code> to  <code>l: phis' x_i &lt;- s1' op s2'</code></li> </ol> </li> <li>case <code>l: phis ifn t goto l'</code> <ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li><code>t' = ren(K, t)</code></li> <li>set <code>Q(l)</code> to  <code>l: phis' ifn t' goto l'</code> </li> </ol> </li> <li>case <code>l: phis ret</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li>set <code>Q(l)</code> to <code>l: phis' ret</code></li> </ol> </li> <li>case <code>l: phis goto l'</code><ol> <li><code>(phis', K, result_list) = processphi(phis, K, vars)</code></li> <li>set <code>Q(l)</code> to <code>l: phis' goto l'</code></li> </ol> </li> </ol> </li> <li>For each successor <code>k</code> of <code>l</code> in the CFG \\(G\\)<ol> <li><code>R = if k in Q { Q } else { R }</code></li> <li>Pattern match <code>R(k)</code> <ol> <li>case <code>k: phis i</code><ol> <li>for each <code>x &lt;- phi(j:x', m:x'')</code> in <code>phis</code><ol> <li>if <code>K(origin(x))</code> is empty, do not add this phi assignment in the result list**.</li> <li>if <code>j == l</code>, <code>x &lt;- phi(j:ren(K,x'), m:x'')</code> into the result list</li> <li>if <code>m == l</code>, <code>x &lt;- phi(j:x', m:ren(K,x''))</code> into the result list</li> </ol> </li> <li>the result list is <code>phis'</code></li> <li>update <code>R(k)</code> to  <code>k: phis' i</code> </li> </ol> </li> <li>case <code>others</code>, no change</li> </ol> </li> </ol> </li> <li>Recursively apply step 3 to the children of <code>l</code> in the \\(T\\).</li> <li>For each <code>x</code> in <code>vars</code>, <code>K(x).pop()</code></li> </ul> <p>Where <code>ren(K, s)</code> is defined as         </p> <pre><code>ren(K,c) = c\nren(K, input) = input\nren(K, r) = r\nren(K, t) = K(t).peek() match \n    case None =&gt; error(\"variable use before being defined.\")\n    case Some(i) =&gt; t_i\n</code></pre> <p>and <code>next(K, x)</code> is defined as </p> <pre><code>next(K, x) = K(x).peek() match \n    case None =&gt; \n        K(x).push(1)\n        0\n    case Some(i) =&gt;\n        K(x).push(i+1)\n        i\n</code></pre> <p>and <code>processphi(phis, K)</code> is defined as  <pre><code>prcessphi(phis, K, vars) = \n    foreach x &lt;- phi(j:x', k:x'') in phis\n        i = K(x).peek() + 1\n        K(x).push(i)\n        append x to vars\n        put x_i &lt;- phi(j:x', k:x'') into result_list\n    return (result_list, K, vars)\n</code></pre></p> <p>and <code>stem(x)</code> returns the original version of <code>x</code> before renaming, e.g. <code>stem(x) = x</code> and <code>stem(x1) = x</code>. We assume there exists some book-keeping mechanism to keep track of that the fact that <code>x</code> is the origin form of <code>x_1</code>.</p> <p>Note on **: In Cytron's paper, all variables must be initialized in the starting vertex of the program. This is not the case in our context. A temp variable can be created to handle nested binary operation, it is might not be initialized. This can be fixed by skipping any phi-assignment of which one of the preceding branch has no such variable assigned. This is sound as this would means  * The phi-assignment is not needed, in case of while statement where the variable is introduced in the while body, or  * The phi-assignment is not fully initialized, in case of if-else where the variable is only introduced in one of the branch.</p> <p>We describe the application of the algorithm to <code>PRE_SSA_PA1</code> (with the dominator tree <code>Tree2_PA1</code> and CFG <code>Graph1_PA1</code>) with the following table.</p> label P(l) Q(l) K P(succ(l)) Q(succ(l)) vars 1 <code>1:x&lt;-input</code> <code>1:x0&lt;-input</code> <code>{x:[0], s:[], c:[], t:[]}</code> <code>{1:{x}}</code> 2 <code>2:s&lt;-0</code> <code>2:s0&lt;-0</code> <code>{x:[0], s:[0], c:[], t:[]}</code> <code>{1:{x}, 2:{s}}</code> 3 <code>3:c&lt;-0</code> <code>3:c0&lt;-0</code> <code>{x:[0], s:[0], c:[0], t:[]}</code> <code>4:s&lt;-phi(3:s0,8:s);c&lt;-phi(3:c0,8:c);t&lt;-c&lt;x</code> <code>{1:{x}, 2:{s}. 3:{c}}</code> 4 <code>4:s&lt;-phi(3:s0,8:s);c&lt;-phi(3:c0,8:c);t&lt;-c&lt;x</code> <code>4:s1&lt;-phi(3:s0,8:s);c1&lt;-phi(3:c0,8:c);t0&lt;-c1&lt;x0</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> 5 <code>5:ifn t goto 9</code> <code>5:ifn t0 goto 9</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> 6 <code>6:s&lt;-c+s</code> <code>6:s2&lt;-c1+s1</code> <code>{x:[0], s:[0,1,2], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}}</code> 7 <code>7:c&lt;-c+1</code> <code>7:c2&lt;-c1+1</code> <code>{x:[0], s:[0,1,2], c:[0,1,2], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}, 6:{s}, 7:{c}}</code> 8 <code>8:goto 4</code> <code>8:goto 4</code> <code>{x:[0], s:[0,1,2], c:[0,1,2], t:[0]}</code> <code>4:s1&lt;-phi(3:s0,8:s2);c1&lt;-phi(3:c0,8:c2);t0&lt;-c1&lt;x0</code> 9 <code>9:rret&lt;-s</code> <code>9:rret&lt;-s1</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> 10 <code>10:ret</code> <code>10:ret</code> <code>{x:[0], s:[0,1], c:[0,1], t:[0]}</code> <code>{1:{x}, 2:{s}. 3:{c}, 4:{s,c,t}}</code> <ul> <li>The label column denotes the current label  being considered.</li> <li>The P(l) column denotes the input labeled instruction being considered.</li> <li>The Q(l) column denotes the output labeled instruction.</li> <li>The K column denotes the set of stacks after the current recursive call.</li> <li>The P(succ(l)) column denotes the modified successor intruction in P, (this applies only when the instruction is not yet available in Q)</li> <li>The Q(succ(l)) column denotes the modified successor instruction in Q.</li> <li>The vars column denotes a mapping of recursive call (indexed by the current label) to the set of variables' ids have been generated (which require popping at the end of the recursive call).</li> </ul> <p>The above derivation eventually yield <code>SSA_PA1</code>.</p> <p>Note that in case of a variable being use before initialized, <code>ren(K, t)</code> will raise an error. </p>"},{"location":"name_analysis/#ssa-back-to-pseudo-assembly","title":"SSA back to Pseudo Assembly","text":"<p>To convert a SSA back to Pseudo Assembly, we have to \"resolve\" the phi-assignments to by moving the branch-dependent assignment back to the preceding labeled instruction. For instance, translating <code>SSA_PA1</code> back to PA while keeping the renamed variables, we have </p> <pre><code>// PA2\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n3: s1 &lt;- s0\n3: c1 &lt;- c0\n4: t0 &lt;- c1 &lt; x0\n5: ifn t0 goto 9\n6: s2 &lt;- c1 + s1\n7: c2 &lt;- c1 + 1\n8: s1 &lt;- s2\n8: c1 &lt;- c2\n8: goto 4\n9: rret &lt;- s1 \n10: ret\n</code></pre> <p>In the above we break the phi-assignments found in </p> <pre><code>4: s1 &lt;- phi(3:s0, 8:s2)\n   c1 &lt;- phi(3:c0, 8:c2)\n   t0 &lt;- c1 &lt; x0\n</code></pre> <p>into </p> <p><code>PhiFor3</code> <pre><code>s1 &lt;- s0 // for label 3\nc1 &lt;- c0\n</code></pre></p> <p>and </p> <p><code>PhiFor8</code> <pre><code>s1 &lt;- s2 // for label 8\nc1 &lt;- c2\n</code></pre></p> <p>We move <code>PhiFor3</code> to label 3</p> <pre><code>3:  c0 &lt;- 0\n3:  s1 &lt;- s0\n3:  c1 &lt;- c0\n</code></pre> <p>and <code>PhiFor8</code> to label 8</p> <pre><code>8: s1 &lt;- s2\n8: c1 &lt;- c2\n8: goto 4\n</code></pre> <p>The \"moving\" phi-assignment operation can be defined in the following algorithm.</p>"},{"location":"name_analysis/#relocating-the-phi-assignments","title":"Relocating the phi-assignments","text":"<p>Input: a PA program \\(P\\) being viewed as a list of labeled instructions. Output: a PA program \\(Q\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(Q\\))</p> <ol> <li>For each \\(l: \\overline{\\phi}\\ i \\in P\\), append \\(l: i\\) to \\(Q\\).</li> <li>For each \\(l: \\overline{\\phi}\\ i\\).<ol> <li>For each <code>x = phi(l1:x1, l2:x2)</code> in \\(\\overline{\\phi}\\)<ol> <li>append <code>l1:x &lt;- x1</code> and <code>l2:x &lt;- x2</code> to \\(Q\\).</li> <li>note that the relocated assignment must be placed before the control flow transition from <code>l1</code> to <code>succ(l1)</code> (and <code>l2</code> to <code>succ(l2)</code>) </li> </ol> </li> </ol> </li> <li>Sort \\(Q\\) by labels using a stable sorting algorithm.</li> </ol> <p>Now since there are repeated labels in <code>PA2</code>, we need an extra relabelling step to convert <code>PA2</code> to <code>PA3</code></p> <pre><code>// PA3\n1: x0 &lt;- input\n2: s0 &lt;- 0\n3: c0 &lt;- 0\n4: s1 &lt;- s0\n5: c1 &lt;- c0\n6: t0 &lt;- c1 &lt; x0\n7: ifn t0 goto 13\n8: s2 &lt;- c1 + s1\n9: c2 &lt;- c1 + 1\n10: s1 &lt;- s2\n11: c1 &lt;- c2\n12: goto 6\n13: rret &lt;- s1 \n14: ret\n</code></pre>"},{"location":"name_analysis/#relabelling","title":"Relabelling","text":"<p>This re-labeling step can be described in the following algorithm.</p> <p>Input: a PA program \\(P\\) being viewed as a list of labeled instructions. (duplicate labels are allowed in \\(P\\)) Output: a PA program \\(Q\\) being viewed as a list of labeled instructions.</p> <ol> <li>Initialize a counter <code>c = 1</code>,</li> <li>Initialize a mapping from old label to new label, <code>M = Map()</code>.</li> <li>Initialize \\(Q\\) as an empty list</li> <li>For each <code>l: i</code> \\(\\in P\\)<ol> <li><code>M = M + (l -&gt; c)</code></li> <li>incremeant <code>c</code> by 1</li> </ol> </li> <li>For each <code>l: i</code> \\(\\in P\\)<ol> <li>append <code>M(l): relabel(i, M)</code> to \\(Q\\)</li> </ol> </li> </ol> <p>where <code>relabel(i, M)</code> is defined as follows</p> <pre><code>relabel(ifn t goto l,M) = ifn t goto M(l)\nrelabel(goto l, M) = goto M(l)\nrelabel(i, M) = i\n</code></pre>"},{"location":"name_analysis/#structured-ssa","title":"Structured SSA","text":"<p>Besides unstructured SSA, it is possible to construct SSA based on a structured program such as SSA. For instance, </p> <pre><code>x = input;\ns = 0;\nc = 0;\nwhile c &lt; x {\n    s = c + s;\n    c = c + 1;\n}\nreturn s;\n</code></pre> <p>Can be converted into a structured SSA </p> <pre><code>x1 = input;\ns1 = 0;\nc1 = 0;\njoin { s2 = phi(s1,s3); c2 = phi(c1,c3); } \nwhile c2 &lt; x1 {\n    s3 = c2 + s2;\n    c3 = c2 + 1;\n}\nreturn s2;\n</code></pre> <p>In the above SSA form, we have a <code>join ... while ...</code> loop.  The join clause encloses the phi assignments merging variable definitions coming from the statement preceding the join while loop and  also the body of the loop.  (Similarly we can introduce a <code>if ... else ... join ...</code> statement).</p> <p>Structured SSA allows us to </p> <ol> <li>conduct name analysis closer to the source language. </li> <li> <p>conduct flow insensitive analysis by incorporating the use-def information. In some cases we get same precision as the flow sensitive analysis. </p> </li> <li> <p>perform code obfuscation. </p> </li> </ol>"},{"location":"name_analysis/#futher-readings","title":"Futher Readings","text":"<ul> <li>https://dl.acm.org/doi/10.1145/2955811.2955813</li> <li>https://dl.acm.org/doi/abs/10.1145/3605156.3606457</li> <li>https://dl.acm.org/doi/10.1145/202530.202532</li> </ul>"},{"location":"schedule/","title":"Schedule","text":"Week Session 1 Session 2 Session 3 Assessment 1 Introduction FP: Expression, Function, Conditional, Recursion Cohort Problem 1,  Homework 1 Homework 1 no submission required. Please refer to the markdown file for instructions. 2 FP: List, Pattern Matching FP: Algebraic Data Type Cohort Problem 2,  Homework 2 3 FP: Generics, GADT FP: Type Classes, Functor Cohort Problem 3,  Homework 2 (Cont'd) Homework 2 5% 4 FP: Applicative FP: Monad Cohort Problem 4,  Homework 3 5 Syntax Analysis: Lexing, Parsing Top-down Parsing Cohort Problem 5,  Homework 3 (Cont'd) Homework 3 5% 6 Parser-Combinator IR: Pseudo-Assembly Cohort Problem 5 Cont'd,  Homework 4,  Project Briefing Bottom-up Parsing for self-study. it won't show up in exams. 7 Homework 4 5% 8 1. Semantic Analysis 2. Dynamic Semantics Mid-term exam (during class hours) Cohort Problem 7 Mid-term 10% 9 Guest Lecture 1. Static Semantics for SIMP  2. Static Semantics for Lambda Calculus Cohort Problem 8, Homework 5 Project Lab 1 10% 10 Name Analysis, SSA Name Analysis, SSA Cohort Problem 9: Name Analysis 11 Lattice, Sign Analysis Liveness Analysis Code Generation: Stack Machine Project Lab 2 10%,  Homework 5 5% 12 Code Generation: Register Allocation Information Flow Analysis Cohort Problem 10: Sign Analysis 13 Memory Management Memory Management Project Lab 3 15% 14 Final Exam (17 Dec Wed 9:00AM-11:00AM) 30%"},{"location":"semantic_analysis/","title":"50.054 - Semantic Analysis","text":""},{"location":"semantic_analysis/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Articulate the meaning of program semantics</li> <li>List different types of program semantics.</li> <li>Explain the limitation of static analysis. </li> </ol>"},{"location":"semantic_analysis/#what-is-program-semantic","title":"What is Program Semantic","text":"<p>In contrast to program syntax which defines the validity of a program, the program semantics define the behavior of a program.</p>"},{"location":"semantic_analysis/#dynamic-semantics","title":"Dynamic Semantics","text":"<p>Dynamic Semantics defines the meaning and behaviors of the given program. The term \"behavior\" could mean</p> <ol> <li>How does the program get executed?</li> <li>What does the program compute / return?</li> </ol>"},{"location":"semantic_analysis/#static-semantics","title":"Static Semantics","text":"<p>Static Semantics describes a set of properties that the given program holds. For example, a typing system (a kind of static semantics) ensures that a well-typed program is free of run-time type errors such as using a string variable in the context of an if condition expression, or adding a float value to a character value.</p>"},{"location":"semantic_analysis/#semantics-analysis","title":"Semantics Analysis","text":"<p>Recall the compiler pipeline</p> <pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]</code></pre> <p>But in fact it could be</p> <pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]\nD --&gt; C</code></pre> <ul> <li>Lexing</li> <li>Input: Source file in String</li> <li>Output: A sequence of valid tokens according to the language specification (grammar)</li> <li>Parsing</li> <li>Input: Output from the Lexer</li> <li>Output: A parse tree representing parsed result according to the parse derivation</li> <li>Semantic Analysis</li> <li>Input: A parse tree or an internal representation<ul> <li>a source parse tree is considered an internal representation</li> </ul> </li> <li>Output:<ul> <li>if succeeds, a parse tree or an internal representation</li> <li>otherwise, an error report</li> </ul> </li> </ul>"},{"location":"semantic_analysis/#goal-of-semantic-analysis","title":"Goal of Semantic Analysis","text":"<p>There mainly two goals of semantic analysis.</p>"},{"location":"semantic_analysis/#optimization","title":"Optimization","text":"<pre><code>x = input;\ny = 0;\ns = 0;\nwhile (y &lt; x) { \n    y = y + 1;\n    t = s;  // t is not used.\n    s = s + y;  \n}\nreturn s;\n</code></pre>"},{"location":"semantic_analysis/#fault-detection","title":"Fault Detection","text":"<pre><code>x = input; \n\nwhile (x &gt;= 0) {\n    x = x - 1;\n}\ny = Math.sqrt(x); // error, can't apply sqrt() to a negative number.\nreturn y;\n</code></pre>"},{"location":"semantic_analysis/#dynamic-semantics-analysis","title":"Dynamic Semantics Analysis","text":"<p>Dynamic semantics analysis aims to find faults and ascertains quality by supplying actual inputs to the target programs. The following are some of the commony used techniques, (we have learned some of them in other modules).</p> <ol> <li>Testing</li> <li>Run-time verification - analyse the target programs with instrumentation by checking the logs and traces against its specification.</li> </ol>"},{"location":"semantic_analysis/#static-semantic-analysis","title":"Static Semantic Analysis","text":"<p>Static Semantic Analysis focuses on achieving the same goal as dynamic semantic analysis by analysing the given program without actually running it.</p> <ol> <li>Type checking and type inference</li> <li>Control flow analysis - to determine the control flow graph of a given program. It gets harder has higher order function and function pointers introduced.</li> <li>Data flow analysis - the goal is determine the possible values being held by a variable at a particular program location.</li> <li>Model checking - given a specification, to reason the program's correctness using a math model, e.g. logic constraints.</li> <li>Program Slicing - try to decompose a program into \"slices\", small units of codes, that exhibit the behaviors of interests.</li> </ol> <p>The advantage is that we gain some generality of the results without worry about the limitation of code coverage. The disadvantage is that we often loose accuracy through approximation</p>"},{"location":"semantic_analysis/#limitation-of-static-semantic-analysis","title":"Limitation of Static Semantic Analysis","text":"<p>It follow Rice's theorem that all non-trivial semantic properties of programs are undecidable. i.e. there exists no algorithm that can decide all semantic properties for all given programs.</p> <p>For example, assume we can find an algorithm that determine whether the variable <code>x</code> in the following function is positive or negative without executing it. </p> <pre><code>def f(path):\n  p = open(path, \"r\")\n  x = 1\n  if eval(p):\n    x = -1\n  return x\n</code></pre> <p>In the above program the analysis of <code>x</code>'s sign (positive or negative) is subject to whether <code>eval(p)</code> is <code>true</code> or <code>false</code>. If such an algorithm exists, as a side effect we can also statically detect whether the given program in <code>path</code> is terminating, which is of course undecidable. </p>"},{"location":"sign_analysis_lattice/","title":"50.054 - Sign Analysis and Lattice Theory","text":""},{"location":"sign_analysis_lattice/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Explain the objective of Sign Analysis</li> <li>Define Lattice and Complete Lattice</li> <li>Define Monotonic Functions</li> <li>Explain the fixed point theorem</li> <li>Apply the fixed pointed theorem to solve equation constraints of sign analysis</li> </ol>"},{"location":"sign_analysis_lattice/#recap","title":"Recap","text":"<p>Recall that one of the goals of semantic analyses is to detect faults without executing the program.</p> <pre><code>// SIMP1\nx = input;\nwhile (x &gt;= 0) {\n    x = x - 1;\n}\ny = Math.sqrt(x); // error, can't apply sqrt() to a negative number\nreturn y;\n</code></pre> <p>Note that our current SIMP syntax does not support <code>&gt;=</code>. We could extend both SIMP and Pseudo Assembly to support a new binary operator <code>||</code> so that we can <code>x&gt;=0</code> into <code>(x &gt; 0) || (x == 0)</code></p> <p>Note that for In Pseudo Assembly we use <code>0</code> to encode <code>false</code> and <code>1</code> to encode <code>true</code>. Hence <code>||</code> can be encoded as <code>+</code>.</p> <p>To detect that the application of <code>sqrt(x)</code> is causing an error, we could apply the sign analysis.</p>"},{"location":"sign_analysis_lattice/#sign-analysis","title":"Sign Analysis","text":"<p>Sign Analysis is a static analysis which statically determines the possible signs of integer variables at the end of a statement in a program. For example </p> <pre><code>// SIMP1\nx = input;        // x could be +, - or 0\nwhile (x &gt;= 0) {  // x could be +, - or 0 \n    x = x - 1;    // x could be +, - or 0\n}                 // x must be -\ny = Math.sqrt(x); // x must be -, y could be +, - or 0\nreturn y;         // x must be -, y could be +, - or 0\n</code></pre> <p>We put the comments as the results of the analysis. </p>"},{"location":"sign_analysis_lattice/#can-we-turn-sign-analysis-into-a-type-inference-problem","title":"Can we turn Sign Analysis into a type inference problem?","text":"<p>The answer is yes, but it is rather imprecise. Let's consider a simple example.</p> <pre><code>// SIMP2\nx = 0;\nx = x + 1;\nreturn x;\n</code></pre> <p>Suppose we introduce 3 subtypes of the <code>Int</code> type, namely <code>Zero</code>, <code>PosInt</code> and <code>NegInt</code></p> <ol> <li>The first statement, we infer <code>x</code> has type <code>Zero</code>.</li> <li>The second statement, we infer <code>x</code> on the RHS, has type <code>Int</code>, the LHS <code>x</code> has type <code>Int</code>. </li> </ol> <p>Unification would fail when we try to combine the result of <code>(x : Zero)</code> and <code>(x : Int)</code>. It is also unsound to conclude that <code>Zero</code> is the final type.</p> <p>This is because the type inference algorithm is a flow-insensitive analysis, which does not take into account that the program is executed from top to bottom.</p>"},{"location":"sign_analysis_lattice/#abstract-domain","title":"Abstract Domain","text":"<p>To analyse the sign property of the variables statically, we could model the sign property using a set of values instead of sub-typing. </p> <p>For example, we may use </p> <ul> <li>\\(\\{\\}\\) to denote the empty set</li> <li>\\(+\\) to denote the set of all positive integers</li> <li>\\(-\\) to denote the set of all ngative integers</li> <li>\\(\\{0\\}\\) to denote the set containing <code>0</code></li> <li>\\(+ \\cup - \\cup \\{0\\}\\) to denote all integers .</li> </ul> <p>For convenience, let's use \\(\\bot\\) to denote \\(\\{\\}\\), \\(\\top\\) to denote \\(+ \\cup - \\cup \\{0\\}\\) and \\(0\\) to denote \\(\\{0\\}\\).  These symbols are the abstract values of the sign property. </p> <p>Since they are sets of values, we can define the subset relation among them.</p> \\[ \\begin{array}{c} \\bot \\subseteq 0  \\\\  \\bot \\subseteq +  \\\\  \\bot \\subseteq -  \\\\  0 \\subseteq \\top \\\\  {+} \\subseteq \\top \\\\  {-} \\subseteq \\top  \\end{array} \\] <p>If we put each abstract domain values in a graph we have the following graph <code>Graph1</code></p> <pre><code>graph\n    A[\"\u22a4\"]---B[-]\n    A---C[0]\n    A---D[+]\n    B---E\n    C---E\n    D---E[\u22a5]</code></pre> <p>informally the above graph structure is called a lattice in math. </p> <p>We will discuss the formal details of lattice shortly. For now let's consider applying the above abstract domain to analyse the sign property of <code>SIMP2</code>. For the ease of implementation we conduct the sign analysis on the Pseudo Assembly instead of SIMP. (The design choice of using Pseudo Assembly is to better align with the project of this module, it is possible to apply the same technique to the SIMP programs directly.)</p> <pre><code>// PA2        // x -&gt; top\n1: x &lt;- 0     // x -&gt; 0\n2: x &lt;- x + 1 // x -&gt; 0 ++ + -&gt; +\n3: rret &lt;- x  // x -&gt; +\n4: ret\n</code></pre> <p>we can follow the flow of the program, before the program starts, we assign \\(\\top\\) to <code>x</code>, as <code>x</code> could be any value. After instruction 1, we deduce that <code>x</code> must be having the abstract value <code>0</code>, since we assign <code>0</code> to <code>x</code>. After instruction 2, we deduce that <code>x</code> has the abstract value <code>+</code> because we add (<code>++</code>) <code>1</code> to an abstract value <code>0</code>. (Note that the <code>0</code>, <code>1</code> and <code>++</code> in the comments are abstract values and abstract operator. Their overloaded definition will be discussed later in this unit.) For simplicity, we ignore the sign analysis for special variable <code>input</code> (which is always \\(\\top\\)) and the register <code>rret</code> (whose sign is not useful.)</p> <p>Let's consider another example </p> <pre><code>// PA3                // x -&gt; top, t -&gt; top\n1: x &lt;- 0             // x -&gt; 0, t -&gt; top\n2: t &lt;- input &lt; 0     // x -&gt; 0, t -&gt; top\n3: ifn t goto 6       // x -&gt; 0, t -&gt; top\n4: x &lt;- x + 1         // x -&gt; +, t -&gt; top\n5: goto 6             // x -&gt; +, t -&gt; top\n6: rret &lt;- x          // x -&gt; upperbound(+, 0) -&gt; top, t -&gt; top\n7: ret                \n</code></pre> <p>We start off by assigning \\(\\top\\) to <code>x</code>, then <code>0</code> to <code>x</code> at the instruction 1. At instruction 2, we assign the result of the boolean condition to <code>t</code> which could be 0 or 1 hence <code>top</code> is the abstract value associated with <code>t</code>. Instruction 3 is a conditional jump. Instruction 4 is the then-branch, we update <code>x</code>'s sign to <code>+</code>. Instruction 6 is the end of the if-else statement, where we need to merge the two possibility of <code>x</code>'s sign. If <code>t</code>'s value is 0, <code>x</code>'s sign is <code>0</code>, otherwise <code>x</code>'s sign is <code>+</code>. Hence we take the upperbound of <code>+</code>, <code>0</code> according to <code>Graph1</code> which is \\(\\top\\).</p> <p>Let's consider the formalism of the lattice and this approach we just presented. </p>"},{"location":"sign_analysis_lattice/#lattice-theory","title":"Lattice Theory","text":""},{"location":"sign_analysis_lattice/#definition-1-partial-order","title":"Definition 1 - Partial Order","text":"<p>A set \\(S\\) is a partial order iff there exists a binary relation \\(\\sqsubseteq\\) with the following condition.</p> <ol> <li>reflexivity: \\(\\forall x \\in S, x \\sqsubseteq x\\)</li> <li>transitivity: \\(\\forall x,y,z \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq z\\) implies \\(x \\sqsubseteq z\\). </li> <li>anti-symmetry: \\(\\forall x,y \\in S, x \\sqsubseteq y \\wedge y\\sqsubseteq x\\) implies \\(x = y\\).</li> </ol> <p>For instance, the set of abstract values in <code>Graph1</code> forms a partial order if we define \\(x \\sqsubseteq y\\) as \"\\(x\\) is at least as precise than \\(y\\)\", (i.e. \\(x\\) is the same or more precise than \\(y\\)). </p>"},{"location":"sign_analysis_lattice/#definition-2-upper-bound","title":"Definition 2 - Upper Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\). We say \\(y\\) is an upper bound of \\(T\\) (written as \\(T\\sqsubseteq y\\)) iff \\(\\forall x \\in T, x \\sqsubseteq y\\). </p>"},{"location":"sign_analysis_lattice/#definition-3-least-upper-bound","title":"Definition 3 - Least Upper Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\), We say \\(y\\) is the least upper bound of \\(T\\) (written as \\(y = \\bigsqcup T\\)) iff \\(\\forall z \\in S, T \\sqsubseteq z\\) implies \\(y \\sqsubseteq z\\).</p> <p>For example, in <code>Graph1</code>, 0 is an upper bound of \\(\\{\\bot\\}\\), but it is not a least upper bound. \\(\\top\\) is a least upper bound of \\(\\{+, - ,0, \\bot\\}\\).</p>"},{"location":"sign_analysis_lattice/#definition-4-lower-bound","title":"Definition 4 - Lower Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\). We say \\(y\\) is a lower bound of \\(T\\) (written as \\(y\\sqsubseteq T\\)) iff \\(\\forall x \\in T, y \\sqsubseteq x\\). </p>"},{"location":"sign_analysis_lattice/#definition-5-greatest-lower-bound","title":"Definition 5 - Greatest Lower Bound","text":"<p>Let \\(S\\) be a partial order, and \\(T \\subseteq S\\), \\(y \\in S\\), We say \\(y\\) is the greatest lower bound of \\(T\\) (written as \\(y = {\\Large \\sqcap} T\\)) iff \\(\\forall z \\in S, z \\sqsubseteq T\\) implies \\(z \\sqsubseteq y\\).</p> <p>For example, in <code>Graph2</code>, 0 is a lower bound of \\(\\{\\top\\}\\), but it is not a greatest lower bound. \\(\\bot\\) is a greatest lower bound of \\(\\{+, - ,0, \\top\\}\\).</p>"},{"location":"sign_analysis_lattice/#definition-6-join-and-meet","title":"Definition 6 - Join and Meet","text":"<p>Let \\(S\\) be a partial order, and \\(x, y \\in S\\). </p> <ol> <li>We define the join of \\(x\\) and \\(y\\) as \\(x \\sqcup y = \\bigsqcup \\{x, y\\}\\). </li> <li>We define the meet of \\(x\\) and \\(y\\) as \\(x \\sqcap y = {\\Large \\sqcap} \\{x, y\\}\\). </li> </ol>"},{"location":"sign_analysis_lattice/#definition-7-lattice","title":"Definition 7 - Lattice","text":"<p>A partial order \\((S, \\sqsubseteq)\\) is a lattice iff \\(\\forall x, y\\in S\\), \\(x \\sqcup y\\) and \\(x \\sqcap y\\) exist.</p>"},{"location":"sign_analysis_lattice/#definition-8-complete-lattice-and-semi-lattice","title":"Definition 8 - Complete Lattice and Semi-Lattice","text":"<p>A partial order \\((S, \\sqsubseteq)\\) is a complete lattice iff \\(\\forall X \\subseteq S\\), \\(\\bigsqcup X\\) and \\({\\Large \\sqcap} X\\) exist.</p> <p>A partial order \\((S, \\sqsubseteq)\\) is a join semilattice iff \\(\\forall X \\subseteq S\\), \\(\\bigsqcup X\\) exists.</p> <p>A partial order \\((S, \\sqsubseteq)\\) is a meet semilattice iff \\(\\forall X \\subseteq S\\), \\({\\Large \\sqcap} X\\) exists.</p> <p>For example the set of abstract values in <code>Graph1</code> and the \"as least as precise\" relation \\(\\sqsubseteq\\) form a complete lattice. </p> <p><code>Graph1</code> is the Hasse diagram of this complete lattice.</p>"},{"location":"sign_analysis_lattice/#lemma-9","title":"Lemma 9","text":"<p>Let \\(S\\) be a non empty finite set and \\((S, \\sqsubseteq)\\) is a lattice, then \\((S, \\sqsubseteq)\\) is a complete lattice.</p> <p>In the next few subsections, we introduce a few commonly use lattices. </p>"},{"location":"sign_analysis_lattice/#powerset-lattice","title":"Powerset Lattice","text":"<p>Let \\(A\\) be a set. We write \\({\\cal P}(A)\\) to denote the powerset of \\(A\\). Then \\(({\\cal P}(A), \\subseteq)\\) forms a complete lattice.  We call it powerset lattice. </p> <p>The above is valid because when we define \\(\\sqsubseteq = \\subseteq\\) and each abstract element in \\({\\cal P}(A)\\), we find that for any \\(T \\subseteq {\\cal P}(A)\\). \\(\\bigsqcup T = \\bigcup_{x \\in T} x\\) and \\({\\Large \\sqcap} T = \\bigcap_{x \\in T} x\\).</p> <p>Can you show that the power set of <code>{1,2,3,4}</code> and \\(\\subseteq\\) form a complete lattice? What is the \\(\\top\\) element and what is the \\(\\bot\\) element? Can you draw the diagaram?</p>"},{"location":"sign_analysis_lattice/#product-lattice","title":"Product Lattice","text":"<p>Let \\(L_1,...,L_n\\) be complete lattices, then \\((L_1 \\times ... \\times  L_n)\\) is a complete lattice where the \\(\\sqsubseteq\\) is defined as </p> \\[ (x_1, ..., x_n) \\sqsubseteq (y_1, ..., y_n)\\ {\\tt iff}\\ \\forall i \\in [1,n], x_i \\sqsubseteq y_i \\] <p>We sometimes write \\(L^n\\) as a short-hand for \\((L_1 \\times ... \\times  L_n)\\).</p> <p>For example in <code>PA3</code>, to analyse the signs for variables we need two lattices, one for variable <code>x</code> and the other for variable <code>t</code>, which forms a product lattice. \\(Sign \\times Sign\\) where \\(Sign\\) is a complete lattice is defined as \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\).</p> <pre><code>graph TD;\n    tt[\"(\u22a4,\u22a4)\"] --- t+[\"(\u22a4,+)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- t0[\"(\u22a4,0)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- tm[\"(\u22a4,-)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- +t[\"(+,\u22a4)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- 0t[\"(0,\u22a4)\"]\n    tt[\"(\u22a4,\u22a4)\"] --- mt[\"(-,\u22a4)\"]\n    t+[\"(\u22a4,+)\"] --- tb[\"(\u22a4,\u22a5)\"]\n    t+[\"(\u22a4,+)\"] --- ++[\"(+,+)\"]\n    t+[\"(\u22a4,+)\"] --- 0+[\"(0,+)\"]\n    t+[\"(\u22a4,+)\"] --- m+[\"(-,+)\"]\n    t0[\"(\u22a4,0)\"] --- tb[\"(\u22a4,\u22a5)\"]\n    t0[\"(\u22a4,0)\"] --- 00[\"(0,0)\"]\n    t0[\"(\u22a4,0)\"] --- +0[\"(+,0)\"]\n    t0[\"(\u22a4,0)\"] --- m0[\"(-,0)\"]\n    tm[\"(\u22a4,-)\"] --- tb[\"(\u22a4,\u22a5)\"]\n    tm[\"(\u22a4,-)\"] --- +m[\"(+,-)\"]\n    tm[\"(\u22a4,-)\"] --- 0m[\"(0,-)\"]\n    tm[\"(\u22a4,-)\"] --- mm[\"(-,-)\"]\n    +t[\"(+,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"]\n    +t[\"(+,\u22a4)\"] --- ++[\"(+,+)\"]\n    +t[\"(+,\u22a4)\"] --- +0[\"(+,0)\"]\n    +t[\"(+,\u22a4)\"] --- +m[\"(+,-)\"]\n    0t[\"(0,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"]\n    0t[\"(0,\u22a4)\"] --- 0+[\"(0,+)\"]\n    0t[\"(0,\u22a4)\"] --- 00[\"(0,0)\"]\n    0t[\"(0,\u22a4)\"] --- 0m[\"(0,-)\"]\n    mt[\"(-,\u22a4)\"] --- bt[\"(\u22a5,\u22a4)\"] \n    mt[\"(-,\u22a4)\"] --- m+[\"(-,+)\"] \n    mt[\"(-,\u22a4)\"] --- m0[\"(-,0)\"] \n    mt[\"(-,\u22a4)\"] --- mm[\"(-,-)\"] \n    ++[\"(+,+)\"] --- b+[\"(\u22a5,+)\"]\n    ++[\"(+,+)\"] --- +b[\"(+,\u22a5)\"]\n    0+[\"(0,+)\"] --- b+[\"(\u22a5,+)\"]\n    0+[\"(0,+)\"] --- 0b[\"(0,\u22a5)\"]\n    m+[\"(-,+)\"] --- b+[\"(\u22a5,+)\"]\n    m+[\"(-,+)\"] --- mb[\"(-,\u22a5)\"]\n    00[\"(0,0)\"] --- b0[\"(\u22a5,0)\"]\n    00[\"(0,0)\"] --- 0b[\"(0,\u22a5)\"]\n    +0[\"(+,0)\"] --- b0[\"(\u22a5,0)\"]\n    +0[\"(+,0)\"] --- +b[\"(+,\u22a5)\"]\n    m0[\"(-,0)\"] --- b0[\"(\u22a5,0)\"]\n    m0[\"(-,0)\"] --- mb[\"(-,\u22a5)\"]\n    +m[\"(+,-)\"] --- bm[\"(\u22a5,-)\"]\n    +m[\"(+,-)\"] --- +b[\"(+,\u22a5)\"]\n    0m[\"(0,-)\"] --- bm[\"(\u22a5,-)\"]\n    0m[\"(0,-)\"] --- 0b[\"(0,\u22a5)\"]\n    mm[\"(-,-)\"] --- bm[\"(\u22a5,-)\"] \n    mm[\"(-,-)\"] --- mb[\"(-,\u22a5)\"]\n    bt[\"(\u22a5,\u22a4)\"] --- b+[\"(\u22a5,+)\"]\n    bt[\"(\u22a5,\u22a4)\"] --- b0[\"(\u22a5,0)\"]\n    bt[\"(\u22a5,\u22a4)\"] --- bm[\"(\u22a5,-)\"]\n    b+[\"(\u22a5,+)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    b0[\"(\u22a5,0)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    bm[\"(\u22a5,-)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    tb[\"(\u22a4,\u22a5)\"] --- +b[\"(+,\u22a5)\"]\n    tb[\"(\u22a4,\u22a5)\"] --- 0b[\"(0,\u22a5)\"]\n    tb[\"(\u22a4,\u22a5)\"] --- mb[\"(-,\u22a5)\"]\n    +b[\"(+,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    0b[\"(0,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]\n    mb[\"(-,\u22a5)\"] --- bb[\"(\u22a5,\u22a5)\"]</code></pre>"},{"location":"sign_analysis_lattice/#map-lattice","title":"Map Lattice","text":"<p>Let \\(L\\) be a complete lattice, \\(A\\) be a set. Let \\(A \\rightarrow L\\) denotes a set of functions </p> \\[ \\{ m \\mid x \\in A \\wedge m(x) \\in L \\} \\] <p>and the \\(\\sqsubseteq\\) relation among functions \\(m_1, m_2 \\in A \\rightarrow L\\) is defined as </p> \\[ m_1 \\sqsubseteq m_2\\ {\\tt iff}\\ \\forall x\\in A, m_1(x) \\sqsubseteq m_2(x) \\] <p>Then \\(A \\rightarrow L\\) is a complete lattice. </p> <p>Note that the term \"function\" used in this definition refers a math function. We could interpret it as a hash table or a Haskell <code>Data.Map.Map a l</code> object where elements of \\(a\\) are keys and elements of \\(l\\) are the values associated with the keys. </p> <p>Map lattice offers a compact alternative to lattices for sign analysis of variables in program like <code>PA3</code> when there are many variables. </p> <p>We can define a map lattice consisting of functions that map variables (<code>x</code> or <code>t</code>) to abstract values in the complete lattice of \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\). </p> <p>For instance, one of the element \"functions\" in the above-mentioned map lattice could be </p> \\[ m_1 = [ x \\mapsto \\top, t \\mapsto + ]  \\] <p>another element function could be </p> \\[ m_2 = [ x \\mapsto \\top, t \\mapsto \\top ]  \\] <p>We conclude that \\(m_1\\sqsubseteq m_2\\). Let \\(Var\\) denote the set of all variables, and \\(Sign\\) denote the complete lattice \\((\\{ \\bot, \\top, 0, + , -\\}, \\sqsubseteq)\\). <code>m1</code> and <code>m2</code> are elements of the complete lattice \\(Var \\rightarrow Sign\\)</p> <pre><code>graph TD;\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t+[\"(x\u2192\u22a4,t\u2192+)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- t0[\"(x\u2192\u22a4,t\u21920)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- tm[\"(x\u2192\u22a4,t\u2192-)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- +t[\"(x\u2192+,t\u2192\u22a4)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- 0t[\"(x\u21920,t\u2192\u22a4)\"]\n    tt[\"(x\u2192\u22a4,t\u2192\u22a4)\"] --- mt[\"(x\u2192-,t\u2192\u22a4)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- ++[\"(x\u2192+,t\u2192+)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- 0+[\"(x\u21920,t\u2192+)\"]\n    t+[\"(x\u2192\u22a4,t\u2192+)\"] --- m+[\"(x\u2192-,t\u2192+)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- 00[\"(x\u21920,t\u21920)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- +0[\"(x\u2192+,t\u21920)\"]\n    t0[\"(x\u2192\u22a4,t\u21920)\"] --- m0[\"(x\u2192-,t\u21920)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- tb[\"(x\u2192\u22a4,t\u2192\u22a5)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- +m[\"(x\u2192+,t\u2192-)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- 0m[\"(x\u21920,t\u2192-)\"]\n    tm[\"(x\u2192\u22a4,t\u2192-)\"] --- mm[\"(x\u2192-,t\u2192-)\"]\n    +t[\"(x\u2192+,t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"]\n    +t[\"(x\u2192+, t\u2192\u22a4)\"] --- ++[\"(x\u2192+, t\u2192+)\"]\n    +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +0[\"(x\u2192+, t\u21920)\"]\n    +t[\"(x\u2192+, t\u2192\u22a4)\"] --- +m[\"(x\u2192+, t\u2192-)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5,t\u2192\u22a4)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0+[\"(x\u21920, t\u2192+)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- 00[\"(x\u21920, t\u21920)\"]\n    0t[\"(x\u21920, t\u2192\u22a4)\"] --- 0m[\"(x\u21920, t\u2192-)\"]\n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] \n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m+[\"(x\u2192-, t\u2192+)\"] \n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- m0[\"(x\u2192-, t\u21920)\"] \n    mt[\"(x\u2192-, t\u2192\u22a4)\"] --- mm[\"(x\u2192-, t\u2192-)\"] \n    ++[\"(x\u2192+, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"]\n    ++[\"(x\u2192+, t\u2192+)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"]\n    0+[\"(x\u21920, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"]\n    0+[\"(x\u21920, t\u2192+)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"]\n    m+[\"(x\u2192-, t\u2192+)\"] --- b+[\"(x\u2192\u22a5, t\u2192+)\"]\n    m+[\"(x\u2192-, t\u2192+)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"]\n    00[\"(x\u21920, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"]\n    00[\"(x\u21920, t\u21920)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"]\n    +0[\"(x\u2192+, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"]\n    +0[\"(x\u2192+, t\u21920)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"]\n    m0[\"(x\u2192-, t\u21920)\"] --- b0[\"(x\u2192\u22a5, t\u21920)\"]\n    m0[\"(x\u2192-, t\u21920)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"]\n    +m[\"(x\u2192+, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"]\n    +m[\"(x\u2192+, t\u2192-)\"] --- +b[\"(x\u2192+, t\u2192\u22a5)\"]\n    0m[\"(x\u21920, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"]\n    0m[\"(x\u21920, t\u2192-)\"] --- 0b[\"(x\u21920, t\u2192\u22a5)\"]\n    mm[\"(x\u2192-, t\u2192-)\"] --- bm[\"(x\u2192\u22a5, t\u2192-)\"] \n    mm[\"(x\u2192-, t\u2192-)\"] --- mb[\"(x\u2192-, t\u2192\u22a5)\"]\n    bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b+[\"(x\u2192\u22a5,t\u2192+)\"]\n    bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- b0[\"(x\u2192\u22a5,t\u21920)\"]\n    bt[\"(x\u2192\u22a5, t\u2192\u22a4)\"] --- bm[\"(x\u2192\u22a5,t\u2192-)\"]\n    b+[\"(x\u2192\u22a5, t\u2192+)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    b0[\"(x\u2192\u22a5, t\u21920)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    bm[\"(x\u2192\u22a5, t\u2192-)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- +b[\"(x\u2192+,t\u2192\u22a5)\"]\n    tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- 0b[\"(x\u21920,t\u2192\u22a5)\"]\n    tb[\"(x\u2192\u22a4, t\u2192\u22a5)\"] --- mb[\"(x\u2192-,t\u2192\u22a5)\"]\n    +b[\"(x\u2192+, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    0b[\"(x\u21920, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]\n    mb[\"(x\u2192-, t\u2192\u22a5)\"] --- bb[\"(x\u2192\u22a5, t\u2192\u22a5)\"]</code></pre>"},{"location":"sign_analysis_lattice/#sign-analysis-with-lattice","title":"Sign analysis with Lattice","text":"<p>As we informally elaborated earlier, the sign analysis approach \"infer\" the signs of the variables based on the \"previous states\" set by the previous statements.</p> <pre><code>// PA2         // s0 = [x -&gt; top]\n1: x &lt;- 0      // s1 = s0[x -&gt; 0]\n2: x = x + 1   // s2 = s1[x  -&gt; s1(x) ++ +]\n3: rret &lt;- x   // s3 = s2\n4: ret\n</code></pre> <p>In the above, we analyse <code>SIMP2</code> program's sign by \"packaging\" the variable to sign bindings into some state variables, <code>s1</code>, <code>s2</code>, <code>s3</code> and <code>s4</code>. Each state variable is mapping from variable to the abstract values from \\(\\{\\top, \\bot, +, -, 0\\}\\). Since \\(\\{\\top, \\bot, +, -, 0\\}\\) is a lattice, the set of state variables is a map lattice. </p> <p>Note that we could also model the state variables as a tuple of lattice as a produce lattice.</p> <p>Next we would like to model the change of variable signs based on the previous instructions. We write <code>s[x -&gt; v]</code> to denote a new state <code>s'</code> which is nearly the same as <code>s</code> except that the mapping of variable <code>x</code> is changed to <code>v.</code> (In Haskell style syntax, assuming <code>s</code> is a <code>Data.Map.Map Var Sign</code> object, then <code>s[x-&gt;v]</code> is actually <code>Data.Map.insert x v s</code> in Haskell.)</p> <p>We write <code>s(x)</code> to denote a query of variable <code>x</code>'s value in state <code>s</code>. (In Haskell style syntax, it is <code>case Data.Map.lookup x s of { Just v -&gt; v }</code>)</p> <p>In the above example, we define <code>s2</code> based on <code>s1</code> by \"updating\" variable <code>x</code>'s sign to <code>0</code>. We update <code>x</code>'s sign in <code>s3</code> based on <code>s2</code> by querying <code>x</code>'s sign in <code>s2</code> and modifying it by increasing by <code>1</code>. We define the <code>++</code> abstract operator for abstract values \\(\\{\\top, \\bot, +, -, 0\\}\\) as follows</p> ++ \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) - \\(\\top\\) \\(\\top\\) - - \\(\\bot\\) 0 \\(\\top\\) + - 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>Where the first column from the 2<sup>nd</sup> rows onwards are the left operand and the first row from the 2<sup>nd</sup> column onwards are the right operand. Similarly we can define the other abstract operators</p> -- \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) + + \\(\\bot\\) - \\(\\top\\) - \\(\\top\\) - \\(\\bot\\) 0 \\(\\top\\) - + 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) ** \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) 0 \\(\\bot\\) + \\(\\top\\) + - 0 \\(\\bot\\) - \\(\\top\\) - + 0 \\(\\bot\\) 0 0 0 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) &lt;&lt; \\(\\top\\) + - 0 \\(\\bot\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\top\\) \\(\\bot\\) + \\(\\top\\) \\(\\top\\) 0 0 \\(\\bot\\) - \\(\\top\\) + \\(\\top\\) + \\(\\bot\\) 0 \\(\\top\\) + 0 0 \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) \\(\\bot\\) <p>Given the definitions of the abstract operators, our next task is to solve the equation among the state variable <code>s0</code>, <code>s1</code>, <code>s2</code> and <code>s3</code></p> <pre><code>s0 = [x -&gt; top]\ns1 = s0[x -&gt; 0]\ns2 = s1[x  -&gt; s1(x) ++ +]\ns3 = s2\n</code></pre> <p>Note that we can't use unification here as <code>x</code> is assocated with different sign abstract values at different states (instructions).</p> <p>Question: If we use SSA PA instead of PA, can the generated equations be solved using unification?</p> <p>To solve the set of equation constraints we could process the equations from top to bottom.</p> <pre><code>s0 = [x -&gt; top]\ns1 = [x -&gt; 0]\ns2 = [x -&gt; +]\ns3 = [x -&gt; +]\n</code></pre> <p>Then we can conclude that the sign of variable <code>x</code> at instruction 3 is positive. Note that all the states, <code>s0</code>, <code>s1</code>, <code>s2</code> and <code>s3</code> are elements in the map lattice \\(Var \\rightarrow Sign\\).</p> <p>However, we need a more general solver as the equation systems could be recursive in the presence of loops.  For example.</p> <pre><code>// PA4              // s0 = [x -&gt; top, y -&gt; top, t -&gt; top]\n1: x &lt;- input       // s1 = s0\n2: y &lt;- 0           // s2 = s1[y -&gt; 0]\n3: t &lt;- x &gt; 0       // s3 = upperbound(s2,s7)[t -&gt; top]\n4: ifn t goto 8     // s4 = s3\n5: y &lt;- y + 1       // s5 = s4[y -&gt; s4(y) ++ +]\n6: x &lt;- x - 1       // s6 = s5[x -&gt; s5(x) -- +]\n7: goto 3           // s7 = s6\n8: rret &lt;- y        // s8 = s4\n9: ret \n</code></pre> <p>In the above the <code>upperbound(s, t)</code> can be define as \\(s \\sqcup t\\), assuming \\(s\\) and \\(t\\) are elements of a complete lattice. </p> <p>Note that all the states in the above analysis are elements of \\(Var \\rightarrow Sign\\), hence \\(s \\sqcup t\\) can be defined as </p> \\[ [ x\\mapsto s(x) \\sqcup t(x) \\mid x \\in Var ] \\] <p>To solve equation systems like the above, we need some \"special\" functions that operates on lattices.</p>"},{"location":"sign_analysis_lattice/#definition-10-monotonic-function","title":"Definition 10 - Monotonic Function","text":"<p>Let \\(L_1\\) and \\(L_2\\) be lattices, a function \\(f : L_1 \\longrightarrow L_2\\) is monotonic iff \\(\\forall x,y \\in L_1, x \\sqsubseteq y\\) implies \\(f(x) \\sqsubseteq f(y)\\).</p> <p>Note that the term \"function\" in the above is can be treated as the function/method that we define in a programl.</p> <p>For instance given the lattice described in <code>Graph1</code>, we define the following function </p> \\[ \\begin{array}{rcl} f_1(x) &amp; = &amp; \\top \\end{array} \\] <p>Function \\(f_1\\) is monotonic because </p> \\[ \\begin{array}{r} f_1(\\bot) = \\top \\\\ f_1(0) = \\top \\\\  f_1(+) = \\top \\\\  f_1(-) = \\top \\\\  f_1(\\top) = \\top \\end{array}  \\] <p>and \\(\\top \\sqsubseteq \\top\\)</p> <p>Let's consider another function \\(f_2\\) </p> \\[ \\begin{array}{rcl} f_2(x) &amp; = &amp; x \\sqcup +   \\end{array} \\] <p>is \\(f_2\\) monotonic? Recall \\(\\sqcup\\) computes the least upper bound of the operands</p> \\[ \\begin{array}{r} f_2(\\bot) = \\bot \\sqcup + = + \\\\ f_2(0) = 0 \\sqcup + = \\top \\\\  f_2(+) = + \\sqcup + = + \\\\  f_2(-) = - \\sqcup + = \\top \\\\  f_2(\\top) = \\top \\sqcup + = \\top  \\end{array}  \\] <p>Note that </p> \\[ \\begin{array}{r} \\bot \\sqsubseteq + \\sqsubseteq \\top\\\\  \\bot \\sqsubseteq 0 \\sqsubseteq \\top\\\\  \\bot \\sqsubseteq - \\sqsubseteq \\top \\end{array} \\] <p>when we apply \\(g\\) to all the abstract values in the above inequalities, we find that </p> \\[ \\begin{array}{r} f_2(\\bot) \\sqsubseteq f_2(+) \\sqsubseteq f_2(\\top)\\\\  f_2(\\bot) \\sqsubseteq f_2(0) \\sqsubseteq f_2(\\top)\\\\  f_2(\\bot) \\sqsubseteq f_2(-) \\sqsubseteq f_2(\\top) \\end{array} \\] <p>hold. Therefore \\(g\\) is monotonic. </p> <p>Let \\(L\\) be a lattice and \\(L_1 \\times ... \\times L_n\\) be a product lattice. It follows from Definition 10 that \\(f : L_1 \\times ... \\times L_n \\rightarrow L\\) is monotone iff \\(\\forall (v_1, ..., v_n) \\sqsubseteq (v_1', ..., v_n')\\) imples \\(f (v_1, ..., v_n) \\sqsubseteq f  (v_1', ..., v_n')\\)</p>"},{"location":"sign_analysis_lattice/#lemma-11-constant-function-is-monotonic","title":"Lemma 11 - Constant Function is Monotonic.","text":"<p>Every constant function \\(f\\) is monotonic.</p>"},{"location":"sign_analysis_lattice/#lemma-12-sqcup-and-sqcap-are-monotonic","title":"Lemma 12 - \\(\\sqcup\\) and \\(\\sqcap\\) are Monotonic.","text":"<p>Let's treat \\(\\sqcup\\) as a function \\(L \\times L \\rightarrow L\\), then \\(\\sqcup\\) is monotonic.</p> <p>Similar observation applies to \\(\\sqcap\\).</p>"},{"location":"sign_analysis_lattice/#definition-13-fixed-point-and-least-fixed-point","title":"Definition 13 - Fixed Point and Least Fixed Point","text":"<p>Let \\(L\\) be a lattice and \\(f: L \\rightarrow L\\) is be function. We say \\(x \\in L\\) is a fixed point of \\(f\\) iff \\(x = f(x)\\). We say \\(x\\) is a least fixed point of \\(f\\) iff \\(\\forall y \\in L\\), \\(y\\) is a fixed point of \\(f\\) implies \\(x \\sqsubseteq y\\).</p> <p>For example, for function \\(f_1\\), \\(\\top\\) is a fixed point and also the least fixed point. For function \\(f_2\\), \\(+\\), \\(\\top\\) are the fixed points and \\(+\\) is the least fixed point.</p>"},{"location":"sign_analysis_lattice/#theorem-14-fixed-point-theorem","title":"Theorem 14 - Fixed Point Theorem","text":"<p>Let \\(L\\) be a complete lattice with finite height, every monotonic  function \\(f\\) has a unique least fixed point point, namely \\({\\tt lfp}(f)\\), defined as </p> \\[ {\\tt lfp}(f) = \\bigsqcup_{i\\geq 0}f^i(\\bot) \\] <p>Where \\(f^n(x)\\) is a short hand for </p> \\[ \\overbrace{f(...(f(x)))}^{n\\ {\\tt times}} \\] <p>The height of a complete lattice is the length of the longest path from \\(\\top\\) to  \\(\\bot\\).</p> <p>The intution of this theorem is that if we start from the \\(\\bot\\) of the lattice and keep applying a monotonic function \\(f\\), we will reach a fixed point and it must be the only least fixed point.  The presence of \\(\\bigsqcup\\) in the definition above is find the common upper bound for all these applications. Note that the \\(f^{i}(\\bot) \\sqcup f^{i+1}(\\bot) = f^{i+1}(\\bot)\\) as \\(f\\) is monotonic. Eventually, we get rid of the \\(\\bigsqcup\\) in the result.</p> <p>For example, consider function \\(f_2\\). If we start from \\(\\bot\\) and apply \\(f_2\\) repetively, we reach \\(+\\) which is the least fixed point.</p>"},{"location":"sign_analysis_lattice/#lemma-15-map-update-with-monotonic-function-is-monotonic","title":"Lemma 15 - Map update with monotonic function is Monotonic","text":"<p>Let \\(f : L_1 \\rightarrow (A \\rightarrow L_2)\\) be a monotonic function from a lattice \\(L_1\\) to a map lattice \\(A \\rightarrow L_2\\). Let \\(g: L_1 \\rightarrow L_2\\) be another monotonic function. Then \\(h(x) = f(x)[a \\mapsto g(x)]\\) is a monotonic function of \\(L_1 \\rightarrow (A \\rightarrow L_2)\\).</p> <p>To gain some intuition of this lemma, let's try to think in terms of Haskell. Recall that the map lattice is \\(A \\rightarrow L_2\\) can be treated as  <code>Map a l2</code> in Haskell style, and <code>l2</code> is a lattice. <code>f :: l1 -&gt; Map a l2</code> is a Haskell function that's monotonic, <code>g:: l1 -&gt; l2</code> is another Haskell function which is monotonic. Then we can conclude that </p> <p><pre><code>a :: A\na = ... --  a is an element of A, where A is a ground type.\nh :: l1 -&gt; Map a l2 \nh x = insert a (g x) (f x)\n</code></pre> <code>h</code> is also monotonic. </p> <p>Since \\(f\\) is monotonic, given \\(x \\sqsubseteq y\\), we have \\(f(x) \\sqsubseteq f(y)\\). It follows that \\(f(x)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(x)]\\). Since \\(g\\) is monotonic, we have \\(f(y)[ a\\mapsto g(x)] \\sqsubseteq f(y)[ a\\mapsto g(y)]\\).</p> <p>With the fixed point theoem and Lemma 15, we are ready to define a general solution to solve the equations sytems generated from the sign analysis.</p>"},{"location":"sign_analysis_lattice/#naive-fixed-point-algorithm","title":"Naive Fixed Point Algorithm","text":"<p>input: a function <code>f</code>.</p> <ol> <li>initialize <code>x</code> as \\(\\bot\\)</li> <li>apply <code>f(x)</code> as <code>x1</code></li> <li>check <code>x1 == x</code> <ol> <li>if true return <code>x</code></li> <li>else, update <code>x = x1</code>, go back to step 2.</li> </ol> </li> </ol> <p>For instance, if we apply the above algorithm to the \\(f_2\\) with the lattice in <code>Graph1</code>, we have the following iterations.</p> <ol> <li>\\(x = \\bot, x_1 = f_2(x) = +\\)</li> <li>\\(x = +, x_1 = f_2(x) = +\\)</li> <li>fixed point is reached, return \\(x\\). </li> </ol>"},{"location":"sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa2","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of <code>PA2</code>","text":"<p>Recall the set of equations generated from <code>PA2</code></p> <pre><code>s0 = [x -&gt; top]\ns1 = s0[x -&gt; 0]\ns2 = s1[x  -&gt; s1(x) ++ +]\ns3 = s2\n</code></pre> <p>and we use \\(Var\\) to denote the set of variables, in this case we have only one variable \\(x\\). and \\(Sign\\) to denote the sign lattice described in <code>Graph1</code>.</p> <p>We model the equation systems by defining one lattice for each equation, \\((Var \\rightarrow Sign)\\). In total. we have four map lattices, one for <code>s0</code>, one for <code>s1</code>, and etc. Then we \"package\" these four map lattices into a product lattice  \\(L = (Var \\rightarrow Sign)^4\\).  Since \\(Sign\\) is a complete lattice, so is \\(L\\).</p> <p>Next we want to define the monotonic function \\(f_3\\) that helps us to find least fixed point which will be the solution of the above equation systems.  The type of \\(f_3\\) should be \\(L \\rightarrow L\\), or </p> \\[((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign)) \\rightarrow ((Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign) \\times (Var \\rightarrow Sign))\\] <p>in its unabridge form.</p> <p>Reminder: Even though we write map lattice as \\(Var \\rightarrow Sign\\), but it is like a <code>Map[Var, Sign]</code>.</p> <p>Next we re-model the relations among <code>s0,s1,s2,s3</code> in above equation system in \\(f_3\\) as follows</p> \\[ f_3(s_0,s_1,s_2,s_3) = ([x \\mapsto \\top],s_0[x \\mapsto 0], s_1[x \\mapsto (s_1(x) {\\tt ++}\\ {\\tt +} )], s_2) \\] <p>Thanks to Lemma 15, \\(f_3\\) is monotonic.</p> <p>The last step is to apply the naive fixed point algorithm to \\(f_3\\) with \\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\) as the starting point.</p> <ol> <li>\\(s_0 = s_1 = s_2 = s_3 = [x \\mapsto \\bot]\\),  $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) &amp;  = &amp; ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_1(x) {\\tt ++} {\\tt +})], s_2) \\ &amp; = &amp; ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto \\bot]) \\end{array} $$</li> <li> <p>\\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto \\bot]\\),  $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) &amp; = &amp; ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_1(x) {\\tt ++} {\\tt +})], s_2) \\ &amp; = &amp; ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$</p> </li> <li> <p>\\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\),  $$ \\begin{array}{rcl} f_3(s_0,s_1,s_2,s_3) &amp; = &amp; ([x \\mapsto \\top], s_0[x \\mapsto 0], s_1[x \\mapsto (s_0(x) {\\tt ++} {\\tt +})], s_2) \\ &amp; = &amp; ([x \\mapsto \\top], [x \\mapsto 0], [x \\mapsto +], [x \\mapsto +]) \\end{array} $$</p> </li> <li> <p>fixed point reached, the solution is \\(s_0 = [x \\mapsto \\top], s_1 =[x \\mapsto 0], s_2 = [x \\mapsto +], s_3 = [x \\mapsto +]\\).</p> </li> </ol>"},{"location":"sign_analysis_lattice/#applying-naive-fixed-point-algorithm-to-sign-analysis-problem-of-pa4","title":"Applying Naive Fixed Point Algorithm to Sign Analysis Problem of <code>PA4</code>","text":"<p>Recall the set of equations generated from <code>PA4</code>'s sign analysis <pre><code>s0 = [x -&gt; top, y -&gt; top, t -&gt; top]\ns1 = s0\ns2 = s1[y -&gt; 0]\ns3 = upperbound(s2,s7)[t -&gt; top]\ns4 = s3\ns5 = s4[y -&gt; s4(y) ++ +]\ns6 = s5[x -&gt; s5(x) -- +]\ns7 = s6\ns8 = s4\n</code></pre></p> <p>We define a monotonic function \\(f_4 : (Var \\rightarrow Sign)^9 \\rightarrow (Var \\rightarrow Sign)^9\\) as follows</p> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       s_0, \\\\      s_1[y \\mapsto 0], \\\\      (s_2 \\sqcup s_7)[t \\mapsto \\top], \\\\      s_3, \\\\      s_4[y \\mapsto s_4(y) {\\tt ++} \\ {\\tt +}], \\\\      s_5[x \\mapsto s_5(x) {\\tt --} \\ {\\tt +}], \\\\      s_6, \\\\      s_4     \\end{array}      \\right ) \\end{array} \\] <ol> <li>\\(s_0 = s_1 = s_2 = s_3 = s_4 = s_5 = s_6 = s_7 = s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]\\), </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]     \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\top], \\\\  s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot]     \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\top]      \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_2 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\bot], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top]      \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[  \\begin{array}{l}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\ s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\       [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top], \\\\ s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\   s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto \\bot, t \\mapsto \\bot], \\\\  s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right ) \\end{array} \\] <ol> <li> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_8 = [x \\mapsto \\bot, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\          [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right )     \\end{array} \\] <ol> <li> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      s_6 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] </li> </ol> \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right )     \\end{array} \\] <p>9.</p> \\[ \\begin{array}{c}  s_0 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\   s_1 = [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\  s_2 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_3 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\ s_4 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\  s_5 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      s_6 = [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\  s_7 = [x \\mapsto \\bot, y \\mapsto +, t \\mapsto \\top], \\\\  s_8 = [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top] \\end{array}  \\] \\[ \\begin{array}{rcl} f_4(s_0, s_1, s_2, s_3, s_4, s_5, s_6, s_7, s_8, s_9) &amp; = &amp; \\left (     \\begin{array}{c}      [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\       [x \\mapsto \\top, y \\mapsto \\top, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\     [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto +, t \\mapsto \\top], \\\\      [x \\mapsto \\top, y \\mapsto 0, t \\mapsto \\top]     \\end{array}      \\right )     \\end{array} \\] <p>If we apply \\(f_4\\) one more time to the above set of states, we get the same states. At this point, we reach the fixed point of the \\(f_4\\) function w.r.t the \\((Var \\rightarrow Sign)^9\\) lattice. </p>"},{"location":"sign_analysis_lattice/#optimization","title":"Optimization","text":"<p>This naive fixed point algorithm works but not efficient, namely it blindly applies the \"update\" of a state \\(s_i\\) based on \\(s_{i-1}\\) though there is no changes to \\(s_{i-1}\\) in the last iteration. For example from step 7 to step 8, \\(s_3\\) is updated though there is no change to \\(s_2\\). </p> <p>A more efficient algorithm can be derived if we keep track of the dependencies among the states and perform the \"update of a state \\(s_i\\) if \\(s_i\\) is based on \\(s_{i-1}\\) and \\(s_{i-1}\\) has changed.</p>"},{"location":"sign_analysis_lattice/#generalizing-the-monotone-constraints-for-sign-analysis","title":"Generalizing the monotone constraints for sign analysis","text":"<p>We would like to have a systematic way to define the monotone constraints (i.e. monotonic functions) for analyses like sign analysis.</p> <p>Let \\(v_i\\) denote a vertex in CFG. We write \\(pred(v_i)\\) to denote the set of predecesors of \\(v_i\\). let \\(s_i\\) denote the state variable of the vertex \\(v_i\\) in the CFG. We write \\(pred(s_i)\\) to denote the set of state variables of the predecessor of \\(v_i\\).</p> <p>For sign analysis,  we define the following helper function </p> \\[join(s) = \\bigsqcup pred(s)\\] <p>To avoid confusion, we write \\(src\\) to denote the source operands in PA instead of \\(s\\). Let \\(V\\) denotes the set of variables in the PA program's being analysed.</p> <p>The monotonic functions can be defined by the following cases.</p> <ul> <li>case \\(l == 0\\), \\(s_0 = \\lbrack x \\mapsto \\top \\mid x \\in V\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src\\), \\(s_l = join(s_l) \\lbrack t \\mapsto join(s_l)(src)\\rbrack\\)</li> <li>case \\(l: t \\leftarrow src_1\\ op\\ src_2\\), \\(s_l = join(s_l) \\lbrack t \\mapsto (join(s_l)(src_1)\\ abs(op)\\ join(s_l)(src_2))\\rbrack\\)</li> <li>other cases: \\(s_l = join(s_l)\\)</li> </ul> <p>Let \\(m\\) be a map lattice object, and \\(src\\) be a PA source operand, the lookup operation \\(m(src)\\) for sign analysis is defined as follows </p> \\[ \\begin{array}{rcl} m(c) &amp; = &amp; \\left \\{          \\begin{array}{cc}         0 &amp; c == 0 \\\\         + &amp; c &gt; 0 \\\\         - &amp; c &lt; 0              \\end{array}          \\right . \\\\ \\\\ m(t) &amp; = &amp; \\left \\{         \\begin{array}{cc}         v &amp; t \\mapsto v \\in m \\\\          error &amp; otherwise         \\end{array}             \\right . \\\\ \\\\  m(r) &amp; = &amp; error \\end{array} \\] <p>Let \\(op\\) be PA operator, we define the abstraction operation \\(abs(op)\\) for sign analysis as follows, </p> \\[ \\begin{array}{rcl} abs(+) &amp; = &amp; ++\\\\ abs(-) &amp; = &amp; -- \\\\ abs(*) &amp; = &amp; ** \\\\ abs(&lt;) &amp; = &amp; &lt;&lt; \\\\  abs(==) &amp; = &amp; ===  \\end{array} \\] <p>We have seen the definitions of \\(++, --, **\\) and \\(&lt;&lt;\\)</p> <p>Question: can you define \\(===\\)?</p> <p>Question: the abstraction operations are pretty coarse (not accurate). For instance, <code>&lt;&lt;</code> and <code>===</code> should return either <code>0</code> or <code>1</code> hence \\(\\top\\) is too coarse. Can you define a lattice for sign analysis which offers better accuracy? </p> <p>Question: Convert <code>SIMP1</code> into a PA. Can we apply the sign analysis to find out that the <code>sqrt(x)</code> is definifely failing?</p>"},{"location":"static_semantics/","title":"50.054 Static Semantics For SIMP","text":""},{"location":"static_semantics/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Explain what static semantics is.</li> <li>Apply type checking rules to verify the type correctness property of a SIMP program.</li> <li>Explain the relation between type system and operational semantics.</li> <li>Apply type inference algorithm to generate a type environment given a SIMP program.</li> </ol>"},{"location":"static_semantics/#what-is-static-semantics","title":"What is static semantics?","text":"<p>While dynamic semantics defines the run-time behavior of the given program, static semantics defines the compile-time properties of the given program.</p> <p>For example, a statically correct program, must satisfy some properties</p> <ol> <li>all uses of variables in it must be defined somewhere earlier. </li> <li>all the use of variables, the types must be matching with the expected type in the context.</li> <li>... </li> </ol> <p>Here is a statically correct SIMP program,</p> <pre><code>x = 0;\ny = input;\nif y &gt; x {\n    y = 0;\n}\nreturn y;\n</code></pre> <p>because it satifies the first two properties. </p> <p>The following program is not statically correct.</p> <pre><code>x = 0;\ny = input;\nif y + x { // type error\n    x = z; // the use of an undefined variable z\n}\nreturn x;\n</code></pre> <p>Static checking is to rule out the statically incorrect programs.</p>"},{"location":"static_semantics/#type-checking-for-simp","title":"Type Checking for SIMP","text":"<p>We consider the type checking for SIMP programs.</p> <p>Recall the syntax rules for SIMP</p> \\[ \\begin{array}{rccl} (\\tt Statement) &amp; S &amp; ::= &amp; X = E ; \\mid return\\ X ; \\mid nop; \\mid if\\ E\\ \\{ \\overline{S} \\}\\ else\\ \\{ \\overline{S} \\} \\mid while\\ E\\ \\{ \\overline{S} \\} \\\\ (\\tt Expression) &amp; E &amp; ::= &amp; E\\ OP\\ E \\mid X \\mid C  \\mid (E) \\\\ (\\tt Statements) &amp; \\overline{S} &amp; ::= &amp; S \\mid S\\ \\overline{S} \\\\ (\\tt Operator) &amp; OP &amp; ::= &amp; + \\mid - \\mid * \\mid &lt; \\mid == \\\\  (\\tt Constant) &amp; C &amp; ::= &amp; 0 \\mid 1 \\mid 2 \\mid ... \\mid true \\mid false \\\\  (\\tt Variable) &amp; X &amp; ::= &amp; a \\mid b \\mid c \\mid d \\mid ... \\\\   {\\tt (Types)} &amp; T &amp; ::= &amp; int \\mid bool  \\\\   {\\tt (Type\\ Environments)} &amp; \\Gamma &amp; \\subseteq &amp; (X \\times T) \\end{array} \\] <p>We use the symbol \\(\\Gamma\\) to denote a type environments mapping SIMP variables to types. \\(T\\) to denote a type. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\), i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\). We assume for all \\(x \\in dom(\\Gamma)\\), there exists only one entry of \\((x,T) \\in \\Gamma\\).</p> <p>We define two different relations, </p> <ol> <li>\\(\\Gamma \\vdash E : T\\), which type-checks a SIMP expresion \\(E\\) against a type \\(T\\) under \\(\\Gamma\\).</li> <li>\\(\\Gamma \\vdash \\overline{S}\\), which type-checks a SIMP statement sequence \\(\\overline{S}\\) under \\(\\Gamma\\).</li> </ol>"},{"location":"static_semantics/#type-checking-rules-for-simp-expressions","title":"Type checking rules for SIMP Expressions","text":"\\[ \\begin{array}{rc} {\\tt (tVar)} &amp; \\begin{array}{c}                 (X,T) \\in \\Gamma                 \\\\ \\hline                 \\Gamma \\vdash X : T                 \\end{array}  \\end{array} \\] <p>In the rule \\({\\tt (tVar)}\\), we type check the variable \\(X\\) having type \\(T\\) under the type environment \\(\\Gamma\\) if we can find the entry \\((X,T)\\) in \\(\\Gamma\\).</p> \\[ \\begin{array}{rc} {\\tt (tInt)} &amp; \\begin{array}{c}                 C\\ {\\tt is\\ an\\ integer}                 \\\\ \\hline                 \\Gamma \\vdash C : int                 \\end{array} \\\\ \\\\  {\\tt (tBool)} &amp; \\begin{array}{c}                 C \\in \\{true,false\\}                 \\\\ \\hline                 \\Gamma \\vdash C : bool                 \\end{array}  \\end{array} \\] <p>In the rule \\({\\tt (tInt)}\\), we type check an integer constant having type \\(int\\). Similarly, we type check a boolean constant having type \\(bool\\). </p> \\[ \\begin{array}{rc} {\\tt (tOp1)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in \\{ +, -, * \\}                 \\\\ \\hline                 \\Gamma \\vdash E_1\\ OP\\ E_2 : int                 \\end{array} \\\\ \\\\  {\\tt (tOp2)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E_1:int \\ \\ \\ \\Gamma \\vdash E_2:int\\ \\ \\ OP \\in \\{ ==, &lt;\\}                 \\\\ \\hline                 \\Gamma \\vdash E_1\\ OP E_2 : bool                 \\end{array} \\\\ \\\\ {\\tt (tOp3)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E_1:bool \\ \\ \\ \\Gamma \\vdash E_2:bool\\ \\ \\ OP \\in \\{ ==, &lt;\\}                 \\\\ \\hline                 \\Gamma \\vdash E_1\\ OP\\ E_2 : bool                 \\end{array}  \\end{array} \\] <p>In the rule \\({\\tt (tOp1)}\\), we type check an integer arithmetic operation having type \\(int\\), if both operands can be type-checked against \\(int\\). In the rule \\({\\tt (tOp2)}\\), we type check an integer comparison operation having type \\(bool\\), if both operands can be type-checked against \\(int\\). In the rule \\({\\tt (tOp3)}\\), we type check a boolean comparison operation having type \\(bool\\), if both operands can be type-checked against \\(bool\\).</p> \\[ \\begin{array}{rc} {\\tt (tParen)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E :T                 \\\\ \\hline                 \\Gamma \\vdash (E) :T                 \\end{array} \\end{array} \\] <p>Lastly in rule \\({\\tt (tParen)}\\), we type check a parenthesized expression by type-checking the inner expression. </p>"},{"location":"static_semantics/#type-checking-rules-for-simp-statements","title":"Type Checking rules for SIMP Statements","text":"<p>The typing rules for statement is in form of \\(\\Gamma \\vdash \\overline{S}\\) instead of  \\(\\Gamma \\vdash \\overline{S} : T\\), this is because  statements do not return a value (except for return statement, which returns a value for the entire program.)</p> \\[ \\begin{array}{rc} {\\tt (tSeq)} &amp; \\begin{array}{c}                 \\Gamma \\vdash S \\ \\ \\  \\Gamma \\vdash \\overline{S}                 \\\\ \\hline                 \\Gamma \\vdash S \\overline{S}                \\end{array} \\end{array} \\] <p>The \\({\\tt (tSeq)}\\) rule type checks a non empty sequence of statement \\(S \\overline{S}\\) under the type environment \\(\\Gamma\\).  It is typeable (a proof exists) iff if \\(S\\) is typeable under \\(\\Gamma\\) and \\(\\overline{S}\\) is typeable under \\(\\Gamma\\).</p> \\[ \\begin{array}{rc} {\\tt (tAssign)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E : T \\ \\ \\  \\Gamma \\vdash X : T                 \\\\ \\hline                 \\Gamma \\vdash X = E                 \\end{array} \\end{array} \\] <p>The \\({\\tt (tAssign)}\\) rule type checks an assignment statement \\(X = E\\) under \\(\\Gamma\\). It is typeable if both \\(X\\) and \\(E\\) are typeable under \\(\\Gamma\\) respectively and their types agree.</p> \\[ \\begin{array}{rc}  {\\tt (tReturn)} &amp; \\begin{array}{c}                 \\Gamma \\vdash X : T                 \\\\ \\hline                 \\Gamma \\vdash return\\ X                 \\end{array} \\\\ \\\\  {\\tt (tNop)} &amp; \\Gamma \\vdash nop  \\end{array} \\] <p>The \\({\\tt (tReturn)}\\) rule type checks the return statement. It is typeable, if the variable \\(X\\) is typeable. The \\({\\tt (tNop)}\\) rule type checks the nop statement, which is always typeable.</p> \\[ \\begin{array}{rc}  {\\tt (tIf)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S_1} \\ \\ \\ \\Gamma \\vdash \\overline{S_2}                 \\\\ \\hline                 \\Gamma \\vdash if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}                 \\end{array} \\\\ \\\\  {\\tt (tWhile)} &amp; \\begin{array}{c}                 \\Gamma \\vdash E:bool \\ \\ \\ \\Gamma \\vdash \\overline{S}                 \\\\ \\hline                 \\Gamma \\vdash while\\ E\\ \\{\\overline{S}\\}                 \\end{array}  \\end{array} \\] <p>The \\({\\tt (tIf)}\\) rule type checks the if-else statement, \\(if\\ E\\ \\{\\overline{S_1}\\}\\ else\\ \\{ \\overline{S_2} \\}\\).  It is typeable if \\(E\\) has type \\(bool\\) under \\(\\Gamma\\) and both then- and else- branches are typeable under the \\(\\Gamma\\). The \\({\\tt (tWhile)}\\) rule type checks the while statement in a similar way.</p> <p>We say that a SIMP program \\(\\overline{S}\\) is typeable under \\(\\Gamma\\), i.e. it type checks with \\(\\Gamma\\) iff \\(\\Gamma \\vdash \\overline{S}\\). On the other hand, we say that a SIMP program \\(\\overline{S}\\) is not typeable, i.e. it does not type check, iff there exists no \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\). </p> <p>Let \\(\\Gamma = \\{ (input, int), (x,int), (s,int) \\}\\), we consider the type checking derivation of </p> \\[x = input; s = 0; while\\ s&lt;x\\ \\{ s = s + 1;\\}\\ return\\ s;\\] <pre><code>                              \u0393 |- s:int (tVar)\n                              \u0393 |- 0:int (tInt) \n\u0393 |- input:int (tVar)         -----------------(tAssign)   [sub tree 1]\n\u0393 |- x:int (tVar)             \u0393 |- s=0  \n------------------(tAssign)   --------------------------------------(tSeq)\n\u0393 |- x=input;                 \u0393 |- s=0; while s&lt;x { s = s + 1;} return s; \n---------------------------------------------------------------------(tSeq)\n\u0393 |- x=input; s=0; while s&lt;x { s = s + 1;} return s;\n</code></pre> <p>Where [sub tree 1] is</p> <pre><code>                                          \u0393 |- 0:int (tInt)\n                                          \u0393 |- s:int (tVar)\n\u0393 |- s:int (tVar)                         -----------------(tOp1)\n\u0393 |- x:int (tVar)      \u0393 |-s:int (tVar)   \u0393 |- s+1:int \n--------------(tOp2)   -------------------------------(tAssign)\n\u0393 |- s&lt;x:bool          \u0393 |- s = s + 1                   \u0393 |- s:int (tVar)\n---------------------------------------------(tWhile)  ---------------(tReturn)\n\u0393 |- while s&lt;x { s = s + 1;}                            \u0393 |- return s\n--------------------------------------------------------------------(tSeq)\n\u0393 |- while s&lt;x { s = s + 1;} return s; \n</code></pre> <p>Note that the following two programs are not typeable.</p> <p><pre><code>// untypeable 1\nx = 1;\ny = 0;\nif x {\n    y = 0; \n} else {\n    y = 1;\n}\nreturn y;\n</code></pre> The above is untypeable because we use x of type <code>int</code> in a context where it is also expected as <code>bool</code>.</p> <pre><code>// untypeable 2\nx = input;\nif (x &gt; 1) {\n    y = true;\n} else {\n    y = 0;\n}\nreturn y;\n</code></pre> <p>The above is unteable because we can't find a type environment which has both <code>(y,int)</code> and <code>(y,bool)</code>.</p> <p>So far these two \"counter\" examples are bad programs. However we also note that our type system is too conservative.</p> <pre><code>// untypeable 3\nx = input;\nif (x &gt; 1) {\n    if ( x * x * x &lt; x * x) {\n        y = true;\n    } else {\n        y = 1;\n    }\n} else {\n    y = 0;\n}\nreturn y;\n</code></pre> <p>Even though we note that when <code>x &gt; 1</code>, we have <code>x * x * x &lt; x * x == false</code> hence the statement <code>y = true</code> is not executed. Our type system still rejects this program. We will discuss this issue in details in the upcoming units.</p> <p>Let's connect the type-checking rules for SIMP with it dynamic semantics.</p>"},{"location":"static_semantics/#definition-1-type-and-value-environments-consistency","title":"Definition 1 - Type and Value Environments Consistency","text":"<p>We say \\(\\Gamma \\vdash \\Delta\\) iff for all \\((X,C) \\in \\Delta\\) we have \\((X,T) \\in \\Gamma\\) and \\(\\Gamma \\vdash C : T\\). </p> <p>It means the type environments and value environments are consistent.</p>"},{"location":"static_semantics/#property-2-progress","title":"Property 2 - Progress","text":"<p>The following property says that a well typed SIMP program must not be stuck until it reaches the return statement.</p> <p>Let \\(\\overline{S}\\) be a SIMP statement sequence. Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\overline{S}\\). Then \\(\\overline{S}\\) is either  1. a return statement, or  1. a sequence of statements, and there exist \\(\\Delta\\), \\(\\Delta'\\) and \\(\\overline{S'}\\) such that \\(\\Gamma \\vdash \\Delta\\) and \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\).</p>"},{"location":"static_semantics/#property-3-preservation","title":"Property 3 - Preservation","text":"<p>The following property says that the evaluation of a SIMP program does not change its typeability.</p> <p>Let \\(\\Delta\\), \\(\\Delta'\\) be value environments. Let \\(\\overline{S}\\) and \\(\\overline{S'}\\) be SIMP statement sequences such that \\((\\Delta, \\overline{S}) \\longrightarrow (\\Delta', \\overline{S'})\\).  Let \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash \\Delta\\) and \\(\\Gamma \\vdash \\overline{S}\\). Then \\(\\Gamma \\vdash \\Delta'\\) and \\(\\Gamma \\vdash \\overline{S'}\\).</p>"},{"location":"static_semantics/#what-is-type-inference","title":"What is Type Inference","text":"<p>Type inference is also known as type reconstruction is a static semantics analysis process that aims to reconstruct the missing (or omitted) typing info from the source programs. </p> <p>For example, given the Scala function</p> <pre><code>def f(x:Int) = x + 1\n</code></pre> <p>the compiler is able to deduce that the type of <code>f</code> is <code>Int =&gt; Int</code>. </p> <p>Likewise for the following SIMP program</p> <p><pre><code>y = y + 1\n</code></pre> we can also deduce that <code>y</code> is of type <code>int</code>.</p> <p>What we aim to achieve is a sound and systematic process to deduce the omitted type information.</p>"},{"location":"static_semantics/#type-inference-for-simp-program","title":"Type inference for SIMP program","text":"<p>Given a SIMP program \\(\\overline{S}\\), the goal of type inference is to find the \"best\" type environment \\(\\Gamma\\) such that \\(\\Gamma \\vdash \\overline{S}\\).</p> <p>Given that \\(\\Gamma\\) is a set of variable to type mappings, the \"best\" can be defined as the smallest possible set that make \\(\\overline{S}\\) typeable. This is also called the most general solution.</p>"},{"location":"static_semantics/#definition-most-general-type-environment","title":"Definition - Most general type (environment)","text":"<p>Let \\(\\Gamma\\) be type environment and \\(\\overline{S}\\) be a sequence of SIMP statements, such that \\(\\Gamma \\vdash \\overline{S}\\). \\(\\Gamma\\) is the most general type environment iff for all \\(\\Gamma'\\) such that \\(\\Gamma' \\vdash \\overline{S}\\) we have \\(\\Gamma \\subseteq \\Gamma'\\).</p>"},{"location":"static_semantics/#type-inference-rules","title":"Type Inference Rules","text":"<p>We would like to design type inference process using a deduction system. First of all, let's introduce some extra meta syntax terms that serve as intermediate data structures.</p> \\[ \\begin{array}{rccl} {\\tt (Extended\\ Types)} &amp; \\hat{T} &amp; ::=  &amp;\\alpha \\mid T \\\\  {\\tt (Constraints)} &amp; \\kappa &amp; \\subseteq &amp; (\\hat{T} \\times \\hat{T}) \\\\  {\\tt (Type\\ Substitution)} &amp; \\Psi &amp; ::= &amp; [\\hat{T}/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi  \\end{array} \\] <p>Where \\(\\alpha\\) denotes a type variable. \\(\\kappa\\) define a set of pairs of extended types that are supposed to be equal, e.g. \\(\\{ (\\alpha, \\beta), (\\beta, int) \\}\\) means \\(\\alpha = \\beta \\wedge \\beta = int\\).</p> <p>Type substititution replace type variable to some other type. </p> \\[ \\begin{array}{rcll} \\lbrack\\rbrack\\hat{T} &amp; = &amp; \\hat{T} \\\\  \\lbrack\\hat{T}/\\alpha\\rbrack\\alpha &amp; = &amp; \\hat{T}  \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack\\beta &amp; = &amp; \\beta &amp; if\\ \\alpha \\neq \\beta \\\\ \\lbrack\\hat{T}/\\alpha\\rbrack T &amp; = &amp; T  \\end{array} \\] <p>Type substiution can be compositional.</p> \\[ \\begin{array}{rcll}  (\\Psi_1 \\circ \\Psi_2) \\hat{T} &amp; = &amp; \\Psi_1(\\Psi_2(\\hat{T})) \\end{array} \\] <p>The SIMP type inference rules are defined in terms of a deduction system consists of two type of rule forms. </p>"},{"location":"static_semantics/#type-inference-rules-for-simp-statements","title":"Type Inference Rules for SIMP statements","text":"<p>The type inference rules for SIMP statements are described in a form of \\(\\overline{S} \\vDash \\kappa\\), which reads give a sequence of statements \\(\\overline{S}\\), we generate a set of type constraints \\(\\kappa\\). </p> \\[ \\begin{array}{rc} {\\tt (tiNOP)} &amp; nop\\vDash \\{\\} \\\\ \\\\  {\\tt (tiReturn)} &amp; return\\ X \\vDash \\{\\}   \\end{array} \\] <p>The \\({\\tt (tiNOP)}\\) rule handles the \\(nop\\) statement, an empty constraint set is returned.  Similar observation applies to the return statement. </p> \\[ \\begin{array}{rc} {\\tt (tiSeq)} &amp; \\begin{array}{c}                  S \\vDash \\kappa_1 \\ \\ \\ \\ \\overline{S} \\vDash \\kappa_2                 \\\\ \\hline                 S \\overline{S} \\vDash \\kappa_1 \\cup \\kappa_2                  \\end{array}  \\end{array} \\] <p>The \\({\\tt (tiSeq)}\\) rule generates the type constraints of a sequence statement \\(S\\overline{S}\\). We can do so by first generate the constraints \\(\\kappa_1\\) from \\(S\\) and \\(\\kappa_2\\) from \\(\\overline{S}\\) and union \\(\\kappa_1\\) and \\(\\kappa_2\\).  </p> \\[ \\begin{array}{rc} {\\tt (tiAssign)} &amp;  \\begin{array}{c}                     E \\vDash \\hat{T}, \\kappa                      \\\\ \\hline                     X = E \\vDash \\{ (\\alpha_X, \\hat{T}) \\} \\cup \\kappa                      \\end{array} \\\\ \\\\ \\end{array} \\] <p>The inference rule for assignment statement requires the premise \\(E \\vDash \\hat{T}, \\kappa\\), the inference for the expression \\(E\\) returning the type of \\(E\\) and a constraint set \\(\\kappa\\), which will be discussed shortly. The \\({\\tt (tiAssign)}\\) rule \"calls\" the expression inference rule to generate the type \\(\\hat{T}\\) and the constraints \\(\\kappa\\), it prepends an entry \\((\\alpha_X,\\hat{T})\\) to \\(\\kappa\\) to ensure that \\(X\\)'s type and the type of the assignment's RHS must agree. </p> \\[ \\begin{array}{rc} {\\tt (tiIf)} &amp; \\begin{array}{c}                 E \\vDash \\hat{T_1},\\kappa_1 \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2 \\ \\ \\ \\ \\overline{S_3} \\vDash \\kappa_3                 \\\\ \\hline                 if\\ E\\ \\{\\overline{S_2}\\}\\ else \\{\\overline{S_3}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2 \\cup \\kappa_3                 \\end{array} \\\\ \\\\  \\end{array} \\] <p>The inference rule for if-else statatement first infers the type of the conditional expression \\(E\\)'s type has \\(\\hat{T_1}\\) and the constraints \\(\\kappa_1\\). \\(\\kappa_2\\) and \\(\\kappa_3\\) are the constraints inferred from the then- and else-branches. The final result is forming a union of \\(\\kappa_1\\), \\(\\kappa_2\\) and \\(\\kappa_3\\), in addition, requiring \\(E\\)'s type must be \\(bool\\). </p> \\[ \\begin{array}{rc} {\\tt (tiWhile)} &amp; \\begin{array}{c}                     E \\vDash \\hat{T_1}, \\kappa_1 \\ \\ \\ \\ \\overline{S_2} \\vDash \\kappa_2                     \\\\ \\hline                     while\\ E\\ \\{\\overline{S_2}\\} \\vDash \\{(\\hat{T_1}, bool)\\} \\cup \\kappa_1 \\cup \\kappa_2                   \\end{array}  \\end{array} \\] <p>The inference for while statement is very similar to if-else statement. We skip the explanation. </p>"},{"location":"static_semantics/#type-inference-rules-for-simp-expressions","title":"Type Inference Rules for SIMP expressions","text":"<p>The type inference rules for the SIMP expressions are defined in a form of \\(E \\vDash \\hat{T}, \\kappa\\). </p> \\[ \\begin{array}{rc}  {\\tt (tiInt)} &amp; \\begin{array}{c}                 C\\ {\\tt is\\ an\\ integer}                 \\\\ \\hline                 C \\vDash int, \\{\\}                 \\end{array} \\\\ \\\\  {\\tt (tiBool)} &amp; \\begin{array}{c}                 C\\ \\in \\{true, false\\}                 \\\\ \\hline                 C \\vDash bool, \\{\\}                 \\end{array}  \\end{array} \\] <p>When the expression is an integer constant, we return \\(int\\) as the inferred type and an empty constraint set. Likewise for boolean constant, we return \\(bool\\) and \\(\\{\\}\\). </p> \\[ \\begin{array}{rc} {\\tt (tiVar)} &amp; X \\vDash \\alpha_X, \\{\\}  \\end{array} \\] <p>The \\({\\tt (tiVar)}\\) rule just generates a \"skolem\" type variable \\(\\alpha_X\\) which is specifically \"reserved\" for variable \\(X\\). A skolem type variable is a type variable that is free in the current context but it has a specific \"purpose\".</p> <p>For detailed explanation of skolem variable, refer to https://stackoverflow.com/questions/12719435/what-are-skolems and https://en.wikipedia.org/wiki/Skolem_normal_form.</p> \\[ \\begin{array}{rc} {\\tt (tiOp1)} &amp; \\begin{array}{c}                 OP \\in \\{+, -, *\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2                 \\\\ \\hline                 E_1\\ OP\\ E_2 \\vDash int, \\{(\\hat{T_1}, int), (\\hat{T_2}, int)\\} \\cup \\kappa_1 \\cup \\kappa_2                 \\end{array} \\\\ \\\\  {\\tt (tiOp2)} &amp; \\begin{array}{c}                 OP \\in \\{&lt;, ==\\} \\ \\ \\ E_1 \\vDash \\hat{T_1}, \\kappa_1\\ \\ \\ \\ E_2 \\vDash \\hat{T_2}, \\kappa_2                 \\\\ \\hline                 E_1\\ OP\\ E_2 \\vDash bool, \\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa_1 \\cup \\kappa_2                 \\end{array} \\end{array} \\] <p>The rules \\({\\tt (tiOp1)}\\) and \\({\\tt (tiOp2)}\\) infer the type of binary operation expressions. Note that they can be broken into 6 different rules to be syntax-directed. \\({\\tt (tiOp1)}\\) is applied when the operator is an arithmethic operation, the returned type is \\(int\\) and the inferred constraint set is the union of the constraints inferred from the operands plus the entries of enforcing both \\(\\hat{T_1}\\) and \\(\\hat{T_2}\\) are \\(int\\). \\({\\tt (tiOp2)}\\) supports the case where the operator is a boolean comparison. </p> \\[ \\begin{array}{rc} {\\tt (tiParen)} &amp; \\begin{array}{c}                   E \\vDash \\hat{T}, \\kappa                   \\\\ \\hline                   (E) \\vDash \\hat{T}, \\kappa                   \\end{array} \\end{array} \\] <p>The inference ruel for parenthesis expression is trivial, we infer the type from the inner expression.</p>"},{"location":"static_semantics/#unification","title":"Unification","text":"<p>To solve the set of generated type constraints from the above inference rules, we need to use a unification algorithm. </p> \\[ \\begin{array}{rcl} mgu(int, int) &amp; = &amp; [] \\\\  mgu(bool, bool) &amp; = &amp; [] \\\\  mgu(\\alpha, \\hat{T}) &amp; = &amp; [\\hat{T}/\\alpha] \\\\  mgu(\\hat{T}, \\alpha) &amp; = &amp; [\\hat{T}/\\alpha] \\\\ \\end{array} \\] <p>The \\(mgu(\\cdot, \\cdot)\\) function generates a type substitution that unifies the two arguments. \\(mgu\\) is a short hand for most general unifier. Note that \\(mgu\\) function is a partial function, cases that are not mentioned in the above will result in a unification failure. </p> <p>At the moment \\(mgu\\) only unifies two extended types. We overload \\(mgu()\\) to apply to a set of constraints as follows</p> \\[ \\begin{array}{rcl} mgu(\\{\\}) &amp; = &amp; [] \\\\  mgu(\\{(\\hat{T_1}, \\hat{T_2})\\} \\cup \\kappa ) &amp; = &amp; let\\ \\Psi_1 = mgu(\\hat{T_1}, \\hat{T_2}) \\\\  &amp; &amp; \\ \\ \\ \\ \\ \\ \\kappa'  = \\Psi_1(\\kappa) \\\\  &amp; &amp; \\ \\ \\ \\ \\ \\ \\Psi_2   = mgu(\\kappa') \\\\  &amp; &amp; in\\  \\Psi_2 \\circ \\Psi_1   \\end{array} \\] <p>There are two cases.</p> <ol> <li>the constraint set is empty, we return the empty (identity) substitution.</li> <li>the constriant set is non-empty, we apply the first version of \\(mgu\\) to unify one entry \\((\\hat{T_1}, \\hat{T_2})\\), which yields a subsitution \\(\\Psi_1\\). We apply \\(\\Psi_1\\) to the rest of the constraints \\(\\kappa\\) to obtain \\(\\kappa'\\). Next we apply \\(mgu\\) to \\(\\kappa'\\) recursively to generate another type substitution \\(\\Psi_2\\). The final result is a composition of \\(\\Psi_2\\) with \\(\\Psi_1\\). </li> </ol> <p>Note that the choice of the particular entry \\((\\hat{T_1}, \\hat{T_2})\\) does not matter, the algorithm will always produce the same result when we apply the final subsitution to all the skolem type variable \\(\\alpha_X\\). We see that in an example shortly. </p>"},{"location":"static_semantics/#an-example","title":"An Example","text":"<p>Consider the following SIMP program</p> <pre><code>x = input;          // (\u03b1_x, \u03b1_input)      \ny = 0;              // (\u03b1_y, int)\nwhile (y &lt; x) {     // (\u03b1_y, \u03b1_x)\n    y = y + 1;      // (\u03b1_y, int)\n}\n</code></pre> <p>For the ease of access we put the inferred constraint entry as comments next to the statements. The detail derivation of the inference algorithm is as follows</p> <pre><code>input|=\u03b1_input,{} (tiVar)\n-------------------------(tiAssign)    [subtree 1]\nx=input|={(\u03b1_x,\u03b1_input)}   \n-----------------------------------------------------------------------------(tiSeq)\nx=input; y=0; while (y&lt;x) { y=y+1; } return y; |= {(\u03b1_x,\u03b1_input),(a_y,int),(\u03b1_y,\u03b1_x)} \n</code></pre> <p>Where [subtree 1] is as follows</p> <pre><code>y|=\u03b1_y,{} (tiVar)\n0|=int,{} (tiInt)\n------------------(tiAssign)   [subtree 2]\ny=0|={(\u03b1_y,int)}\n--------------------------------------------------------(tiSeq)\ny=0; while (y&lt;x) { y=y+1; } return y; |= {(a_y,int),(\u03b1_y,\u03b1_x)} \n</code></pre> <p>Where [subtree 2] is as follows</p> <pre><code>                        y|=\u03b1_y,{} (tiVar)\n                        1|=int,{} (tiInt)\ny|=\u03b1_y,{} (tiVar)       --------------(tiOp1)\nx|=\u03b1_x,{} (tiVar)       y+1|=int,{(\u03b1_y,int)}\n--------------(tiOp2)  ----------------------(tiAssign)\ny&lt;x|=bool,{(\u03b1_y,\u03b1_x)}  y=y+1|= {(\u03b1_y,int)} \n---------------------------------------------(tiWhile) --------------(tiReturn)\nwhile (y&lt;x) { y=y+1; } |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)}        return y|= {}\n---------------------------------------------------------------------(tiSeq)\nwhile (y&lt;x) { y=y+1; } return y; |= {(\u03b1_y,\u03b1_x),(\u03b1_y,int)} \n</code></pre>"},{"location":"static_semantics/#from-type-substitution-to-type-environment","title":"From Type Substitution to Type Environment","text":"<p>To derive the inferred type environment, we apply the type substitution to all the type variabales we created. </p> <p>Let \\(V(\\overline{S})\\) denote all the variables used in a SIMP program \\(\\overline{S}\\).</p> <p>Given a type substitution \\(\\Psi\\) obtained from the unification step, the type environment \\(\\Gamma\\) can be computed as follows,</p> \\[ \\Gamma = \\{ (X, \\Psi(\\alpha_X)) | X \\in V(\\overline{S}) \\} \\] <p>Recall that the set of constraints generated from the running example is </p> \\[ \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}  \\]"},{"location":"static_semantics/#unification-from-left-to-right","title":"Unification from left to right","text":"<p>Suppose the unification progress pick the entries from left to right</p> \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_x,\\alpha_{input}}),(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = mgu(\\alpha_x,\\alpha_{input}) \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{x})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{input}/ \\alpha_x] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_{y},int),(\\alpha_{y},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 &amp; \\longrightarrow  \\end{array} \\] <p>Where derivation of \\(mgu(\\kappa_1)\\) </p> \\[ \\begin{array}{ll} mgu(\\{(\\underline{\\alpha_{y},int}),(\\alpha_{y},\\alpha_{input})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_{21} = mgu(\\alpha_{y},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{y},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{y},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{y}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\end{array} \\] <p>Hence the final result is </p> \\[  [int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x] \\] <p>We apply this type substitution to all the variables in the program.</p> \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{input} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} &amp; = \\\\   [int/\\alpha_{input}]  \\alpha_{input} &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{x} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}])\\alpha_{input} &amp; = \\\\   [int/\\alpha_{input}]  \\alpha_{input} &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}] \\circ [\\alpha_{input}/ \\alpha_x])\\alpha_{y} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{y}]) \\alpha_{y} &amp; = \\\\   [int/\\alpha_{input}] int &amp; = \\\\   int \\\\ \\\\  \\end{array} \\] <p>So we have computed the inferred type environment</p> \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\]"},{"location":"static_semantics/#unification-from-right-to-left","title":"Unification from right to left","text":"<p>Now let's consider a different of order of applying the \\(mgu\\) function to the constraint set. Instead of going from left to right, we solve the constraints from right to left. </p> \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int),(\\underline{\\alpha_{y},\\alpha_{x}})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = mgu(\\alpha_{y},\\alpha_{x}) \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\Psi_1\\{(\\alpha_x,\\alpha_{input}),(\\alpha_{y},int)\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1  &amp; \\longrightarrow \\\\  let\\ \\Psi_1 = [\\alpha_{x}/ \\alpha_y] \\\\  \\ \\ \\ \\ \\ \\ \\kappa_1 = \\{(\\alpha_x,\\alpha_{input}),(\\alpha_{x},int)\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_2 = mgu(\\kappa_1) \\\\ in\\ \\Psi_2 \\circ \\Psi_1 &amp; \\longrightarrow  \\end{array} \\] <p>Where derivation of \\(mgu(\\kappa_1)\\) </p> \\[ \\begin{array}{ll} mgu(\\{(\\alpha_x,\\alpha_{input}),(\\underline{\\alpha_{x},int})\\}) &amp; \\longrightarrow \\\\  let\\ \\Psi_{21} = mgu(\\alpha_{x},int) \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\Psi_{21}\\{(\\alpha_{x},\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = mgu(\\kappa_2) \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ let\\ \\Psi_{21} = [int/\\alpha_{x}] \\\\ \\ \\ \\ \\ \\ \\ \\kappa_2 = \\{(int,\\alpha_{input})\\} \\\\  \\ \\ \\ \\ \\ \\ \\Psi_{22} = [int/\\alpha_{input}] \\\\  in\\ \\Psi_{22} \\circ \\Psi_{21} &amp; \\longrightarrow \\\\ [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\end{array} \\] <p>Hence the final result is </p> \\[  [int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y] \\] <p>We apply this type substitution to all the variables in the program.</p> \\[ \\begin{array}{rl} ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{input} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{input} &amp; = \\\\   [int/\\alpha_{input}]  \\alpha_{input} &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{x} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}])\\alpha_{x} &amp; = \\\\   [int/\\alpha_{input}]  int &amp; = \\\\   int \\\\ \\\\  ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}] \\circ [\\alpha_{x}/ \\alpha_y])\\alpha_{y} &amp; = \\\\   ([int/\\alpha_{input}] \\circ [int/\\alpha_{x}]) \\alpha_{x} &amp; = \\\\   [int/\\alpha_{input}] int &amp; = \\\\   int \\\\ \\\\  \\end{array} \\] <p>So we have computed the inferred the same type environment</p> \\[ \\Gamma = \\{(input, int), (x, int), (y,int) \\} \\] <p>In face regardless the order of picking entries from the constraint sets, we compute the same \\(\\Gamma\\).  </p> <p>If you have time, you can try another order.</p>"},{"location":"static_semantics/#inputs-type","title":"Input's type","text":"<p>In our running example, our inference algorithm is able to infer the program's input type i.e. \\(\\alpha_{input}\\).</p> <p>This is not always possible. Let's consider the following program.</p> <pre><code>x = input;          // (\u03b1_x, \u03b1_input)      \ny = 0;              // (\u03b1_y, int)\nwhile (y &lt; 3) {     // (\u03b1_y, int)\n    y = y + 1;      // (\u03b1_y, int)\n}\n</code></pre> <p>In the genereated constraints, our algorithm can construct the subtitution </p> \\[[\\alpha_{input}/\\alpha_x] \\circ [int/\\alpha_y]\\] <p>Which fails to \"ground\" type variables \\(\\alpha_{input}\\) and \\(\\alpha_x\\). </p> <p>We may argue that this is an ill-defined program as <code>input</code> and <code>x</code> are not used in the rest of the program, which should be rejected if we employ some name analysis, (which we will learn in the upcoming lesson). Hence we simply reject this kind of programs. </p> <p>Alternatively, we can preset the type of the program, which is a common practice for many program languages. When generating the set of constraint \\(\\kappa\\), we manually add an entry \\((\\alpha_{input}, int)\\) assuming the input's type is expected to be \\(int\\). </p>"},{"location":"static_semantics/#uninitialized-variable","title":"Uninitialized Variable","text":"<p>There is another situatoin in which the inference algorithm fails to ground all the type variables.</p> <p><pre><code>x = z;              // (\u03b1_x, \u03b1_z)      \ny = 0;              // (\u03b1_y, int)\nwhile (y &lt; 3) {     // (\u03b1_y, int)\n    y = y + 1;      // (\u03b1_y, int)\n}\n</code></pre> in this case, we can't ground \\(\\alpha_x\\) and \\(\\alpha_z\\) as <code>z</code> is not initialized before use. In this case we argue that such a program should be rejected either by the type inference or the name analysis.</p>"},{"location":"static_semantics/#property-4-type-inference-soundness","title":"Property 4: Type Inference Soundness","text":"<p>The following property states that the type environment generated from a SIMP program by the type inference algorithm is able to type check the SIMP program.</p> <p>Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma \\vdash \\overline{S}\\). </p>"},{"location":"static_semantics/#property-5-principality","title":"Property 5: Principality","text":"<p>The following property states that the type environment generated from a SIMP program by the type inference algorithm is a principal type environment.</p> <p>Let \\(\\overline{S}\\) be a SIMP program and \\(\\Gamma\\) is a type environment inferred using the described inference algorithm. Then \\(\\Gamma\\) is the most general type environment that can type-check \\(\\overline{S}\\). </p>"},{"location":"static_semantics_2/","title":"50.054 Static Semantics for Lambda Calculus","text":""},{"location":"static_semantics_2/#learning-outcomes","title":"Learning Outcomes","text":"<ol> <li>Apply type checking algorithm to type check a simply typed lambda calculus expression.</li> <li>Apply Hindley Milner algorithm to type check lambda calculus expressions.</li> <li>Apply Algorithm W to infer type for lambda calculus.</li> </ol>"},{"location":"static_semantics_2/#type-checking-for-lambda-calculus","title":"Type Checking for Lambda Calculus","text":"<p>To illustrate the proocess of type checking for lambda calculus, we consider adding types and type annotations to the lambda calculus language. </p> <p>Recall the lambda calculus syntax, with the following adaptation</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x:T.t \\mid t\\ t \\mid let\\ x:T =\\ t\\ in\\ t \\mid  if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\  {\\tt (Builtin\\ Operators)} &amp; op &amp; ::= &amp; + \\mid - \\mid * \\mid / \\mid\\ == \\\\  {\\tt (Builtin\\ Constants)} &amp; c &amp; ::= &amp; 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\  {\\tt (Types)} &amp; T &amp; ::= &amp; int \\mid bool \\mid T \\rightarrow T \\\\   {\\tt (Type\\ Environments)} &amp; \\Gamma &amp; \\subseteq &amp; (x \\times T) \\end{array} \\] <p>The difference is that the lambda abstraction \\(\\lambda x:T.t\\) now carries a type annotation of the lambda-bound variable. (Similar observation applies to let-binding) \\(T\\) is a type symbol which can be \\(int\\) or \\(bool\\) or a function type \\(T \\rightarrow T\\). The \\(\\rightarrow\\) type operator is right associative, i.e. \\(T_1 \\rightarrow T_2 \\rightarrow T_3\\) is parsed as \\(T_1 \\rightarrow (T_2 \\rightarrow T_3)\\). Let's call this extended version of lambda calculus as Simply Typed Lambda Calculus.</p> <p>Note that all the existing definitions for dynamic semantics of lambda calculus can be brought-forward (and extended) to support Simply Typed Lambda Calculus. We omit the details.</p> <p>We formalize the type-checking process in a relation \\(\\Gamma \\vdash t : T\\), where \\(\\Gamma\\) is a mapping from variables to types. We write \\(dom(\\Gamma)\\) to denote the domain of \\(\\Gamma\\), i.e. \\(\\{ X \\mid (x,T) \\in \\Gamma \\}\\). We assume for all \\(x \\in dom(\\Gamma)\\), there exists only one entry of \\((x,T) \\in \\Gamma\\).</p> <p>Since \\(\\Gamma \\vdash t : T\\) is relation, what type-checking attempts to verify is the following. Given a type environment \\(\\Gamma\\) and lambda term \\(t\\) and a type \\(T\\), \\(t\\) can be given a type \\(T\\) under \\(\\Gamma\\).</p> \\[ \\begin{array}{cc} {\\tt (lctInt)} &amp; \\begin{array}{c} \\\\                       c\\ {\\tt is\\ an\\ integer}                       \\\\ \\hline                       \\Gamma \\vdash c : int                       \\end{array} \\\\ \\\\   {\\tt (lctBool)} &amp; \\begin{array}{c}                        c\\in \\{ true, false\\}                       \\\\ \\hline                       \\Gamma \\vdash c : bool                       \\end{array} \\end{array} \\] <p>The rule \\({\\tt (lctInt)}\\) checks whether the given constant value is an integer. The rule \\({\\tt (lctBool)}\\) checks whether the given constant value is a boolean.</p> \\[ \\begin{array}{cc} {\\tt (lctVar)} &amp; \\begin{array}{c}                 (x, T) \\in \\Gamma  \\\\                \\hline                \\Gamma \\vdash x : T                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctVar)}\\), we type check a variable \\(x\\) against a type \\(T\\), which is only valid where \\((x,T)\\) can be found in the type environment \\(\\Gamma\\).</p> \\[ \\begin{array}{cc} {\\tt (lctLam)} &amp; \\begin{array}{c}                \\Gamma \\oplus (x, T) \\vdash t : T'  \\\\                \\hline                \\Gamma \\vdash \\lambda x : T.t :T \\rightarrow T'                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctLam)}\\), we type check a lambda abstraction against a type \\(T\\rightarrow T'\\). This is only valid if the body of the lambda expression \\(t\\) has type \\(T'\\) under the extended type environment \\(\\Gamma \\oplus (x, T)\\).</p> \\[ \\begin{array}{cc} {\\tt (lctApp)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\                \\Gamma \\vdash t_2 : T_1 \\\\                \\hline                \\Gamma \\vdash  t_1\\ t_2 :T_2                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctApp)}\\), we type check a function application, applying \\(t_1\\) to \\(t_2\\), against a type \\(T_2\\). This is only valid if \\(t_1\\) is having type \\(T_1 \\rightarrow T_2\\) and \\(t_2\\) is having type \\(T_1\\).</p> \\[ \\begin{array}{cc} {\\tt (lctLet)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : T_1 \\ \\ \\ \\                \\Gamma \\oplus (x, T_1) \\vdash t_2 : T_2 \\\\                \\hline                \\Gamma \\vdash  let\\ x:T_1 = t_1\\ in\\ t_2 :T_2                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (lctLet)}\\), we type check a let binding, \\(let\\ x:T_1 = t_1\\ in\\ t_2\\) against type \\(T_2\\). This is only valid if \\(t_1\\) has type \\(T_1\\) and \\(t_2\\) has type \\(T_2\\) under the extended environment  \\(\\Gamma \\oplus (x, T_1)\\).</p> \\[ \\begin{array}{cc} {\\tt (lctIf)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : T \\ \\ \\ \\ \\Gamma \\vdash t_3 : T \\\\                \\hline                \\Gamma \\vdash  if\\ t_1\\ then\\ t_2\\ else\\ t_3 : T                 \\end{array} \\end{array} \\] <p>In rule \\({\\tt (lctIf)}\\), we type check a if-then-else expression against type \\(T\\). This is only valid if  \\(t_1\\) has type \\(bool\\) and both \\(t_1\\) and \\(t_2\\) have type \\(T\\).</p> \\[ \\begin{array}{cc} {\\tt (lctOp1)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\                \\hline                \\Gamma \\vdash  t_1\\ op\\ t_2 : int                 \\end{array} \\\\ \\\\  {\\tt (lctOp2)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\  {\\tt (lctOp3)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\  \\end{array} \\] <p>The above three rules type check the binary operations. \\({\\tt (lctOp1)}\\) handles the case where the \\(op\\) is an arithmatic operation, which requires both operands having type \\(int\\). \\({\\tt (lctOp2)}\\) and \\({\\tt (lctOp3)}\\) handle the case where \\(op\\) is the equality test. In this case, the types of the operands must agree.</p> \\[ \\begin{array}{cc} {\\tt (lctFix)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t : (T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2                 \\\\  \\hline                 \\Gamma \\vdash fix\\ t:T_1 \\rightarrow T_2                \\end{array}  \\end{array} \\] <p>The last rule \\({\\tt (lctFix)}\\) type checks the fix operator application against the type \\(T_1 \\rightarrow T_2\\). We enforce that the argument \\(t\\) must be a fixed point function of type \\((T_1 \\rightarrow T_2) \\rightarrow T_1 \\rightarrow T_2\\).</p> <p>For example, we would like to type check the following simply typed lambda term.</p> <p>$$ fix (\\lambda f:int\\rightarrow int.(\\lambda x:int. (if x == 0 then 1 else (f (x-1))* x))) $$ against the type \\(int \\rightarrow int\\)</p> <p>We added the optional parantheses for readability. </p> <p>We find the the following type checking derivation (proof tree).</p> <p>Let <code>\u0393</code> be the initial type environment.</p> <pre><code>\u0393\u2295(f:int-&gt;int)\u2295(x:int)|- x:int (lctVar)\n\u0393\u2295(f:int-&gt;int)\u2295(x:int)|- 0:int (lctInt)\n---------------------------------------(lctOp2)  [sub tree 1]   [sub tree 2]\n\u0393\u2295(f:int-&gt;int)\u2295(x:int)|- x == 0: bool\n------------------------------------------------------------------------------- (lctIf)\n\u0393\u2295(f:int-&gt;int)\u2295(x:int)|-if x == 0 then 1 else (f (x-1))*x:int\n--------------------------------------------------------------------(lctLam)\n\u0393\u2295(f:int-&gt;int)|-\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x):int-&gt;int\n--------------------------------------------------------------------------------(lctLam)\n\u0393 |- \u03bbf:int-&gt;int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x)):(int-&gt;int)-&gt;int-&gt;int\n---------------------------------------------------------------------------------(lctFix)\n\u0393 |- fix (\u03bbf:int-&gt;int.(\u03bbx:int.(if x == 0 then 1 else (f (x-1))*x))):int-&gt;int\n</code></pre> <p>Let <code>\u03931=\u0393\u2295(f:int-&gt;int)\u2295(x:int)</code> Where [sub tree 1] is </p> <pre><code>\u03931|- 1:int (lctInt)\n</code></pre> <p>and [sub tree 2] is  <pre><code>                           \u03931|-x:int (lctVar) \n                           \u03931|-1:int (lctInt)\n                           -----------------(lctOp1)\n\u03931|- f:int-&gt;int (lctVar)   \u03931|- x-1:int \n-------------------------------------------------(lctApp)  \n\u03931|- f (x-1):int                                           \u03931 |- x:int (lctVar)\n-------------------------------------------------------------------------(lctOp1)\n\u03931|- (f (x-1))*x:int\n</code></pre></p> <p>Another (counter) example which shows that we can't type check the following program </p> \\[ let\\ x:int = 1\\ in\\ (if\\ x\\ then\\ x\\ else\\ 0) \\] <p>against the type \\(int\\).</p> <pre><code>                   fail, no proof exists\n                   ---------------------- \n                   \u0393\u2295(x:int)|- x:bool\n                   ----------------------------------(lctIf)\n\u0393|-1:int (lctInt)  \u0393\u2295(x:int)|-if x then x else 0:int\n--------------------------------------------------------(lctLet)\n\u0393|- let x:int = 1 in (if x then x else 0):int\n</code></pre>"},{"location":"static_semantics_2/#property-1-uniqueness","title":"Property 1 - Uniqueness","text":"<p>The following property states that if a lambda term is typable, its type must be unique.</p> <p>Let \\(t\\) be a simply typed lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\), \\(x \\in dom(\\Gamma)\\). Let \\(T\\) and \\(T'\\) be types such that \\(\\Gamma \\vdash t : T\\) and \\(\\Gamma \\vdash t:T'\\). Then \\(T\\) and \\(T'\\) must be the same.</p> <p>Where \\(dom(\\Gamma)\\) refers to the domain of \\(\\Gamma\\), i.e. all the variables being mapped.</p>"},{"location":"static_semantics_2/#property-2-progress","title":"Property 2 - Progress","text":"<p>The second property states that if a closed lambda term is typeable under the empty type environment, it must be runnable and not getting stuck.</p> <p>Let \\(t\\) be a simply typed lambda calculus term such that \\(fv(t) = \\{\\}\\).  Let \\(T\\) be a type such that \\(\\{\\} \\vdash t : T\\). Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\).</p>"},{"location":"static_semantics_2/#property-3-preservation","title":"Property 3 - Preservation","text":"<p>The third property states that the type of a lambda term does not change over evaluation.</p> <p>Let \\(t\\) and \\(t'\\) be simply typed lambda calculus terms such that \\(t \\longrightarrow t'\\). Let \\(T\\) be a type and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:T\\). Then \\(\\Gamma \\vdash t':T\\).</p>"},{"location":"static_semantics_2/#issue-with-let-binding","title":"Issue with let-binding","text":"<p>The current type checking rules for Simply-typed Lambda Calculus fails to type check the following lambda calculus term.</p> \\[ \\begin{array}{l} let\\ f = \\lambda x:\\alpha.x \\\\ in\\ let\\ g = \\lambda x:int.\\lambda y:bool.x \\\\  \\ \\ \\ \\ in\\ (g\\ (f\\ 1)\\ (f\\ true)) \\end{array} \\] <p>Where \\(\\alpha\\) denotes some generic type. This is due to the fact that we can only give one type to <code>f</code>, either \\(Int \\rightarrow Int\\) or \\(Bool \\rightarrow Bool\\) but not both.</p> <p>To type check the above program we need to get rid of the type annotations to the let binding (as well as lambda abstraction). This leads us to the Hindley-Milner Type System.</p>"},{"location":"static_semantics_2/#hindley-milner-type-system","title":"Hindley Milner Type System","text":"<p>We define the lambda calculus syntax for Hindley Milner Type System as follows</p> \\[ \\begin{array}{rccl}  {\\tt (Lambda\\ Terms)} &amp; t &amp; ::= &amp; x \\mid \\lambda x.t \\mid t\\ t \\mid let\\ x =\\ t\\ in\\ t \\mid  if\\ t\\ then\\ t\\ else\\ t \\mid t\\ op\\ t \\mid c \\mid fix\\ t \\\\  {\\tt (Builtin\\ Operators)} &amp; op &amp; ::= &amp; + \\mid - \\mid * \\mid / \\mid\\ == \\\\  {\\tt (Builtin\\ Constants)} &amp; c &amp; ::= &amp; 0 \\mid 1 \\mid ... \\mid true \\mid false \\\\  {\\tt (Types)} &amp; T &amp; ::= &amp; int \\mid bool \\mid T \\rightarrow T \\mid \\alpha \\\\   {\\tt (Type Scheme)} &amp; \\sigma &amp; ::= &amp; \\forall \\alpha. \\sigma \\mid T \\\\   {\\tt (Type\\ Environments)} &amp; \\Gamma &amp; \\subseteq &amp; (x \\times \\sigma ) \\\\  {\\tt (Type\\ Substitution)} &amp; \\Psi &amp; ::= &amp; [T/\\alpha] \\mid [] \\mid \\Psi \\circ \\Psi  \\end{array} \\] <p>In the above grammar rules, we remove the type annotations from the lambda abstraction and let binding. Our type inference algorithm should be able to recover them.  We add the type variable directly to the type \\(T\\) rule instead of introducing the \\(\\hat{T}\\) rule for conciseness. We introduce a type scheme term \\(\\sigma\\) which is required for polymorphic types.  </p> <p>We describe the Hindley Milner Type Checking rules as follows</p> \\[ \\begin{array}{rc} {\\tt (hmInt)} &amp; \\begin{array}{c} \\\\                       c\\ {\\tt is\\ an\\ integer}                       \\\\ \\hline                       \\Gamma \\vdash c : int                       \\end{array} \\\\ \\\\  {\\tt (hmBool)} &amp; \\begin{array}{c}                        c\\in \\{ true, false\\}                       \\\\ \\hline                       \\Gamma \\vdash c : bool                       \\end{array} \\end{array} \\] <p>The rules for constants remain unchanged. </p> \\[ \\begin{array}{rc} {\\tt (hmVar)} &amp; \\begin{array}{c}                 (x,\\sigma) \\in \\Gamma                 \\\\ \\hline                 \\Gamma \\vdash x : \\sigma                 \\end{array}  \\end{array} \\] <p>The rule for variable is adjusted to use type signatures instead of types.</p> \\[ \\begin{array}{rc} {\\tt (hmLam)} &amp; \\begin{array}{c}                \\Gamma \\oplus (x, T) \\vdash t : T'  \\\\                \\hline                \\Gamma \\vdash \\lambda x.t :T\\rightarrow T'                 \\end{array} \\\\ \\\\  {\\tt (hmApp)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : T_1 \\rightarrow T_2 \\ \\ \\ \\                \\Gamma \\vdash t_2 : T_1 \\\\                \\hline                \\Gamma \\vdash  t_1\\ t_2 :T_2                 \\end{array}  \\end{array} \\] <p>In rule \\({\\tt (hmLam)}\\) we type check the lambda abstraction against \\(T\\rightarrow T'\\). It is largely the same as the \\({\\tt (lctLam)}\\) rule for simply typed lambda calculus, except that there is no type annotation to the lambda bound variable \\(x\\). The rule \\({\\tt (hmApp)}\\) is exactly the same as \\({\\tt (lctApp)}\\). </p> \\[ \\begin{array}{rc} {\\tt (hmFix)} &amp; \\begin{array}{c}                 (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha)\\in \\Gamma                 \\\\ \\hline                  \\Gamma \\vdash fix:\\forall \\alpha. (\\alpha\\rightarrow \\alpha) \\rightarrow \\alpha                 \\end{array} \\end{array} \\] <p>To type check the \\(fix\\) operator, we assume that \\(fix\\) is predefined in the language library and its type is given in the initial type environment \\(\\Gamma_{init}\\).</p> \\[ \\begin{array}{rc} {\\tt (hmIf)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t_1 : bool \\ \\ \\                  \\Gamma \\vdash t_2 : \\sigma \\ \\ \\                  \\Gamma \\vdash t_3 : \\sigma                  \\\\ \\hline                 \\Gamma \\vdash if\\ t_1\\ \\{ t_2\\}\\ else \\{ t_3 \\}: \\sigma                \\end{array} \\\\ \\\\  \\end{array} \\] <p>We made minor adjustment to the rule handling if-else expression, by replacing \\(T\\) with \\(\\sigma\\).</p> \\[ \\begin{array}{rc} {\\tt (hmOp1)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\ \\ \\ op\\in\\{+,-,*,/\\} \\\\                \\hline                \\Gamma \\vdash  t_1\\ op\\ t_2 : int                 \\end{array} \\\\ \\\\  {\\tt (hmOp2)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : int \\ \\ \\ \\Gamma \\vdash t_2 : int \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\  {\\tt (hmOp3)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : bool \\ \\ \\ \\Gamma \\vdash t_2 : bool \\\\                \\hline                \\Gamma \\vdash  t_1\\ ==\\ t_2 : bool                 \\end{array} \\\\ \\\\   \\end{array} \\] <p>The type checking rules for binary operation remain unchanged.</p> \\[ \\begin{array}{rc} {\\tt (hmLet)} &amp; \\begin{array}{c}                \\Gamma \\vdash t_1 : \\sigma_1 \\ \\ \\ \\                \\Gamma \\oplus (x, \\sigma_1) \\vdash t_2 : T_2 \\\\                \\hline                \\Gamma \\vdash  let\\ x = t_1\\ in\\ t_2 :T_2                 \\end{array} \\\\ \\\\  {\\tt (hmInst)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t : \\sigma_1 \\ \\ \\ \\ \\sigma_1 \\sqsubseteq \\sigma_2                 \\\\ \\hline                 \\Gamma \\vdash t : \\sigma_2                 \\end{array} \\\\ \\\\  {\\tt (hmGen)} &amp; \\begin{array}{c}                 \\Gamma \\vdash t : \\sigma \\ \\ \\ \\ \\alpha \\not\\in ftv(\\Gamma)                 \\\\ \\hline                  \\Gamma \\vdash t : \\forall \\alpha.\\sigma                 \\end{array} \\end{array} \\] <p>In the rule \\({\\tt (hmLet)}\\), we first type check \\(t_1\\) againt \\(\\sigma_1\\), which is a type scheme, which allows \\(t_1\\) to have a generic type. Under the extended type environment \\(\\Gamma \\oplus (x, \\sigma_1)\\) we type-check \\(t_2\\). </p> <p>For the \\({\\tt (hmLet)}\\) rule to work as intended, we need two more rules, namely, \\({\\tt (hmInst)}\\) and \\({\\tt (hmGen)}\\). In rule \\({\\tt (hmInst)}\\) we allow a term \\(t\\) to be type-checked against \\(\\sigma_2\\), provided we can type check it against \\(\\sigma_1\\) and \\(\\sigma_1 \\sqsubseteq \\sigma_2\\).</p>"},{"location":"static_semantics_2/#definition-type-instances","title":"Definition - Type Instances","text":"<p>Let \\(\\sigma_1\\) and \\(\\sigma_2\\) be type schemes. We say \\(\\sigma_1 \\sqsubseteq \\sigma_2\\) iff \\(\\sigma_1 = \\forall \\alpha. \\sigma_1'\\) and there exists a type subsitution \\(\\Psi\\) such that \\(\\Psi(\\sigma_1') = \\sigma_2\\).</p> <p>In otherwords, we say \\(\\sigma_1\\) is more general that \\(\\sigma_2\\) and \\(\\sigma_2\\) is a type instance of \\(\\sigma_1\\).</p> <p>Finally the rule \\({\\tt (hmGen)}\\) generalizes existing type to type schemes. In this rule, if a term \\(t\\) can be type-checked against a type scheme \\(\\sigma\\), then \\(t\\) can also be type-checked against \\(\\forall \\alpha.\\sigma\\) if \\(\\alpha\\) is not a free type variable in \\(\\Gamma\\).</p> <p>The type variable function \\(ftv()\\) can be defined similar to the \\(fv()\\) function we introduced for lambda caculus. </p> \\[ \\begin{array}{rcl} ftv(\\alpha) &amp; = &amp; \\{\\alpha \\} \\\\  ftv(int) &amp; = &amp; \\{ \\} \\\\ ftv(bool) &amp; = &amp; \\{ \\} \\\\ ftv(T_1 \\rightarrow T_2) &amp; = &amp; ftv(T_1) \\cup ftv(T_2) \\\\ ftv(\\forall \\alpha.\\sigma) &amp; = &amp; ftv(\\sigma) - \\{ \\alpha \\}  \\end{array} \\] <p>\\(ftv()\\) is also overloaded to extra free type variables from a type environment.</p> \\[ \\begin{array}{rcl} ftv(\\Gamma) &amp; = &amp; \\{ \\alpha \\mid (x,\\sigma) \\in \\Gamma \\wedge \\alpha \\in ftv(\\sigma) \\} \\end{array} \\] <p>The application of a type substitution can be defined as </p> \\[ \\begin{array}{rcll} [] \\sigma &amp; = &amp; \\sigma \\\\  [T/\\alpha] int &amp; = &amp; int \\\\ [T/\\alpha] bool &amp; = &amp; bool \\\\  [T/\\alpha] \\alpha &amp; = &amp; T \\\\ [T/\\alpha] \\beta &amp; = &amp; \\beta &amp; \\beta \\neq \\alpha \\\\  [T/\\alpha] T_1 \\rightarrow T_2 &amp; = &amp; ([T/\\alpha] T_1) \\rightarrow ([T/\\alpha] T_2) \\\\  [T/\\alpha] \\forall \\beta. \\sigma &amp; = &amp; \\forall \\beta. ([T/\\alpha]\\sigma) &amp; \\beta \\neq \\alpha \\wedge \\beta \\not \\in ftv(T) \\\\  (\\Psi_1 \\circ \\Psi_2)\\sigma &amp; = &amp; \\Psi_1 (\\Psi_2 (\\sigma)) \\end{array} \\] <p>In case of applying a type subtitution to a type scheme, we need to check whether the quantified type variable \\(\\beta\\) is in conflict with the type substitution. In case of conflict, a renaming operation simiilar to \\(\\alpha\\) renaming will be applied to \\(\\forall \\beta. \\sigma\\). </p>"},{"location":"static_semantics_2/#example","title":"Example","text":"<p>Let's consider the type-checking derivation of our running (counter) example. </p> <p>Let <code>\u0393 = {}</code> and <code>\u03931 = {(f,\u2200\u03b1.\u03b1-&gt;\u03b1)}</code>.</p> <pre><code>                           -------------------(hmVar)\n                           \u03931\u2295(x,\u03b2)\u2295(y,\u03b3)|-x:\u03b2 \n                           --------------------(hmLam)\n                           \u03931\u2295(x,\u03b2)|-\u03bby.x:\u03b3-&gt;\u03b2\n------------(hmVar)        -------------------(hmLam)\n\u0393\u2295(x,\u03b1)|-x:\u03b1               \u03931|-\u03bbx.\u03bby.x:\u03b2-&gt;\u03b3-&gt;\u03b2   \u03b3,\u03b2\u2209ftv(\u03931)\n------------(hmLam)        --------------------------(hmGen)\n\u0393|-\u03bbx.x:\u03b1-&gt;\u03b1    \u03b1\u2209ftv(\u0393)   \u03931|-\u03bbx.\u03bby.x:\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2          [subtree 1]\n-----------------(hmGen)   -------------------------------------------(hmLet)\n\u0393|-\u03bbx.x:\u2200\u03b1.\u03b1-&gt;\u03b1            \u03931|-let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int    \n------------------------------------------------------------------- (hmLet)\n\u0393|-let f = \u03bbx.x in (let g = \u03bbx.\u03bby.x in (g (f 1) (f true)):int\n</code></pre> <p>Let <code>\u03932 = {(f,\u2200\u03b1.\u03b1-&gt;\u03b1), (g,\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)}</code>, we find [subtree 1] is as follows</p> <pre><code>--------------------(hmVar)\n\u03932|-g:\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2     \u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2 \u2291 \u2200\u03b3.int-&gt;\u03b3-&gt;int\n----------------------------------(hmInst)\n\u03932|-g:\u2200\u03b3.int-&gt;\u03b3-&gt;int                       [subtree 3]\n-----------------------------------------------(hmApp)\n\u03932|-g (f 1):\u2200\u03b3.\u03b3-&gt;int                      \u2200\u03b3.\u03b3-&gt;int \u2291 bool-&gt;int \n-------------------------------------------------(hmInst)    \n\u03932|-g (f 1):bool-&gt;int                                   [subtree 2]\n---------------------------------------------------------------(hmApp)\n\u03932|-g (f 1) (f true):int\n</code></pre> <p>Where [subtree 2] is as follows</p> <pre><code>--------------(hmVar)\n\u03932|-f:\u2200\u03b1.\u03b1-&gt;\u03b1 \u2200\u03b1.\u03b1-&gt;\u03b1 \u2291 bool-&gt;bool\n-------------------(hmInst)       ----------------(hmBool)\n\u03932|-f:bool-&gt;bool                  \u03932|-true:bool\n----------------------------------------------------(hmApp)\n\u03932|-f true:bool\n</code></pre> <p>Where [subtree 3] is as follows</p> <pre><code>--------------(hmVar)\n\u03932|-f:\u2200\u03b1.\u03b1-&gt;\u03b1 \u2200\u03b1.\u03b1-&gt;\u03b1 \u2291 int-&gt;int\n-------------------(hmInst)       ----------------(hmInt)\n\u03932|-f:int-&gt;int                    \u03932|-1:int\n---------------------------------------------------(hmApp)\n\u03932|-f 1:int\n</code></pre> <p>As we can observe, through the use of rules of \\({\\tt (hmGen)}\\) and \\({\\tt (hmVar)}\\), we are able to give let-bound variables <code>f</code> and <code>g</code> some generic types (AKA parametric polymorphic types). Through rules \\({\\tt (hmApp)}\\) and \\({\\tt (hmInst)}\\) we are able to \"instantiate\" these polymoprhic types to the appropriate monomorphic types depending on the contexts. </p> <p>Note that the goal of Hindley Milner type system is to store the most general (or principal) type (scheme) of a lambda term in the type environment, (especially the program variables and function names), so that when an application is being type-checked, we are able to instantiate a specific type based on the context, as we observe that it is always an combo of \\({\\tt (hmVar)}\\) rule followed by \\({\\tt (hmInst)}\\) rule.</p>"},{"location":"static_semantics_2/#property-4-uniqueness","title":"Property 4 - Uniqueness","text":"<p>The following property states that if a lambda term is typable, its type scheme must be unique modulo type variable renaming.</p> <p>Let \\(t\\) be a lambda calculus term. Let \\(\\Gamma\\) be a type environment such that for all \\(x \\in fv(t)\\), \\(x \\in dom(\\Gamma)\\). Let \\(\\sigma\\) and \\(\\sigma'\\) be type schemes such that \\(\\Gamma \\vdash t : \\sigma\\) and \\(\\Gamma \\vdash t:\\sigma'\\). Then \\(\\sigma\\) and \\(\\sigma'\\) must be the same modulo type variable renaming.</p> <p>For instance, we say type schemes \\(\\forall \\alpha.\\alpha \\rightarrow int\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are the same modulo type variable renaming. But type schemes \\(\\forall \\alpha.\\alpha \\rightarrow bool\\) and \\(\\forall \\beta.\\beta \\rightarrow int\\) are not the same.</p>"},{"location":"static_semantics_2/#property-5-progress","title":"Property 5 - Progress","text":"<p>The Progress property is valid for Hindley Milner type checking.</p> <p>Let \\(t\\) be a lambda calculus term such that \\(fv(t) = \\{\\}\\).  Let \\(\\sigma\\) be a type scheme such that \\(\\Gamma_{init} \\vdash t : \\sigma\\). Then \\(t\\) is either a value or there exists some \\(t'\\) such that \\(t \\longrightarrow t'\\).</p>"},{"location":"static_semantics_2/#property-6-preservation","title":"Property 6 - Preservation","text":"<p>The Presevation property is also held for Hindley Milner type checking.</p> <p>Let \\(t\\) and \\(t'\\) be lambda calculus terms such that \\(t \\longrightarrow t'\\). Let \\(\\sigma\\) be a type scheme and \\(\\Gamma\\) be a type environment such that \\(\\Gamma \\vdash t:\\sigma\\). Then \\(\\Gamma \\vdash t':\\sigma\\).</p>"},{"location":"static_semantics_2/#type-inference-for-lambda-calculus","title":"Type Inference for Lambda Calculus","text":"<p>To infer the type environment as well as the type for lambda calculus term, we need an algorithm called Algorithm W.</p> <p>The algorithm is described in a deduction rule system of shape \\(\\Gamma, t \\vDash T, \\Psi\\), which reads as given input type environment \\(\\Gamma\\) and a lambda term \\(t\\), the algorithm infers the type \\(T\\) and type substitution \\(\\Psi\\).</p> \\[ \\begin{array}{rc} {\\tt (wInt)} &amp; \\begin{array}{c}                 c\\ {\\tt is\\ an\\ integer}                  \\\\ \\hline                 \\Gamma, c \\vDash int, []                 \\end{array} \\\\ \\\\ {\\tt (wBool)} &amp; \\begin{array}{c}                 c\\in \\{true,false \\}                  \\\\ \\hline                 \\Gamma, c \\vDash bool, []                 \\end{array} \\end{array} \\] <p>The rules for integer and boolean constants are straight forward. We omit the explanation.</p> \\[ \\begin{array}{rc} {\\tt (wVar)} &amp; \\begin{array}{c}                 (x,\\sigma) \\in \\Gamma \\ \\ \\ inst(\\sigma) = T                 \\\\ \\hline                 \\Gamma, x \\vDash T, []                 \\end{array} \\\\ \\\\ {\\tt (wFix)} &amp; \\begin{array}{c}                 (fix,\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) \\in \\Gamma \\ \\ \\ inst(\\forall \\alpha. (\\alpha\\rightarrow \\alpha)\\rightarrow \\alpha) = T                 \\\\ \\hline                 \\Gamma, fix \\vDash T, []                 \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wVar)}\\) infers the type for a variable by looking it up from the input type environment \\(\\Gamma\\). Same observation applies to \\({\\tt (wFix)}\\) since we assume that \\(fix\\) is pre-defined in the initial type environment \\(\\Gamma_{init}\\), which serves as the starting input.</p> \\[ \\begin{array}{rc} {\\tt (wLam)} &amp; \\begin{array}{c}                 \\alpha_1 = newvar \\ \\ \\ \\Gamma \\oplus (x,\\alpha_1), t \\vDash T, \\Psi                 \\\\ \\hline                 \\Gamma, \\lambda x.t \\vDash : \\Psi(\\alpha_1 \\rightarrow T ), \\Psi                 \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wLam)}\\) infers the type for a lambda abstraction by \"spawning\" a fresh skolem type variable \\(\\alpha_1\\) which is reserved for the lambda bound variable \\(x\\). Under the extended type environment \\(\\Gamma \\oplus (x,\\alpha_1)\\) it infers the body of the lambda extraction \\(t\\) to have type \\(T\\) and the type substitution \\(\\Psi\\). The inferred type of the entire lambda abstraction is therefore \\(\\Psi(\\alpha_1 \\rightarrow T)\\). The reason is that while infering the type for the lambda body, we might obtain substitution that grounds \\(\\alpha_1\\). For instance \\(\\lambda x. x + 1\\) will ground \\(x\\)'s skolem type variable to \\(int\\).</p> \\[ \\begin{array}{rc} {\\tt (wApp)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2\\ \\ \\\\ \\alpha_3 = newvar\\ \\ \\ \\Psi_3 = mgu(\\Psi_2(T_1), T_2 \\rightarrow \\alpha_3)                  \\\\ \\hline                 \\Gamma, (t_1\\ t_2) \\vDash \\Psi_3(\\alpha_3), \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1                 \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wApp)}\\) infers the type for a function application \\(t_1\\ t_2\\). We first apply the inference recursively to \\(t_1\\), producing a type \\(T_1\\) and a type substitution \\(\\Psi_1\\). Next we apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some of the type variables inside and use it to infer \\(t_2\\)'s type as \\(T_2\\) with a subsitution \\(\\Psi_2\\). To denote the type of the application, we generate a fresh skolem type variable \\(\\alpha_3\\) reserved for this term. We perform a unification between \\(\\Psi_2(T_1)\\) (hoping \\(\\Psi_2\\) will ground some more type variables in \\(T_1\\)), and \\(T_2 \\rightarrow \\alpha_3\\). If the unifcation is successful, it will result in another type substitution \\(\\Psi_3\\). \\(\\Psi_3\\) can potentially ground the type variable \\(\\alpha_3\\). At last we return \\(\\Psi_3(\\alpha_3)\\) as the inferred type and composing all three substitutions as the resulted substitution.</p> \\[ \\begin{array}{rc} {\\tt (wLet)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\\\ \\Psi_1(\\Gamma) \\oplus (x, gen(\\Psi_1(\\Gamma), T_1)), t_2 \\vDash T_2, \\Psi_2                 \\\\ \\hline                 \\Gamma, let\\ x=t_1\\ in\\ t_2 \\vDash T_2, \\Psi_2 \\circ \\Psi_1              \\end{array} \\end{array} \\] <p>The \\({\\tt (wLet)}\\) rule infers a type for the let binding. We first infer the type \\(T_1\\) and type substitutions \\(\\Psi_1\\). By applying \\(\\Psi_1\\) to \\(\\Gamma\\) we hope to ground some type variables in \\(\\Gamma\\). We apply a helper function \\(gen\\) to generalize \\(T_1\\) w.r.t \\(\\Psi_1(\\Gamma)\\), and use it as the type for \\(x\\) to infer \\(t_2\\) type. Finally, we return \\(T_2\\) as the inferred type and \\(\\Psi_2 \\circ \\Psi_1\\) as the type substitutions. </p> \\[ \\begin{array}{rc} {\\tt (wOp1)} &amp; \\begin{array}{c}                 op \\in \\{+,-,*,/\\} \\\\                  \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\\\                  mgu(\\Psi_2(T_1), T_2, int) = \\Psi_3                    \\\\ \\hline                  \\Gamma, t_1\\ op\\ t_2 \\vDash int, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1                  \\end{array} \\\\ \\\\  {\\tt (wOp2)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1 \\ \\ \\ \\Psi_1(\\Gamma), t_2 \\vDash T_2, \\Psi_2 \\\\                  mgu(\\Psi_2(T_1), T_2) = \\Psi_3                    \\\\ \\hline                  \\Gamma, t_1\\ ==\\ t_2 \\vDash bool, \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1                  \\end{array} \\end{array} \\] <p>The rule \\({\\tt (wOp1)}\\) handles the type inference for arithmetic binary operation. The result type must be \\(int\\). In the premises, we infer the type of the left operand \\(t_1\\) to be \\(T_1\\) with a type substitution \\(\\Psi_1\\). We apply \\(\\Psi_1\\) to \\(\\Gamma\\) hoping to ground some type variables. We continue to infer the right operand \\(t_2\\) with a type \\(T_2\\) and \\(\\Psi_2\\). Finally we need to unify  \\(\\Psi_2(T_1)\\), \\(T_2\\) and \\(int\\) to form \\(\\Psi_3\\). Note that we don't need to apply \\(\\Psi_1\\) to \\(T_2\\) during the unification, because \\(T_2\\) is infered from \\(\\Psi_1(\\Gamma)\\), i.e. type variables in \\(T_2\\) is either already in the domain of \\(\\Psi_1(\\Gamma)\\), or it is enirely fresh, i.e. not in \\(T_1\\) and \\(\\Psi_1\\). We return \\(\\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1\\) as the final substitution. </p> <p>In rule \\({\\tt (wOp2)}\\), the binary operator is an equality check. It works similar to the rule \\({\\tt (wOp1)}\\) except that we return \\(bool\\) as the result type, and we do not include \\(int\\) as the additional operand when unifying the the types of \\(\\Psi_2(T_1)\\) and \\(T_2\\). </p> \\[ \\begin{array}{rc} {\\tt (wIf)} &amp; \\begin{array}{c}                 \\Gamma, t_1 \\vDash T_1, \\Psi_1\\ \\ \\                 \\Psi_1' = mgu(bool, T_1) \\circ \\Psi_1 \\\\                 \\Psi_1'(\\Gamma),t_2 \\vDash T_2, \\Psi_2 \\ \\ \\                 \\Psi_1'(\\Gamma),t_3 \\vDash T_3, \\Psi_3 \\\\                 \\Psi_4 = mgu(\\Psi_3(T_2), \\Psi_2(T_3))                  \\\\ \\hline                 \\Gamma, if\\ t_1\\ then\\ t_2\\ else\\ t_3 \\vDash \\Psi_4(\\Psi_3(T_2)),  \\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'               \\end{array} \\end{array} \\] <p>In the rule \\({\\tt (wIf)}\\), we infer the type of \\(if\\ t_1\\ then\\ t_2\\ else\\ t_3\\). In the premises, we first infer the type of \\(t_1\\) to be type \\(T_1\\) and type subsitution \\(\\Psi_1\\). Since \\(t_1\\) is used as a condition expression, we define a refined substitution \\(\\Psi_1'\\) by unifing \\(bool\\) with \\(T_1\\) and composing the result with \\(\\Psi_1\\). We then apply \\(\\Psi_1'\\) to \\(\\Gamma\\) and infer \\(t_2\\) and \\(t_3\\).  Finally we unify the returned types from both branches, i.e. \\(\\Psi_3(T_2)\\) and \\(\\Psi_2(T_3)\\). Note that we have to cross apply the type substitutions to ground some type variables. We return \\(\\Psi_4(\\Psi_2(T_2))\\) as the overall inferred type and \\(\\Psi_4 \\circ \\Psi_3 \\circ \\Psi_2 \\circ \\Psi_1'\\) as the overall type substitution. </p>"},{"location":"static_semantics_2/#helper-functions","title":"Helper functions","text":"<p>We find the list of helper functions defined in Algorithm W.</p>"},{"location":"static_semantics_2/#type-substitution","title":"Type Substitution","text":"\\[ \\begin{array}{rcl} \\Psi(\\Gamma)  &amp;= &amp; \\{ (x,\\Psi(\\sigma)) \\mid (x,\\sigma) \\in \\Gamma \\} \\end{array} \\]"},{"location":"static_semantics_2/#type-instantiation","title":"Type Instantiation","text":"\\[ \\begin{array}{rcl} inst(T) &amp; = &amp; T \\\\ inst(\\forall \\alpha.\\sigma) &amp; = &amp; \\lbrack\\beta_1/\\alpha\\rbrack(inst(\\sigma))\\ where\\ \\beta_1=newvar \\\\ \\end{array} \\] <p>The type instantation function instantiate a type scheme. In case of a simple type \\(T\\), it returns \\(T\\). In case it is a polymorphic type scheme \\(\\forall \\alpha.\\sigma\\), we generate a new skolem type variable \\(\\beta_1\\) and replace all the occurances of \\(\\alpha\\) in \\(inst(\\sigma)\\). In some literature, these skolem type variables are called the unification type variables as they are created for the purpose of unification.</p>"},{"location":"static_semantics_2/#type-generalization","title":"Type Generalization","text":"\\[ \\begin{array}{rcl} gen(\\Gamma, T) &amp; = &amp; \\forall \\overline{\\alpha}.T\\ \\ where\\ \\overline{\\alpha} = ftv(T) - ftv(\\Gamma) \\end{array} \\] <p>The type generation function turns a type \\(T\\) into a type scheme if there exists some free type variable in \\(T\\) but not in \\(ftv(\\Gamma)\\), i.e. skolem variables. </p>"},{"location":"static_semantics_2/#type-unification","title":"Type Unification","text":"\\[ \\begin{array}{rcl} mgu(\\alpha, T) &amp; = &amp; [T/\\alpha] \\\\  mgu(T, \\alpha) &amp; = &amp; [T/\\alpha] \\\\  mgu(int, int) &amp; = &amp; [] \\\\  mgu(bool, bool) &amp; = &amp; [] \\\\  mgu(T_1 \\rightarrow T_2 , T_3\\rightarrow T_4) &amp; = &amp; let\\ \\Psi_1 = mgu(T_1, T_3)\\ \\\\  &amp;  &amp; in\\ \\ let\\ \\Psi_2 = mgu(\\Psi_1(T_2), \\Psi_1(T_4)) \\\\ &amp;  &amp; \\ \\ \\ \\ \\ in\\ \\Psi_2 \\circ \\Psi_1 \\end{array} \\] <p>The type unification process is similar to the one described for SIMP program type inference, except that we included an extra case for function type unification. In the event of unifying two function types \\(T_1 \\rightarrow T_2\\) and \\(T_3 \\rightarrow T_4\\), we first unify the argument types \\(T_1\\) and \\(T_3\\) then apply the result to \\(T_2\\) and \\(T_4\\) and unify them.</p>"},{"location":"static_semantics_2/#examples","title":"Examples","text":"<p>Let's consider some examples </p>"},{"location":"static_semantics_2/#example-1-lambda-xx","title":"Example 1 \\(\\lambda x.x\\)","text":"<p>Let \\(\\Gamma = \\{(fix,\\forall \\alpha. (\\alpha \\rightarrow \\alpha) \\rightarrow \\alpha)\\}\\)</p> <pre><code>           (x,\u03b11)\u2208\u0393\u2295(x,\u03b11)  inst(\u03b11)=\u03b11\n           ----------------------------(wVar)\n\u03b11=newvar  \u0393\u2295(x,\u03b11),x|=\u03b11,[]\n------------------------------------------(wLam)\n\u0393,\u03bbx.x|= \u03b11-&gt;\u03b11, []\n</code></pre>"},{"location":"static_semantics_2/#example-2-lambda-xlambda-yx","title":"Example 2 \\(\\lambda x.\\lambda y.x\\)","text":"<pre><code>                     (x,\u03b21)\u2208\u0393\u2295(x,\u03b21)\u2295(y,\u03b31) inst(\u03b21)=\u03b21\n                     --------------------------------(wVar)\n           \u03b31=newvar \u0393\u2295(x,\u03b21)\u2295(y,\u03b31),x|= \u03b21,[]\n           --------------------------------------(wLam)\n\u03b21=newvar  \u0393\u2295(x,\u03b21),\u03bby.x|=\u03b31-&gt;\u03b21,[]\n-------------------------------------------------(wLam)\n\u0393,\u03bbx.\u03bby.x|= \u03b21-&gt;\u03b31-&gt;\u03b21,[]\n</code></pre>"},{"location":"static_semantics_2/#example-3-let-flambda-xx-in-let-glambda-xlambda-yx-in-g-f-1-f-true","title":"Example 3 \\(let\\ f=\\lambda x.x\\ in\\ (let\\ g=\\lambda x.\\lambda y.x\\ in\\ g\\ (f\\ 1)\\ (f\\ true))\\)","text":"<pre><code>[Example 1]\n-------------------\n\u0393,\u03bbx.x|= \u03b11-&gt;\u03b11, []   gen(\u0393,\u03b11-&gt;\u03b11)=\u2200\u03b1.\u03b1-&gt;\u03b1  [subtree 1]\n------------------------------------------------------------------(wLet)\n\u0393,let f=\u03bbx.x in (let g=\u03bbx.\u03bby.x in g (f 1) (f true))|= int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\n</code></pre> <p>Let <code>\u03931 =\u0393\u2295(f,\u2200\u03b1.\u03b1-&gt;\u03b1)</code>, where [subtree 1] is </p> <pre><code>[Example 2]\n--------------------------   \n\u03931,\u03bbx.\u03bby.x|= \u03b21-&gt;\u03b31-&gt;\u03b21,[]  gen(\u03931,\u03b21-&gt;\u03b31-&gt;\u03b21)=\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2 [subtree 2] \n-------------------------------------------------------------------------------(wLet)\n\u03931,let g=\u03bbx.\u03bby.x in g (f 1) (f true)|=int, \u03a83\u25cb[bool/\u03b32,int/\u03b41]\u25cb[]\n</code></pre> <p>Let <code>\u03932 =\u0393\u2295(f,\u2200\u03b1.\u03b1-&gt;\u03b1)\u2295(g,\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)</code>, where [subtree 2] is</p> <pre><code>[subtree 3]  [subtree 5] \u03b41=newvar  mgu(\u03b32-&gt;int,bool-&gt;\u03b41)=[bool/\u03b32,int/\u03b41]\n--------------------------------------------------------------------------(wApp)\n\u03932, g (f 1) (f true)|= [bool/\u03b32,int/\u03b41](\u03b41), \u03a83\u25cb[bool/\u03b32,int/\u03b41]\n</code></pre> <p>Where [subtree 3] is</p> <pre><code>(g,\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)\u2208\u03932                 \u03b51=newvar\ninst(\u2200\u03b2.\u2200\u03b3.\u03b2-&gt;\u03b3-&gt;\u03b2)=\u03b22-&gt;\u03b32-&gt;\u03b22       mgu(\u03b22-&gt;\u03b32-&gt;\u03b22,int-&gt;\u03b51)=[int/\u03b22,\u03b32-&gt;int/\u03b51]\n--------------------------(wVar)    \n\u03932, g|=\u03b22-&gt;\u03b32-&gt;\u03b22, []        [subtree 4]\n---------------------------------------------------------------------(wApp)\n\u03932, g (f 1)|= [int/\u03b22,\u03b32-&gt;int/\u03b51](\u03b51),[int/\u03b22,\u03b32-&gt;int/\u03b51]\u25cb[int/\u03b61,int/\u03b12]\n</code></pre> <p>Where [subtree 4] is </p> <pre><code>(f,\u2200\u03b1.\u03b1-&gt;\u03b1)\u2208\u03932 \ninst(\u2200\u03b1.\u03b1-&gt;\u03b1)=\u03b12-&gt;\u03b12                      \u03b61=newvar\n-----------------(wVar) ------------(wInt)    \n\u03932, f|=\u03b12-&gt;\u03b12,[]        \u03932,1|=int,[]      mgu(\u03b12-&gt;\u03b12,int-&gt;\u03b61)=[int/\u03b61,int/\u03b12]\n---------------------------------------------------------------------(wApp)\n[](\u03932),f 1|= [int/\u03b61,int/\u03b12](\u03b61), [int/\u03b61,int/\u03b12]\n</code></pre> <p>Let <code>\u03a83=[int/\u03b22,\u03b32-&gt;int/\u03b51]\u25cb[int/\u03b61,int/\u03b12]</code>, note that <code>\u03a83(\u03932) =\u03932</code>,  where [subtree 5] is </p> <pre><code>(f,\u2200\u03b1.\u03b1-&gt;\u03b1)\u2208\u03932\ninst(\u2200\u03b1.\u03b1-&gt;\u03b1)=\u03b13-&gt;\u03b13                      \u03b71=newvar\n----------------(wVar) ----------(wBool) \n\u03932,f|=\u03b13-&gt;\u03b13, []       \u03932,true|=bool,[]   mgu(\u03b13-&gt;\u03b13,bool-&gt;\u03b71)=[bool/\u03b13,bool/\u03b71]\n-----------------------------------------------------------------------[wApp]\n\u03932,f true|=[bool/\u03b13,bool/\u03b71](\u03b13),[bool/\u03b13,bool/\u03b71]\n</code></pre>"},{"location":"static_semantics_2/#property-7-type-inference-soundness","title":"Property 7: Type Inference Soundness","text":"<p>The following property states that the type and subsitution generated by Algorithm W is able to type check the lambda calculus term in Hindley Milners' type system.</p> <p>Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\). Then \\(\\Gamma \\vdash t:gen(\\Gamma_{init},\\Psi(T))\\). </p>"},{"location":"static_semantics_2/#property-8-principality","title":"Property 8: Principality","text":"<p>The following property states that the type generated by Algorithm W is the principal type.</p> <p>Let \\(t\\) be a lambda calculus term and \\(\\Gamma_{init}\\) is a initial type environment and \\(\\Gamma_{init}, t \\vDash T, \\Psi\\). Then \\(gen(\\Gamma_{init}, \\Psi(T))\\) is the most general type scheme to type check \\(t\\). </p>"},{"location":"syntax_analysis/","title":"50.054 - Syntax Analysis","text":""},{"location":"syntax_analysis/#learning-outcome","title":"Learning Outcome","text":"<p>By the end of this lesson, you should be able to</p> <ol> <li>Describe the roles and functionalities of lexers and parsers in a compiler pipeline</li> <li>Describe the difference between top-down parsing and bottom-up parsing</li> <li>Apply left-recursion elimination and left-factoring</li> <li>Construct a <code>LL(1)</code> predictive parsing table</li> <li>Explain first-first conflicts and first-follow conflicts</li> </ol>"},{"location":"syntax_analysis/#a-compiler-pipeline","title":"A compiler pipeline","text":"<pre><code>graph LR\nA[Lexing] --&gt;B[Parsing] --&gt; C[Semantic Analysis] --&gt; D[Optimization] --&gt; E[Target Code Generation]</code></pre> <ul> <li>Lexing</li> <li>Input: Source file in String</li> <li>Output: A sequence of valid tokens according to the language specification (grammar)</li> <li>Parsing</li> <li>Input: Output from the Lexer</li> <li>Output: A parse tree representing parsed result according to the parse derivation</li> </ul> <p>A parse tree can be considered the first intermediate representation (IR).</p>"},{"location":"syntax_analysis/#language-grammar-and-rules","title":"Language, Grammar and Rules","text":""},{"location":"syntax_analysis/#what-is-a-language","title":"What is a language?","text":"<p>A language is a set of strings.</p>"},{"location":"syntax_analysis/#what-is-a-grammar","title":"What is a grammar?","text":"<p>A grammar is a specification of a language, including the set of valid words (vocabulary) and the set of possible structures formed by the words.</p> <p>One common way to define a grammar is by defining a set of production rules.</p>"},{"location":"syntax_analysis/#a-running-example","title":"A running example","text":"<p>Let's consider the language of JSON. Though JSON has no operational semantics, i.e. it's not executable, it serves a good subject for syntax analysis.</p> <p>The grammar rule for JSON is as follows</p> <pre><code>&lt;&lt;Grammar 1&gt;&gt;\n(JSON) J ::= i | 's' | [] | [IS] | {NS}\n(Items) IS ::= J,IS | J\n(Named Objects) NS ::= N,NS | N\n(Named Object) N ::= 's':J\n</code></pre> <p>In the above, the grammar consists of four production rules. Each production rule is of form</p> <pre><code>(Name) LHS ::= RHS \n</code></pre> <p>Sometimes, we omit the Name. Terms in upper case, are the non-terminals, and terms in lower case, and symbol terms are the terminals.</p> <p>For each production rule, the LHS is always a non-terminal. On the RHS, we have alternatives separated by <code>|</code>. Each alternatives consists of terminals, non-terminals and mixture of both.</p> <p>For instance, the production rule <code>(JSON)</code> states that a JSON non-terminal <code>J</code> is either an <code>i</code> (an integer), a <code>'s'</code> (a quoted string), an empty list <code>[]</code>, an non-empty list <code>[IS]</code> and an object <code>{NS}</code>.  </p> <p>A production rule with multiple alternatives, can be rewritten into multiple production rules without alternatives. For instance, the <code>(JSON)</code> production rule can be rewritten as follows,</p> <pre><code>J ::= i\nJ ::= 's'\nJ ::= []\nJ ::= [IS]\nJ ::= {NS}\n</code></pre> <p>For each grammar, we expect the LHS of the first production rule is the starting symbol.</p>"},{"location":"syntax_analysis/#lexing","title":"Lexing","text":"<p>Input: Source file in string</p> <p>Output: A sequence of valid tokens according to the language specification (grammar)</p> <p>The purpose of a lexer is to scan through the input source file to ensure the text is constructed as a sequence of valid tokens specified by the syntax rules of the source langugage. The focus is on token-level. The inter-token constraint validation is performed in the next step, Parsing.</p> <p>Sometimes, a lexer is omitted, as the token validation task can be handled in the parser.</p>"},{"location":"syntax_analysis/#lexical-tokens","title":"Lexical Tokens","text":"<p>The set of tokens of a grammar is basically all the terminals. In this JSON grammar example,</p> <pre><code>{i, s, ', [, ], {, }, :, \\, }\n</code></pre> <p>and white spaces are the Lexical Tokens of the language.</p> <p>If we are to represent it using Scala data types, we could use the following algebraic data type:</p> <pre><code>enum LToken { // lexical Tokens\n    case IntTok(v:Int)\n    case StrTok(v:String)\n    case SQuote\n    case LBracket\n    case RBracket\n    case LBrace\n    case RBrace\n    case Colon\n    case Comma\n    case WhiteSpace\n}\n</code></pre> <p>Note that in the above, we find that <code>IntTok</code> and <code>StrTok</code> have semantic components (i.e. the underlying values.) The rest of the tokens  do not.</p> <p>Given the input</p> <pre><code>{'k1':1,'k2':[]}\n</code></pre> <p>the  lexer function <code>lex(s:String):List[LToken]</code> should return</p> <pre><code>List(LBRace,SQuote,StrTok(\"k1\"),SQuote,Colon,IntTok(1),Comma,SQuote, StrTok(\"k2\"), Colon,LBracket, RBracket, RBrace)\n</code></pre> <p>One could argue that we cheat by assuming the integer and string data types are available as builtin terminals. In case we don't have integer and string as builtin terminals, we could expand the grammar as follows:</p> <pre><code>&lt;&lt;Grammar 2&gt;&gt;\n(JSON) J ::= I | 'STR' | [] | [IS] | {NS}\n(Items) IS ::= J,IS | J\n(Named Objects) NS ::= N,NS | N\n(Named Object) N ::= 'STR':J\n(Integer) I ::= dI | d\n(String) STR ::= aSTR | a\n</code></pre> <p>where <code>d</code> denotes a single digit and <code>a</code> denotes a single ascii character.</p> <p>For the rest of this lesson, we will stick with the first formulation in which we have integer and string terminals builtin, which is common for modern languages.</p>"},{"location":"syntax_analysis/#implementing-a-lexer-using-regular-expression","title":"Implementing a Lexer using Regular Expression","text":"<p>Perhaps one easier way to implement a lexer is to make use of regular expression.</p>"},{"location":"syntax_analysis/#a-simple-example-of-using-scalautilmatchingregex","title":"A simple example of using <code>scala.util.matching.Regex</code>","text":"<p>We can specify a regex pattern as follows. This example was adopted from (https://www.scala-lang.org/api/3.3.6/scala/util/matching/Regex.html)</p> <pre><code>val date = raw\"(\\d{4})-(\\d{2})-(\\d{2})\".r\n</code></pre> <p>Next we can perform a match against the above regex pattern using the <code>match</code> expression.</p> <pre><code>\"2004-01-20\" match {\n  case date(year, month, day) =&gt; s\"$year was a good year for PLs.\"\n}\n</code></pre> <p>The above expression is evaluated to</p> <pre><code>2004 was a good year for PLs.\n</code></pre> <p>We could develop a simple lexer using the above trick. First we define the pattern for reach token.</p> <pre><code>val integer = raw\"(\\d+)(.*)\".r\nval string = raw\"([^']*)(.*)\".r\nval squote = raw\"(')(.*)\".r\nval lbracket = raw\"(\\[)(.*)\".r\nval rbracket = raw\"(\\])(.*)\".r\nval lbrace = raw\"(\\{)(.*)\".r\nval rbrace = raw\"(\\})(.*)\".r\nval colon = raw\"(:)(.*)\".r\nval comma = raw\"(,)(.*)\".r\n</code></pre> <p>For each token, we have two sub patterns, the first sub-pattern capture the token, and second sub-pattern captures the remaining input, so that we can pass it to the next iteration.</p> <p>Next we define the following function which tries to extract a token from the begining of the input string, and return the rest if a match is found, otherwise, an error is returned.</p> <pre><code>import LToken.*\ntype Error = String\ndef lex_one(src:String):Either[String, (LToken, String)] = src match {\n    case integer(s, rest) =&gt; Right((IntTok(s.toInt), rest))\n    case squote(_, rest) =&gt; Right((SQuote, rest))\n    case lbracket(_, rest) =&gt; Right((LBracket, rest))\n    case rbracket(_, rest) =&gt; Right((RBracket, rest)) \n    case lbrace(_, rest) =&gt; Right((LBracket, rest))\n    case rbrace(_, rest) =&gt; Right((RBracket, rest)) \n    case colon(_, rest) =&gt; Right((Colon, rest))\n    case comma(_, rest) =&gt; Right((Comma, rest))\n    case string(s, rest) =&gt; Right((StrTok(s), rest))\n    case _ =&gt; Left(s\"lexer error: unexpected token at ${src}\")\n}\n</code></pre> <p>Note that the order of the Scala patterns is important, since there is some overlapping cases from the above definition (e.g. the regex pattern <code>string</code> and the rest except for <code>squote</code>).</p> <p>Lastly we define the top level <code>lex</code> function by calling <code>lex_one</code> in a recursive function.</p> <pre><code>def lex(src:String):Either[Error, List[LToken]] = {\n    def go(src:String, acc:List[LToken]):Either[Error, List[LToken]] = {\n        if (src.length == 0)  \n        {\n            Right(acc)\n        } \n        else \n        {\n            lex_one(src) match {\n                case Left(error) =&gt; Left(error)\n                case Right((ltoken, rest)) =&gt; go(rest, acc++List(ltoken))\n            }\n        }\n    }\n    go(src, List())\n}\n</code></pre>"},{"location":"syntax_analysis/#implementing-a-lexer-using-a-parser","title":"Implementing a Lexer using a Parser","text":"<p>In general, parsers are capable of handling context free grammar, which is a super-set of the regular grammars. (A grammar that can be expressed as a regular expression is a regular grammar.)</p> <p>Hence it is possible to implement a lexer using a parser, which we are going to discuss in the cohort problems.</p>"},{"location":"syntax_analysis/#parsing","title":"Parsing","text":"<p>Input: Output from the Lexer</p> <p>Output: A parse tree representing parsed result according to the parse derivation</p> <p>Why tree representation?</p> <ol> <li>Firstly, a tree representation allows efficient access to the sub parts of the source code and intuitive transformation.</li> <li>Secondly, a tree representation captures the relationship between the LHS non-terminals and their RHS in the production rules.</li> </ol>"},{"location":"syntax_analysis/#parsing-derivation","title":"Parsing Derivation","text":"<p>Given an input list of tokens, we could \"walk\" through the production rules starting from the starting non-terminal to find the part that is \"matching with\" the RHS.</p> <p>Consider the JSON grammar in its unabridged form,</p> <pre><code>(1) J ::= i\n(2) J ::= 's'\n(3) J ::= []\n(4) J ::= [IS]\n(5) J ::= {NS}\n(6) IS ::= J,IS\n(7) IS ::= J \n(8) NS ::= N,NS\n(9) NS ::= N\n(10) N ::= 's':J\n</code></pre> <p>We take the output from our lexer example as the input, with some simplification by removing the Scala constructors</p> <pre><code>{ , ' , k1 , ' , : , 1 , , , ' , k2 , : , [ , ] , }\n</code></pre> <p>For each token, we attempt to search for a matched rule by scanning the set of production rules from top to bottom.</p>  Rule   Parse tree   Symbols   Input   (5)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]  {  NS }  {  ' k  1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]     NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   (8)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N,NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   ... (for the steps skipped, please refer to syntax_analysis_annex.md)   (3)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];  [ ] } [ ] }  graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];  <p>From the above example, it shows that we could implement a parsing routine by recursively searching for a matching production rule based on the current input string and non-terminal (LHS).</p> <p>This algorithm is easy to understand but it has some flaws.</p> <ol> <li>It does not terminate when the grammar contains left recursion.</li> <li>It involves some trial-and-error (back-tracking), hence it is not efficient</li> </ol>"},{"location":"syntax_analysis/#ambiguous-grammar","title":"Ambiguous Grammar","text":"<p>A grammar is said to be ambiguous if parsing it with an input can produce two different parse trees.</p> <p>Consider the following</p> <pre><code>&lt;&lt;Grammar 3&gt;&gt;\nE ::= E + E \nE ::= E * E\nE ::= i\n</code></pre> <p>Consider the input <code>1 + 2 * 3</code>. Parsing this input with the above grammar produces</p> <pre><code>graph\n  E--&gt;E1[\"E\"]\n  E--&gt;+\n  E--&gt;E2[\"E\"] \n  E1--&gt;i1[\"i(1)\"]\n  E2--&gt;E3[\"E\"]\n  E2--&gt;*\n  E2--&gt;E4[\"E\"]\n  E3--&gt;i2[\"i(2)\"]\n  E4--&gt;i3[\"i(3)\"];</code></pre> <p>or</p> <pre><code>graph\n  E--&gt;E1[\"E\"]\n  E--&gt;* \n  E--&gt;E2[\"E\"] \n  E1--&gt;E3[\"E\"]\n  E1--&gt;+\n  E1--&gt;E4[\"E\"]\n  E2--&gt;i3[\"i(3)\"]\n  E3--&gt;i1[\"i(1)\"]\n  E4--&gt;i2[\"i(2)\"];</code></pre> <p>To resolve ambiguity, the language designers need to make decision to give priority to certain production rules by rewriting it. For example, we argue that <code>*</code> should bind stronger than <code>+</code>. Thus we should rewrite (or rather \"restrict\") the ambiguous Grammar 3 into the following subset</p> <pre><code>&lt;&lt;Grammar 4&gt;&gt;\nE::= T + E\nE::= T\nT::= T * F \nT::= F\nF::= i\n</code></pre> <p>As a result, the input <code>1 + 2 * 3</code> is parsed as</p> <pre><code>graph\n  E--&gt;T1[\"T\"]\n  E--&gt;+ \n  E--&gt;E1[\"E\"] \n  T1--&gt;F1[\"F\"]\n  F1--&gt;i1[\"i(1)\"]\n  E1--&gt;T2[\"T\"]\n  T2--&gt;T3[\"T\"]\n  T3--&gt;F2[\"F\"]\n  F2--&gt;i2[\"i(2)\"]\n  T2--&gt;*\n  T2--&gt;F3[\"F\"]\n  F3--&gt;i3[\"i(3)\"]\n  ;</code></pre>"},{"location":"syntax_analysis/#grammar-with-left-recursion","title":"Grammar with Left Recursion","text":"<p>Let's try to run a top-down recursive parsing algorithm over the following grammar</p> <pre><code>&lt;&lt;Grammar 5&gt;&gt;\nE ::= E + T\nE ::= T\nT ::= i\n</code></pre> <p><code>i</code> and <code>+</code> are terminals, and <code>E</code> and <code>T</code> are the non-terminals. <code>i</code> denotes an integer.</p> <p>Consider applying the top-down recursive parsing mentioned above to the input <code>1</code>, if the first production rule is always selected, the algorithm would not terminate. The issue is that the first production rule containing a recursion at the left-most position. To eliminate left recursion in general, we consider the following transformation.</p> <p>Let <code>N</code> be a non-terminal, \\(\\alpha_i\\) and \\(\\beta_j\\) be sequences of symbols (consist of terminals and non-terminals)</p> <p>Left recursive grammar rules</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; N\\alpha_1 \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; N\\alpha_n \\\\ N &amp; ::= &amp; \\beta_1 \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m \\end{array} \\] <p>can be transformed into</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; \\beta_1 N' \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m N' \\\\ N' &amp; ::= &amp; \\alpha_1 N' \\\\ &amp; ... &amp; \\\\ N' &amp; ::= &amp; \\alpha_n N' \\\\ N' &amp; ::= &amp; \\epsilon \\end{array} \\] <p>Now apply the above to our running example.</p> <ul> <li>\\(N\\) is <code>E</code> and</li> <li>\\(\\alpha_1\\) is <code>+ T</code>,</li> <li><code>T</code> is \\(\\beta_1\\).</li> </ul> <pre><code>&lt;&lt;Grammar 6&gt;&gt;\nE ::= TE'\nE' ::= + TE'\nE' ::= epsilon\nT ::= i\n</code></pre> <p>The resulting Grammar 6 is equivalent the original Grammar 5.  Note that epsilon (\\(\\epsilon\\)) is a special terminal which denotes an empty sequence.</p> <p>There are few points to take note</p> <ol> <li>For indirect left recursion, some substitution steps are required before applying the above transformation. For instance</li> </ol> <pre><code>&lt;&lt;Grammar 7&gt;&gt;\nG ::= H + G\nH ::= G + i\nH ::= i\n</code></pre> <p>We need to substitute <code>H</code> into the first production rule.</p> <pre><code>&lt;&lt;Grammar 8&gt;&gt;\nG ::= G + i + G\nG ::= i + G\n</code></pre> <ol> <li>Since we have changed the grammar production rules, we need to use the transformed grammar for parsing, resulting in the parse trees being generated in the shape of the transformed grammar. We need to perform an extra step of (backwards) transformation to turn the parse trees back to the original grammar. For example, parsing the input <code>1 + 1</code> with Grammar 6 yields the following parse tree</li> </ol>  graph   E--&gt;T1[\"T\"]   E--&gt;Ep1[E']   T1--&gt;i1[\"i(1)\"]   Ep1--&gt;+   Ep1--&gt;T2[T]   Ep1--&gt;Ep2[E']   T2--&gt;i2[\"i(1)\"]   Ep2--&gt;eps1[\u03b5]  <p>which needs to be transformed back to</p>  graph   E--&gt;E1[\"E\"]   E--&gt;+   E--&gt;T1[\"T\"]   E1--&gt;T2[\"T\"]   T1--&gt;i1[\"i(1)\"]   T2--&gt;i2[\"i(1)\"]"},{"location":"syntax_analysis/#predictive-recursive-parsing","title":"Predictive Recursive Parsing","text":"<p>Next we address the inefficiency issue with our naive parsing algorithm. One observation from the derivation example we've seen earlier is that if we are able to pick the \"right\" production rule without trial-and-error, we would eliminate the backtracking.</p> <p>In order to do that we need to ensure the grammar we work with is a particular class of grammar, which is also known as <code>LL(k)</code> grammar. Here <code>k</code> refers to the number of leading symbols from the input we need to check in order to identify a particular production rule to apply without back-tracking.</p> <p>BTW, <code>LL(k)</code> stands for left-to-right, left-most derivation with <code>k</code> tokens look-ahead algorithm.</p> <p>Let \\(\\sigma\\) denote a symbol, (it could be a terminal or a non-terminal). Let \\(\\overline{\\sigma}\\) denote a sequence of symbols. Given a grammar \\(G\\) we define the following functions \\(null(\\overline{\\sigma},G)\\), \\(first(\\overline{\\sigma},G)\\) and \\(follow(\\sigma, G)\\)</p> <p>\\(null(\\overline{\\sigma},G)\\) checks whether the language denoted by \\(\\overline{\\sigma}\\) contains the empty sequence.</p> \\[ \\begin{array}{rcl} null(t,G) &amp; = &amp; false \\\\ null(\\epsilon,G) &amp; = &amp; true \\\\ null(N,G) &amp; = &amp; \\bigvee_{N::=\\overline{\\sigma} \\in G} null(\\overline{\\sigma},G) \\\\ null(\\sigma_1...\\sigma_n,G) &amp; = &amp; null(\\sigma_1,G) \\wedge ... \\wedge null(\\sigma_n,G) \\end{array} \\] <p>\\(first(\\overline{\\sigma},G)\\) computes the set of leading terminals from the language denoted by \\(\\overline{\\sigma}\\).</p> \\[ \\begin{array}{rcl} first(\\epsilon, G) &amp; = &amp; \\{\\} \\\\ first(t,G) &amp; = &amp; \\{t\\} \\\\ first(N,G) &amp; = &amp; \\bigcup_{N::=\\overline{\\sigma} \\in G} first(\\overline{\\sigma},G) \\\\ first(\\sigma\\overline{\\sigma},G) &amp; = &amp;   \\left [     \\begin{array}{ll}       first(\\sigma,G) \\cup first(\\overline{\\sigma},G) &amp; {\\tt if}\\ null(\\sigma,G) \\\\       first(\\sigma,G) &amp; {\\tt otherwise}       \\end{array}   \\right . \\end{array} \\] <p>\\(follow(\\sigma,G)\\) finds the set of terminals that immediately follows symbol \\(\\sigma\\) in any derivation derivable from \\(G\\).</p> \\[ \\begin{array}{rcl} follow(\\sigma,G) &amp; = &amp; \\bigcup_{N::=\\overline{\\sigma}\\sigma{\\overline{\\gamma}} \\in G}   \\left [     \\begin{array}{ll}       first(\\overline{\\gamma}, G) \\cup follow(N,G) &amp; {\\tt if}\\ null(\\overline{\\gamma}, G) \\\\       first(\\overline{\\gamma}, G) &amp; {\\tt otherwise}     \\end{array}   \\right . \\end{array} \\] <p>Sometimes, for convenience we omit the second parameter \\(G\\).</p> <p>For example, let \\(G\\) be Grammar 6, then</p> \\[ \\begin{array}{l} null(E) = null(TE') = null(T) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+TE') \\vee null(\\epsilon) = null(+TE') \\vee true = true \\\\ null(T) = null(i) = false \\\\ \\\\ first(E) = first(TE') = first(T) =  \\{i\\} \\\\ first(E') = first(+TE') \\cup first(\\epsilon) = first(+TE') = \\{+\\} \\\\ first(T) = \\{i\\} \\\\ \\\\ follow(E) = \\{\\} \\\\ follow(E') = follow(E) \\cup follow(E') = \\{\\} \\cup follow(E') \\\\ follow(T) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\end{array} \\] <p>When computing \\(follow(E')\\) and \\(follow(T)\\) we encounter an infinite sequence of \\(\\cup follow(E')\\) which leads to a fix-point. That is, an infinite sequence of repeated operation that does not add any value to the existing result. We can conclude that \\(follow(E') = \\{\\}\\) and \\(follow(T) = \\{+\\}\\).</p> <p>We will discuss fix-point in-depth in some lesson later.</p> <p>Given \\(null\\), \\(first\\) and \\(follow\\) computed, we can construct a predictive parsing table to check whether the grammar is in <code>LL(k)</code>. For simplicity, we check the case <code>k = 1</code>, we construct the following predictive parsing table where each row is indexed a non-terminal, and each column is indexed by a terminal.</p> i + E E' T <p>For each production rule \\(N ::= \\overline{\\sigma}\\), we put the production rule in</p> <ul> <li>cell \\((N,t)\\) if \\(t \\in first(\\overline{\\sigma})\\)</li> <li>cell \\((N,t')\\) if \\(null(\\overline{\\sigma})\\) and \\(t' \\in follow(N)\\)</li> </ul> <p>We fill up the table</p> i + E E ::= TE' E' E' ::= + TE' T T ::= i <p>We conclude that a grammar is in <code>LL(1)</code> if it contains no conflicts. A conflict arises when there are more than one production rule to be applied given a non-terminal and a leading symbol. Given a <code>LL(1)</code> grammar, we can perform predictive top-down parsing by selecting the right production rule by examining the leading input symbol.</p> <p>In general, there are two kinds of conflicts found in grammar that violates the <code>LL(1)</code> grammar requirements.</p> <ol> <li>first-first conflict</li> <li>first-follow conflict</li> </ol>"},{"location":"syntax_analysis/#first-first-conflict","title":"First-first Conflict","text":"<p>Consider the grammar</p> <pre><code>&lt;&lt;Grammar 9&gt;&gt;\nS ::= Xb\nS ::= Yc\nX ::= a\nY ::= a \n</code></pre> <p>We compute \\(null\\), \\(first\\) and \\(follow\\).</p> \\[ \\begin{array}{l} null(S) = null(Xb) = false \\\\ null(X) = null(a) = false \\\\ null(Y) = null(a) = false \\\\ \\\\ first(S) = first(Xb) \\cup first(Yc) = \\{a\\} \\\\ first(X) = first(a) = \\{a \\} \\\\ first(Y) = first(a) = \\{a \\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{b \\} \\\\ follow(Y) = \\{c \\} \\end{array} \\] <p>We fill up the following predictive parsing table</p> a b c S S::=Xb, S::=Yc X X::=a Y Y::=a <p>From the above we find that there are two production rules in the cell <code>(S,a)</code>, namely <code>S::=Xb</code>, and <code>S::=Yc</code>. This is a first-first conflict, since both production rules' first set contains <code>a</code>. This prevents us from constructing a predictable parser by observing the leading symbol from the input.</p> <p>First-first conflict can be eliminated by applying left-factoring (not to be confused with left recursion).</p> <p>From our running example, we find that the cell <code>(S,a)</code> has more than one production rule applicable. This is caused by the fact that both <code>X::=a</code> and <code>Y::=a</code> start with the non-terminal <code>a</code>. We could apply substitution to eliminate <code>X</code> and <code>Y</code>.</p> <pre><code>&lt;&lt;Grammar 10&gt;&gt;\nS ::= ab\nS ::= ac\n</code></pre> <p>Then we could introduce a new non-terminal <code>Z</code> which capture the following languages after <code>a</code>.</p> <pre><code>&lt;&lt;Grammar 11&gt;&gt;\nS ::= aZ\nZ ::= b\nZ ::= c\n</code></pre>"},{"location":"syntax_analysis/#first-follow-conflict","title":"First-Follow Conflict","text":"<p>Consider the following grammar</p> <pre><code>&lt;&lt;Grammar 12&gt;&gt;\nS ::= Xd \nX ::= C \nX ::= Ba\nC ::= epsilon\nB ::= d\n</code></pre> <p>and the \\(null\\), \\(first\\) and \\(follow\\) functions</p> \\[ \\begin{array}{l} null(S) = null(Xd) = null(X) \\wedge null(d) = false \\\\ null(X) = null(C) \\vee null(Ba) = true \\\\ null(C) = null(\\epsilon) = true \\\\ null(B) = null(d) = false \\\\ \\\\ first(S) = first(Xd) = first(X) \\cup first(d) = \\{d\\} \\\\ first(X) = first(C) \\cup first(Ba) = \\{d\\} \\\\ first(C) = \\{\\}\\\\ first(B) = \\{d\\} \\\\ \\\\ follow(S) = \\{\\} \\\\ follow(X) = \\{d\\} \\\\ follow(C) = follow(X) = \\{d\\} \\\\ follow(B) = \\{d\\} \\end{array} \\] <p>We construct the predictive parsing table as follows</p> a d S S::=Xd X X::=Ba, X::=C(S::=Xd) C C::=epsilon (S::=Xd) B B::=d <p>In the cell of <code>(X,d)</code> we find two production rules <code>X::=Ba</code> and <code>X::=C (S::=Xd)</code>. It is a first-follow conflict, because the first production rule is discovered through the <code>first(X)</code> set and the second one is from the <code>follow(X)</code> set.</p> <p>Since first-follow conflicts are introduced by epsilon production rule, we could apply substitution to eliminate first-follow conflicts</p> <p>Substitute <code>B</code> and <code>C</code></p> <pre><code>&lt;&lt;Grammar 13&gt;&gt;\nS ::= Xd \nX ::= epsilon \nX ::= da\n</code></pre> <p>Substitute <code>X</code></p> <pre><code>&lt;&lt;Grammar 14&gt;&gt;\nS ::= d\nS ::= dad\n</code></pre> <p>However, as we can observe, eliminating first-follow conflict by substitution might introduce a new first-first conflict.</p>"},{"location":"syntax_analysis/#to-ll1-or-not-ll1","title":"To <code>LL(1)</code> or not <code>LL(1)</code>","text":"<p>Given a grammar, we could get a <code>LL(1)</code> grammar equivalent in most of the cases.</p> <ol> <li>Disambiguate the grammar if it is ambiguous</li> <li>Eliminate the left recursion</li> <li>Apply left-factoring if there exists some first-first conflict</li> <li>Apply substitution if there exists some first-follow conflict</li> <li>repeat 3 if first-first conflict is introduced</li> </ol> <p>Step 1 is often done manually, (there is no general algorithm to do so.) Steps 2-4 (and 5) can be automated by some algorithm.</p> <p>Let's consider another example (a subset of Grammar 3).</p> <pre><code>&lt;&lt;Grammar 15&gt;&gt;\nE ::= E + E\nE ::= i\n</code></pre> <p>Note that this grammar is ambiguous. Let's suppose we skip this step and directly eliminate the left-recursion</p> <pre><code>&lt;&lt;Grammar 16&gt;&gt;\nE ::= iE'\nE' ::= + EE'\nE' ::= epsilon\n</code></pre> <p>Next we compute the predictive parsing table.</p> \\[ \\begin{array}{l} null(E) = null(iE) = null(i) \\wedge null(E') = false \\wedge null(E') = false \\\\ null(E') = null(+EE') \\vee null(\\epsilon) = null(+E') \\vee true = true \\\\ \\\\ first(E) = first(iE') = \\{i\\} \\\\ first(E') = first(+EE') \\cup first(\\epsilon) = first(+EE') = \\{+\\} \\\\ \\\\ follow(E) = first(E') \\cup follow(E') = \\{+\\} \\cup follow(E') \\\\ follow(E') = follow(E) \\cup follow(E') = \\{+\\} \\cup follow(E') \\cup follow(E') \\end{array} \\] i + E E::= iE' E' E'::= +EE', E'::= epsilon <p>As shown from the above, at a glance, we argue that one of the  cell contains two production rules. We might argue that the grammar contains a first-follow conflict.  However, this grammar is LL(1), because when we are checking for  first-follow conflicts, with <code>E'::= epsilon</code> rule, we need to look up what is the \\(Follow\\) set of <code>E'</code>. In this case, it is <code>+</code>, and there is only one production rule in the grammar starting with <code>+</code>, which is  <code>E' ::= +EE'</code>. Hence there is no conflict in this grammar. </p>"},{"location":"syntax_analysis/#a-short-summary-so-far-for-top-down-recursive-parsing","title":"A short summary so far for top-down recursive parsing","text":"<p>Top-down parsing is simple, however might be inefficient.</p> <p>We need to rewrite the grammar into a more specific (a subset) if the grammar is ambiguous. No general algorithm exists.</p> <p>We need to eliminate left recursion so that the parsing will terminate.</p> <p>We construct the predictive parsing table to check whether the grammar is in <code>LL(k)</code>. If the grammar is in <code>LL(k)</code> we can always pick the right production rule given the first <code>k</code> leading symbols from the input.</p> <p>For most of the cases, <code>LL(1)</code> is sufficient for practical use.</p> <p>We also can conclude that a <code>LL(k+1)</code> grammar is also a <code>LL(k)</code> grammar, but the other way does not hold.</p> <p>Given a particular <code>k</code> and a grammar <code>G</code>, we can check whether <code>G</code> is <code>LL(k)</code>. However given a grammar <code>G</code> to find a <code>k</code> such that <code>G</code> is <code>LL(k)</code> is undecidable.</p>"},{"location":"syntax_analysis/#summary","title":"Summary","text":"<p>We have covered</p> <ul> <li>The roles and functionalities of lexers and parsers in a compiler pipeline</li> <li>There are two major types of parser, top-down parsing and bottom-up parsing</li> <li>How to eliminate left-recursion from a grammar,</li> <li>How to apply left-factoring</li> <li>How to construct a <code>LL(1)</code> predictive parsing table</li> </ul>"},{"location":"syntax_analysis_2/","title":"50.054 - Syntax Analysis 2","text":""},{"location":"syntax_analysis_2/#learning-outcome","title":"Learning Outcome","text":"<p>By the end of this lesson, you should be able to</p> <ol> <li>Construct a <code>LR(0)</code> parsing table</li> <li>Explain shift-reduce conflict</li> <li>Construct a <code>SLR</code> parsing table</li> </ol>"},{"location":"syntax_analysis_2/#bottom-up-parsing","title":"Bottom-up parsing","text":"<p>An issue with <code>LL(k)</code> parsing is that we always need to make sure that we can pick the correct production rule by examining the first <code>k</code> tokens from the input. There is always a limit of how many tokens we should look ahead to pick a particular production rule without relying on backtracking.</p> <p>What if we consider multiple production rules when \"consuming\" input tokens and decide which one to pick when we have enough information? Answering this question leads to bottom-up parsing.</p> <p><code>LR(k)</code> stands for left-to-right, right-most derivation with <code>k</code> lookahead tokens.</p> <p>In essence, <code>LR(k)</code> relies on a parsing table and a stack to decide which production rule to be applied given the current (partial) input. A stack is storing the symbols have been consumed so far, each element in the stack also stores the state of the parser.</p> <p>To understand <code>LR(k)</code> parsing, let's assume that we are given the parsing table. (We will consider how to construct the parsing table shortly.)</p> <p>Let's recall Grammar 6</p> <pre><code>&lt;&lt;Grammar 6&gt;&gt;\n1 S' ::= E$ \n2 E ::= TE'\n3 E' ::= + TE'\n4 E' ::= epsilon\n5 T ::= i\n</code></pre> <p>We added number to each production rule, and we introduce a top level production rule <code>S' ::= E$</code> where <code>$</code> denotes the end of input symbol.</p> <p>Let's consider the following parsing table for Grammar 6.</p> + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 reduce 4 goto 16 16 reduce 3 17 reduce 2 <p>Each cell in the above table is indexed by a symbol of the grammar, and a state. To avoid confusion with the production rule IDs,  we assume that state IDs are having 2 digits, and state <code>10</code> is the starting state. In each cell, we find a set of parsing actions.</p> <ol> <li><code>shift s</code> where <code>s</code> dentes a state ID. Given <code>shift s</code> in a cell (<code>s'</code>, <code>t</code>), we change the parser state from <code>s'</code> to <code>s</code> and consume the leading token <code>t</code> from the input and store it in the stack.</li> <li><code>accept</code>. Given <code>accept</code> found in a cell (<code>s</code>, <code>$</code>), the parsing is completed successfully.</li> <li><code>goto s</code> where <code>s</code> denotes a state ID. Given <code>goto s</code> in a cell (<code>s'</code>, <code>t</code>), we change the parser's state to <code>s</code>.</li> <li><code>reduce p</code> where <code>p</code> denotes a production rule ID. Given <code>reduce p</code> in a cell (<code>s</code>, <code>t</code>), lookup production rule <code>LHS::=RHS</code> from the grammar by <code>p</code>. We pop the items from top of the stack by reversing <code>RHS</code>. Given the state of the current top element of the stack, let's say <code>s'</code>, we lookup the goto action in cell (<code>s'</code>, <code>LHS</code>) and push <code>LHS</code> to the stack and perform the goto action.</li> </ol> <p>Consider the parsing the input <code>1+2+3</code></p> stack input action rule (10) 1+2+3$ shift 13 (10) i(13) +2+3$ reduce 5 T::=i (10) T(11) +2+3$ shift 14 (10) T(11) +(14) 2+3$ shift 13 (10) T(11) +(14) i(13) +3$ reduce 5 T::=i (10) T(11) +(14) T(15) +3$ shift 14 (10) T(11) +(14) T(15) +(14) 3$ shift 13 (10) T(11) +(14) T(15) +(14) i(13) $ reduce 5 T::=i (10) T(11) +(14) T(15) +(14) T(15) $ reduce 4 E'::=epsilon (10) T(11) +(14) T(15) +(14) T(15) E' (16) $ reduce 3 E'::=+TE' (10) T(11) +(14) T(15) E'(16) $ reduce 3 E'::=+TE' (10) T(11) E'(17) $ reduce 2 E::=TE' (10) E(12) $ accept S'::=E$ <p>We start with state (10) in the stack.</p> <ol> <li>Given the first token from the input is <code>1</code> (i.e. an <code>i</code> token), we look up the parsing table and find the <code>shift 13</code> action in cell (<code>10</code>, <code>i</code>). By executing this action, we push <code>i(13)</code> in the stack.</li> <li>The next input is <code>+</code>. Given the current state is (13), we apply the smae strategy to find action <code>reduce 5</code> in cell (<code>13</code>, <code>+</code>). Recall that the production rule with id 5 is <code>T::=i</code>, we pop the <code>i(13)</code> from the stack, and check for the correspondent action in cell (<code>10</code>, <code>T</code>), we find <code>goto 11</code>. Hence we push <code>T(11)</code> into the stack.</li> </ol> <p>We follow the remaining steps to parse the input when we meet the accept action.</p> <p>One interesting observation is that the order of the rules found in the <code>rule</code> column is the reverse order of the list of rules we used in <code>LL(k)</code> parsing.</p> <p>Next we consider how to construct the parsing tables. It turns out that there are multiple ways of construct the parsing tables for <code>LR(k)</code> grammars.</p>"},{"location":"syntax_analysis_2/#lr0-parsing","title":"LR(0) Parsing","text":"<p>We first consider the simplest parsing table where we ignore the leading token from the input, <code>LR(0)</code>.</p> <p>The main idea is that the actions (which define the change and update of the state and stack) are output based on the current state and the current stack. If we recall that this is a form of state machine.</p> <p>From this point onwards, we use pseudo Scala syntax illustrate the algorithm behind the parsing table construction.</p> <p>Let <code>.</code> denote a meta symbol which indicate the current parsing context in a production rule.</p> <p>For instance for production rule 3 <code>E' ::= +TE'</code>, we have four possible contexts</p> <ul> <li><code>E' ::= .+TE'</code></li> <li><code>E' ::= +.TE'</code></li> <li><code>E' ::= +T.E'</code></li> <li><code>E' ::= +TE'.</code></li> </ul> <p>We call each of these possible contexts an <code>Item</code>.</p> <p>We define <code>Items</code> to be a set of <code>Item</code>s, <code>Grammar</code> to be a set of production rules (whose definition is omitted, we use the syntax <code>LHS::=RHS</code> directly in the pseudo-code.)</p> <pre><code>type Items = Set[Item]\ntype Grammar = Set[Prod]\n</code></pre> <p>We consider the following operations.</p> <pre><code>def closure(I:Items)(G:Grammar):Items = { \n  val newItems = for {\n    (N ::= alpha . X beta) &lt;- I\n    (X ::= gamma)          &lt;- G\n  } yield ( X::= . gamma ).union(\n    for {\n      (N ::= . epsilon ) &lt;- I\n    } yield ( N::= epsilon .)\n  )\n  if (newItems.forall(newItem =&gt; I.contains(newItem)))\n  { I }\n  else { closure(I.union(newItems))(G)}\n\ndef goto(I:Items)(G:Grammar)(sym:Symbol):Items = {\n  val J = for {\n    (N ::= alpha . X beta) &lt;- I\n  } yield (N ::= alpha X . beta)\n  closure(J)(G)\n}\n</code></pre> <p>Function <code>closure</code> takes an item set <code>I</code> and a grammar then returns the closure of <code>I</code>. For each item of shape <code>N::=alpha . X beta</code> in <code>I</code>, we look for the correspondent production rule <code>X ::= gamma</code> in <code>G</code> if <code>X</code> is a non-terminal, add <code>X::= . gamma</code> to the new item sets if it is not yet included in <code>I</code>.</p> <p>Noe that Scala by default does not support pattern such as <code>(N ::= alpha . X beta)</code> and <code>(X::= gamma)</code>. In this section, let's pretend that these patterns are allowed in Scala so that we can explain the algorithm in Scala style pseudo-code.</p> <p>Function <code>goto</code> takes an item set <code>I</code> and searches for item inside of shape <code>N::= alpha . X beta</code> then add <code>N::=alpha X. beta</code> as the next set <code>J</code>. We compute the closure of <code>J</code> and return it as result.</p> <pre><code>type State = Items\ntype Transition = (State, Symbol, State)\ncase class StateMachine(states:Set[State], transitions:Set[Transition], accepts:Set[State]) \n\n\ndef buildSM(init:State)(G:Grammar):StateMachine = { \n  def step1(states:Set[State])(trans:Set[Transition]):(Set[State], Set[Transition]) = { // compute all states and transitions\n    val newStateTrans = for {\n      I                      &lt;- states\n      (A ::= alpha . X beta) &lt;- I \n      J                      &lt;- pure(goto(I)(G)(X))\n    } yield (J, (I,X,J))\n    if newStateTrans.forall( st =&gt; st match {\n      case (new_state, _) =&gt; states.contains(new_state)\n    }) { (states, trans) }\n    else {\n      val newStates = newStateTrans.map( x =&gt; x._1) \n      val newTrans  = newStateTrans.map( x =&gt; x._2) \n      step1(states.union(newStates))(trans.union(newTrans))\n      }\n  }\n  def step2(states:Set[State]):Set[State] = { // compute all final states\n    states.filter( I =&gt; I.exists( item =&gt; item match {\n      case (N ::= alpha . $) =&gt; true \n      case _ =&gt; false\n    }))\n  }\n  step1(Set(init))(Set()) match {\n    case (states, trans) =&gt; {\n      val finals = step2(states)\n      StateMachine(states, trans, finals)\n    }\n  }\n}\n</code></pre> <p>Function <code>buildSM</code> consists of two steps. In <code>step1</code> we start with the initial state <code>init</code> and compute all possible states and transitions by applying <code>goto</code>. In <code>step2</code>, we compute all final states.</p> <p>By applying <code>buildSM</code> to Grammar 6 yields the following state diagram.</p> <pre><code>\ngraph\n  State10[\"State10 &lt;br/&gt; S'::=.E$ &lt;br/&gt; E::=.TE' &lt;br/&gt; T::=i \"]--T--&gt;State11[\"State11 &lt;br/&gt; E::=T.E' &lt;br/&gt; E'::=+TE' &lt;br/&gt; E'::= . epsilon &lt;br/&gt; E'::= epsilon.\"]\n  State10--E--&gt;State12[\"State12 &lt;br/&gt; S'::=E.$\"]\n  State10--i--&gt;State13[\"State13 &lt;br/&gt; T::=i.\"]\n  State11--+--&gt;State14[\"State14 &lt;br/&gt; E'::= +.TE' &lt;br/&gt; T::=.i\"]\n  State11--E'--&gt;State17[\"State17 &lt;br/&gt; S'::=E.$\"]\n  State14--i--&gt;State13\n  State14--T--&gt;State15[\"State15 &lt;br/&gt; E'::= +T.E' &lt;br/&gt; E'::=.+TE' &lt;br/&gt; E'::=.epsilon &lt;br/&gt; E'::=epsilon . \"]\n  State15--+--&gt;State14\n  State15--E'--&gt;State16[\"State16 &lt;br/&gt; E'::=+TE'.\"]</code></pre> <pre><code>def reduce(states:List[State]):List[(Items, Prod)] = {\n  states.foldLeft(List())((accI:(List[(Items,Prod)], Items)) =&gt; accI match {\n    case (acc,I) =&gt; I.toList.foldLeft(acc)( ai:(List[(Items,Prod)], Item)) =&gt; ai match {\n      case (a, ( N::= alpha .)) =&gt; a.append(List((I, N::=alpha)))\n      case (a, _) =&gt; a \n    } \n  }\n}\n</code></pre> <p>Function <code>reduce</code> takes a list of states and search for item set that contains an item of shape <code>N::= alpha .</code>.</p> <pre><code>enum Action {\n  case Shift(i:State)\n  case Reduce(p:Prod)\n  case Accept\n  case Goto(i:State)\n}\n\ndef ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match {\n  case (S::= X$) =&gt; {\n    val init = Set(closure(Set(S ::=.X$))(G))\n    buildSM(init)(G) match {\n      case StateMachine(states, trans, finals) =&gt; {\n        val shifts  = for {\n          (I, x, J) &lt;- trans\n          if isTerminal(x)\n        } yield (I, x, Shift(J))\n        val gotos   = for {\n          (I, x, J) &lt;- trans\n          if !isTerminal(x)\n          yield (I, x, Goto(J)))\n        }\n        val reduces = for {\n          (I, N::=alpha) &lt;- reduce(states)\n          x &lt;- allTerminals(G)\n        } yield (I, x, Reduce(N::=alpha))\n        val accepts = for {\n          I &lt;- finals\n        } yield (I, $, Accept)\n        shifts ++ gotos ++ reduces ++ accepts\n      }\n    }\n  } \n}\n</code></pre> <p>Function <code>ptable</code> computes the <code>LR(0)</code> parsing table by making use of the functions defined earlier.</p> <p>Applying <code>ptable</code> to Grammar 6 yields</p> + i $ S' E E' T 10 shift 13 goto 12 goto 11 11 shift 14 / reduce 4 reduce 4 reduce 4 goto 17 12 accept 13 reduce 5 reduce 5 14 shift 13 goto 15 15 shift 14 / reduce 4 reduce 4 reduce 4 goto 16 16 reduce 3 reduce 3 reduce 3 17 reduce 3 reduce 3 reduce 2 <p>The above parsing table is generated by filling up the cells based on the state machine diagram by differentiating the transition via a terminal symbol (<code>shift</code>) and a non-terminal symbol (<code>goto</code>).</p>"},{"location":"syntax_analysis_2/#slr-parsing","title":"SLR parsing","text":"<p>One issue with the above <code>LR(0)</code> parsing table is that we see conflicts in cells with multiple actions, e.g. cell (<code>11</code>, <code>+</code>). This is also known as the shift-reduce conflict. It is caused by the over-approximation of the <code>ptable</code> function. In the <code>ptable</code> function, we blindly assign reduce actions to current state w.r.t. to all symbols.</p> <p>A simple fix to this problem is to consider only the symbols that follows the LHS non-terminal.</p> <pre><code>def reduce(states:List[State]):List[(Items, Symbol, Prod)] = {\n  states.foldLeft(List())((accI:(List[(Items, Symbol,  Prod)], Items)) =&gt; accI match {\n    case (acc,I) =&gt; I.toList.foldLeft(acc)( ai:(List[(Items, Symbol,  Prod)], Item)) =&gt; ai match {\n      case (a, ( N::= alpha .)) =&gt; a ++ (follow(N).map( s =&gt; (I, s N::=alpha))) // fix\n      case (a, _) =&gt; a \n    } \n  }\n}\n\ndef ptable(G:Grammar)(prod:Prod):List[(State, Symbol, Action)] = prod match {\n  case (S::= X$) =&gt; {\n    val init = Set(closure(Set(S ::=.X$))(G))\n    buildSM(init)(G) match {\n      case StateMachine(states, trans, finals) =&gt; {\n        val shifts  = for {\n          (I, x, J) &lt;- trans\n          if isTerminal(x)\n        } yield (I, x, Shift(J))\n        val gotos   = for {\n          (I, x, J) &lt;- trans\n          if !isTerminal(x)\n          yield (I, x, Goto(J)))\n        }\n        val reduces = for {\n          (I, x, N::=alpha) &lt;- reduce(states)\n        } yield (I, x, Reduce(N::=alpha)) // fix\n        val accepts = for {\n          I &lt;- finals\n        } yield (I, $, Accept)\n        shifts ++ gotos ++ reduces ++ accepts\n      }\n    }\n  } \n}\n</code></pre> <p>Given this fix, we are able to generate the conflict-free parsing table that we introduced earlier in this section.</p>"},{"location":"syntax_analysis_2/#lr1-parsing-bonus-materials","title":"LR(1) Parsing (Bonus materials)","text":"<p>Besides <code>SLR</code>, <code>LR(1)</code> parsing also eliminates many conflicts found in <code>LR(0)</code>. The idea is to re-define item to include the look ahead token.</p> <p>For instance for production rule 3 <code>E' ::= +TE'</code>, we have 12 possible items</p> <ul> <li>(<code>E' ::= .+TE'</code>, <code>+</code>)</li> <li>(<code>E' ::= +.TE'</code>, <code>+</code>)</li> <li>(<code>E' ::= +T.E'</code>, <code>+</code>)</li> <li>(<code>E' ::= +TE'.</code>, <code>+</code>)</li> <li>(<code>E' ::= .+TE'</code>, <code>i</code>)</li> <li>(<code>E' ::= +.TE'</code>, <code>i</code>)</li> <li>(<code>E' ::= +T.E'</code>, <code>i</code>)</li> <li>(<code>E' ::= +TE'.</code>, <code>i</code>)</li> <li>(<code>E' ::= .+TE'</code>, <code>$</code>)</li> <li>(<code>E' ::= +.TE'</code>, <code>$</code>)</li> <li>(<code>E' ::= +T.E'</code>, <code>$</code>)</li> <li>(<code>E' ::= +TE'.</code>, <code>$</code>)</li> </ul> <p>We adjust the definition of <code>closure</code> and <code>goto</code></p> <pre><code>def closure(I:Items)(G:Grammar):Items = { \n  val newItems = for {\n    (N ::= alpha . X beta, t) &lt;- I\n    (X ::= gamma)             &lt;- G\n    w                         &lt;- first(beta t)\n  } yield ( X::= . gamma, w).union(\n    for {\n      (N ::= . epsilon, t) &lt;- I\n    } yield ( N::= epsilon ., t)\n  )\n  if (newItems.forall(newItem =&gt; I.contains(newItem)))\n  { I }\n  else { closure(I.union(newItems))(G)}\n\ndef goto(I:Items)(G:Grammar)(sym:Symbol):Items = {\n  val J = for {\n    (N ::= alpha . X beta, t) &lt;- I\n  } yield (N ::= alpha X . beta, t)\n  closure(J)(G)\n}\n</code></pre> <p>When computing the closure of an item <code>(N ::= alpha . X beta, t)</code>, we look up production rule <code>X ::= gamma</code>, to add <code>X ::= .gamma</code> into the new item set, we need to consider the possible leading terminal tokens coming from <code>beta</code>, and <code>t</code> in case <code>beta</code> accepts epsilon.</p> <p>Applying the adjusted definition, we have the follow state diagram</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.E$, ? &lt;br/&gt; E::=.TE', $ &lt;br/&gt; T::=i, $ &lt;br/&gt; T::=i, + \"]--T--&gt;State11[\"State11 &lt;br/&gt; E::=T.E', $ &lt;br/&gt; E'::=+TE', $ &lt;br/&gt; E'::= . epsilon, $ &lt;br/&gt; E'::= epsilon., $\"]\n  State10--E--&gt;State12[\"State12 &lt;br/&gt; S'::=E.$, ?\"]\n  State10--i--&gt;State13[\"State13 &lt;br/&gt; T::=i., + &lt;br/&gt; T::=i., $\"]\n  State11--+--&gt;State14[\"State14 &lt;br/&gt; E'::= +.TE', $ &lt;br/&gt; T::=.i, + \"]\n  State11--E'--&gt;State17[\"State17 &lt;br/&gt; S'::=E.$, ?\"]\n  State14--i--&gt;State13\n  State14--T--&gt;State15[\"State15 &lt;br/&gt; E'::= +T.E', $ &lt;br/&gt; E'::=.+TE', $ &lt;br/&gt; E'::=.epsilon, $ &lt;br/&gt; E'::=epsilon., $\"]\n  State15--+--&gt;State14\n  State15--E'--&gt;State16[\"State16 &lt;br/&gt; E'::=+TE'., $\"]</code></pre> <p>For the top-most production rule, there is no leading token, we put a special symbol <code>?</code>, which does not affect the parsing.</p> <p>To incorporate item's new definition, we adjust the <code>reduce</code> function as follows</p> <pre><code>def reduce(states:List[State]):List[(Items, Symbol, Prod)] = {\n  states.foldLeft(List())((accI:(List[(Items,Symbol, Prod)], Items)) =&gt; accI match {\n    case (acc,I) =&gt; I.toList.foldLeft(acc)( ai:(List[(Items, Symbol, Prod)], Item)) =&gt; ai match {\n      case (a, ( N::= alpha ., t)) =&gt; a.append(List((I, t, N::=alpha)))\n      case (a, _) =&gt; a \n    } \n  }\n}\n</code></pre> <p><code>buildSM</code> and <code>ptable</code> function remain unchanged as per <code>SLR</code> parsing.</p> <p>By applying <code>ptable</code> we obtain the same parsing table as <code>SLR</code> parsing.</p>"},{"location":"syntax_analysis_2/#slr-vs-lr1","title":"SLR vs LR(1)","text":"<p><code>LR(1)</code> covers a larger set of grammar than <code>SLR</code>. For example consider the following grammar.</p> <pre><code>&lt;&lt;Grammar 16&gt;&gt;\n1 S' ::= S$\n2 S ::= A a \n3 S ::= b A c\n4 S ::= d c \n5 S ::= b d a\n6 A ::= d\n</code></pre> <p><code>SLR</code> produces the following state diagram and parsing table.</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.S$&lt;br/&gt; S::=.Aa &lt;br/&gt; S::= .bAc&lt;br/&gt; S::=.dc &lt;br/&gt; S::=.bda &lt;br/&gt; A::=.d \"]--S--&gt;State11[\"State11 &lt;br/&gt; S'::=S.$\"]\n  State10--A--&gt;State12[\"State12 &lt;br/&gt; S::A.a\"]\n  State10--b--&gt;State13[\"State13 &lt;br/&gt; S::=b.Ac&lt;br/&gt; S::=b.da &lt;br/&gt; A::=.d\"]\n  State10--d--&gt;State14[\"State14 &lt;br/&gt; S::= d.c &lt;br/&gt; A::=d.\"]\n  State12--a--&gt;State15[\"State15 &lt;br/&gt; S::=Aa.\"]\n  State13--A--&gt;State16[\"State16 &lt;br/&gt; S::=bA.c\"]\n  State13--d--&gt;State17[\"State17 &lt;br/&gt; A::=d. &lt;br/&gt; S::=bd.a\"]\n  State14--c--&gt;State18[\"State18 &lt;br/&gt; S::=dc.\"]\n  State16--c--&gt;State19[\"State19 &lt;br/&gt; S::=bAc.\"]\n  State17--a--&gt;State20[\"State20 &lt;br/&gt; S::=bda.\"]  </code></pre> a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 / reduce 6 15 reduce 2 16 shift 19 17 shift 20 / reduce 6 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 <p>There exist shift-reduce conflict. This is because in the closure computation when the item  <code>X::= . gamma</code> is added to the closure, we approximate the next leading token by <code>follow(X)</code>. However there might be other alternative production rule for <code>X</code> in the grammar. This introduces extraneous reduce actions.</p> <p><code>LR(1)</code> produces the following state diagram and parsing table.</p> <pre><code>graph\n  State10[\"State10 &lt;br/&gt; S'::=.S$, ? &lt;br/&gt; S::=.Aa, $ &lt;br/&gt; S::= .bAc, $ &lt;br/&gt; S::=.dc, $ &lt;br/&gt; S::=.bda, $ &lt;br/&gt; A::=.d, a \"]--S--&gt;State11[\"State11 &lt;br/&gt; S'::=S.$, ?\"]\n  State10--A--&gt;State12[\"State12 &lt;br/&gt; S::A.a, $\"]\n  State10--b--&gt;State13[\"State13 &lt;br/&gt; S::=b.Ac, $&lt;br/&gt; S::=b.da, $ &lt;br/&gt; A::=.d, c\"]\n  State10--d--&gt;State14[\"State14 &lt;br/&gt; S::= d.c, $ &lt;br/&gt; A::=d., a\"]\n  State12--a--&gt;State15[\"State15 &lt;br/&gt; S::=Aa., $\"]\n  State13--A--&gt;State16[\"State16 &lt;br/&gt; S::=bA.c,$\"]\n  State13--d--&gt;State17[\"State17 &lt;br/&gt; A::=d.,c &lt;br/&gt; S::=bd.a, $\"]\n  State14--c--&gt;State18[\"State18 &lt;br/&gt; S::=dc., $\"]\n  State16--c--&gt;State19[\"State19 &lt;br/&gt; S::=bAc., $\"]\n  State17--a--&gt;State20[\"State20 &lt;br/&gt; S::=bda., $\"]  </code></pre> a b c d $ S' S A 10 shift 13 shift 14 goto 11 goto 12 11 accept 12 shift 15 13 shift 17 goto 16 14 reduce 6 shift 18 15 reduce 2 16 shift 19 17 shift 20 reduce 6 18 reduce 4 19 reduce 3 20 reduce 5 <p>In which the shift-reduce conflicts are eliminated because when given an item <code>(N ::= alpha . X beta, t)</code>, we add <code>X ::= . gamma</code> into the closure, by computing <code>first(beta t)</code>. This is only specific to this production rule <code>X::= gamma</code> and not other alternative.  </p>"},{"location":"syntax_analysis_2/#lr1-and-left-recursion","title":"LR(1) and left recursion","text":"<p><code>LR(1)</code> can't handle all grammar with left recursion. For example processing Grammar 5  (from the previous unit) with <code>LR(1)</code> will result in some shift-reduce conflict.</p>"},{"location":"syntax_analysis_2/#summary","title":"Summary","text":"<p>We have covered</p> <ul> <li>How to construct a <code>LR(0)</code> parsing table</li> <li>How to construct a <code>SLR</code> parsing table</li> </ul>"},{"location":"syntax_analysis_annex/","title":"Syntax analysis annex","text":"Rule   Parse tree   Symbols   Input   (5)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]  {  NS }  {  ' k  1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]     NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   (8)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N,NS }    ' k  1 ' : 1 , ' k 2 ' : [ ] }   (10)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;s   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  ' s':J, NS }  ' k  1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;s   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  s':J, NS }  k 1 ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  ':J, NS }  ' : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]  :J, NS }  : 1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J];   J, NS }   1 , ' k 2 ' : [ ] }   (1)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i;  i, NS }  1 , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"];  , NS }  , ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"];   NS }   ' k 2 ' : [ ] }   (9)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N];   N }   ' k 2 ' : [ ] }   (10)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[s]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  's':J }  ' k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[s]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  s':J }  k 2 ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  ':J }  ' : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];  :J }  : [ ] }   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J];   J }   [ ] }   (3)   graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];  [ ] } [ ] }  graph   J--&gt;LB[\"{\"]   J--&gt;NS   J--&gt;RB[\"}\"]   NS--&gt;N   NS--&gt;,   NS--&gt;NS2[\"NS\"]   N--&gt;LQ[']   N--&gt;S1(\"s(k1)\")   N--&gt;RQ[']   N--&gt;:   N--&gt;J2[J]   J2--&gt;i[\"i(1)\"]   NS2--&gt;N2[N]   N2--&gt;LQ2[']   N2--&gt;S2[\"s(k2)\"]   N2--&gt;RQ2[']   N2--&gt;CL2[\":\"]   N2--&gt;J3[J]   J3--&gt;LSQ[\"[\"]   J3--&gt;RSQ[\"]\"];"},{"location":"syntax_analysis_parser_combinator/","title":"50.054 - Top-down recursive parsing using Parser Combinators","text":""},{"location":"syntax_analysis_parser_combinator/#learning-outcome","title":"Learning Outcome","text":"<p>By the end of this class, you should be able to </p> <ul> <li>Implement a top-down recursive parser with backtracking</li> <li>Implement a top-down recursive parser with on-demand backtracking </li> </ul>"},{"location":"syntax_analysis_parser_combinator/#recap-top-down-parsing","title":"(Recap) Top-down parsing","text":"<p>In this cohort problem we are going to focus on Top-down parsing.</p>"},{"location":"syntax_analysis_parser_combinator/#abstract-syntax-ttree","title":"Abstract Syntax Ttree","text":"<p>To implement top-down parsing, we first consider how to represent a parse tree in Scala.  It's natural to implement the parse trees in terms of some algebraic datatype.  The Grammar 4 can be encoded with the following Scala enum type.</p> <pre><code>enum Exp {\n    case TermExp(t:Term)\n    case PlusExp(t:Term, e:Exp)\n}\n\nenum Term {\n    case FactorTerm(f:Factor)\n    case MultTerm(t:Term, f:Factor)\n}\n\ncase class Factor(v:Int)\n</code></pre>"},{"location":"syntax_analysis_parser_combinator/#left-recursion-elimination","title":"Left Recursion Elimination","text":"<p>Recall Grammar 4 defined above contains some left recursion. </p> <p>To eliminate the left recursion, we apply the same trick by rewriting left recursive grammar rules</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; N\\alpha_1 \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; N\\alpha_n \\\\ N &amp; ::= &amp; \\beta_1 \\\\  &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m  \\end{array} \\] <p>into</p> \\[ \\begin{array}{rcl} N &amp; ::= &amp; \\beta_1 N' \\\\ &amp; ... &amp; \\\\ N &amp; ::= &amp; \\beta_m N' \\\\ N' &amp; ::= &amp; \\alpha_1 N' \\\\  &amp; ... &amp; \\\\ N' &amp; ::= &amp; \\alpha_n N' \\\\ N' &amp; ::= &amp; \\epsilon \\end{array} \\] <p>Grammar 4 can be rewritten into</p> <pre><code>E  ::= T + E\nE  ::= T\nT  ::= FT'\nT' ::= *FT'\nT' ::= epsilon\nF  ::=i\n</code></pre> <p>None of the production rules above contains common leading terminal symbols, hence there is no need to apply left-factorization.</p> <p>Note that in the above Grammar 4 with left recursion eliminated, the only rules affected are thos with non-terminal <code>T</code>,</p> <p>Hence we only need to added the following enum type</p>"},{"location":"syntax_analysis_parser_combinator/#additional-abstract-syntax-tree","title":"Additional Abstract Syntax Tree","text":"<p>We could model it using Algebraic data type. <pre><code>case class TermLE(f:Factor, tp:TermLEP)\n\nenum TermLEP {\n    case MultTermLEP(f:Factor, tp:TermLEP)\n    case Eps\n}\n</code></pre></p> <p>The main idea is when parsing a <code>Term</code>, instead of parsing directly, we parse a <code>TermLE</code> then convert it back to <code>Term</code>.</p> <p><pre><code>List(IntTok(1), PlusTok, IntTok(2), AsterixTok, IntTok(3))\n</code></pre> A parser method <code>parse</code> should generate</p> <p><pre><code>PlusExp(FactorTerm(Factor(1)),TermExp(MultTerm(FactorTerm(Factor(2)),Factor(3))))\n</code></pre> where </p> <ul> <li>sub term <code>IntTok(1)</code> was first parsed as <code>TermLE(Factor(1), Eps)</code> then converted to <code>FactorTerm(Factor(1))</code>, and </li> <li>sub term <code>IntTok(2), AsterixTok, IntTok(3)</code> was first parsed as <code>TermLE(Factor(2), MultTerm(Factor(3), Eps))</code> and converted to <code>MultTerm((Factor(2), Factor(3)))</code>.</li> </ul>"},{"location":"syntax_analysis_parser_combinator/#parser-combinator-with-backtracking","title":"Parser Combinator with Backtracking","text":"<p>We consider implementing the naive top-down recursive parser in Scala.  Let's start with the simplest cases. Let's say we would like to write a parsing function that takes a list of lexical tokens and \"consumes\" a token, then return the rest.</p> <p><pre><code>enum Result[A] {\n  case Failed(msg:String)\n  case Ok(v:A)\n}\n\ndef item(toks:List[LToken]):Result[(LToken, List[Token])] = toks match {\n  case Nil =&gt; Failed(\"item() is called with an empty input\")\n  case (t::ts) =&gt; Ok((t, ts))\n}\n</code></pre> We define a variant of the <code>Option</code> datatype, <code>Result</code> which is either a failure with an error message, or an \"Ok\" result. The <code>item</code> function is what we would like to implement. It returns the extracted leading token with the rest of input if the input is non-empty, and signals a failure otherwise. </p> <p>Apply the same idea we could define a conditional parsing function.</p> <pre><code>def sat(toks:List[LToken])(p:LToken =&gt; Boolean):Result[(LToken, List[Token])] = toks match {\n  case Nil =&gt; Failed(\"sat() is called with an empty input\")\n  case (t::ts) if p(t) =&gt; Ok((t, ts))\n  case (t::ts)         =&gt; Failed(\"sat() is called with an input that does not satisfy the input predicate.\")\n}\n</code></pre> <p>We may want to combine these basic parsing functions to form a larger parsing task, e.g.</p> <pre><code>def aBitMoreComplexParser(toks:List[LToken]):Result[(LToken,List[LToken])] = \n  item(toks) match {\n    case Failed(msg) =&gt; Failed(msg)\n    case Ok((_, toks2)) =&gt; sat(toks2)(t =&gt; t match {\n      case AsterixTok =&gt; true\n      case _          =&gt; false\n    }, \"Expecting an asterix.\" )\n  }\n</code></pre> <p>In the above, we define a parsing task which skips the first token and searches for the following asterix. We could imagine that to build a practical parser, we would need many of the basic parsing functions like <code>sat</code> and <code>item</code>, and combine them.</p> <p>What we can observe from the above is that there are some similarity between <code>sat</code> and <code>item</code>, i.e. they both take in a list of tokens and returns the remaining tokens. If we view the lists of tokens as states, we could think of using the State Monad. We would also need this top-down parser to be able to backtrack in case of parsing failures. Recall the <code>MonadError</code> type class</p> <p><pre><code>trait Monad[F[_]] extends Applicative[F] {\n    def bind[A, B](fa: F[A])(f: A =&gt; F[B]): F[B]\n    def pure[A](v: A): F[A]\n    def ap[A, B](ff: F[A =&gt; B])(fa: F[A]): F[B] =\n        bind(ff)((f: A =&gt; B) =&gt; bind(fa)((a: A) =&gt; pure(f(a))))\n}\ntrait MonadError[F[_], E] extends Monad[F] with ApplicativeError[F, E] {\n    override def pure[A](v: A): F[A]\n    override def raiseError[A](e: E): F[A]\n    override def handleErrorWith[A](fa: F[A])(f: E =&gt; F[A]): F[A]\n}\n</code></pre> We include one extra method <code>handleErrorWith</code> which takes a functor <code>fa</code> and executes it, in case of error being raised in <code>fa</code>, it applies <code>f</code> to the error to recover from the error.</p> <p>We define the <code>Parser</code> case class as follows, similar to the <code>State</code> case class in the State Monad, except that we fix the state to be <code>List[T]</code>, where <code>T</code> is a parametric type for tokens, for instance <code>LToken</code>.</p> <pre><code>case class Parser[T, A](p: List[T] =&gt; Result[(A, List[T])]) {\n    def map[B](f: A =&gt; B): Parser[T, B] = this match {\n        case Parser(ea) =&gt;\n            Parser(env =&gt;\n                ea(env) match {\n                    case Failed(err)   =&gt; Failed(err)\n                    case Ok((a, env1)) =&gt; Ok((f(a), env1))\n                }\n            )\n    }\n    def flatMap[B](f: A =&gt; Parser[T, B]): Parser[T, B] = this match {\n        case Parser(ea) =&gt;\n            Parser(env =&gt;\n                ea(env) match {\n                    case Failed(err) =&gt; Failed(err)\n                    case Ok((a, env1)) =&gt;\n                        f(a) match {\n                            case Parser(eb) =&gt; eb(env1)\n                        }\n                }\n            )\n    }\n}\n\ndef run[T, A](\n    parser: Parser[T, A]\n)(env: List[T]): Result[(A, List[T])] = parser match {\n    case Parser(p) =&gt; p(env)\n}\n\ntype ParserM = [T] =&gt;&gt; [A] =&gt;&gt; Parser[T, A]\n</code></pre> <p>Next we define an instance of <code>MonadError[ParserM[T], Error]</code>.</p> <pre><code>given parsecMonadError[T]: MonadError[ParserM[T], Error] =\n    new MonadError[ParserM[T], Error] {\n        override def pure[A](a: A): Parser[T, A] =\n            Parser(cs =&gt; Ok((a, cs)))\n        override def bind[A, B](\n            fa: Parser[T, A]\n        )(f: A =&gt; Parser[T, B]): Parser[T, B] = fa.flatMap(f)\n        override def raiseError[A](e: Error): Parser[T, A] =\n            Parser(env =&gt; Failed(e))\n        override def handleErrorWith[A](\n            fa: Parser[T, A]\n        )(f: Error =&gt; Parser[T, A]): Parser[T, A] = fa match {\n            case Parser(ea) =&gt;\n                Parser(env =&gt;\n                    ea(env) match {\n                        case Failed(err) =&gt; run(f(err))(env)\n                        case Ok(v)       =&gt; Ok(v)\n                    }\n                )\n        }\n    }\n</code></pre> <p>With the Monad instance in-place, we can re-define the <code>item()</code> and <code>sat()</code> methods in monadic style.</p> <p><pre><code>def item[T]: Parser[T, T] =\n    Parser(env =&gt; {\n        val toks = env\n        toks match {\n            case Nil =&gt;\n                Failed(s\"item() is called with an empty token stream\")\n            case (c :: cs) =&gt; Ok((c, cs))\n        }\n    })\n\ndef sat[T](p: T =&gt; Boolean, err:String=\"\"): Parser[T, T] = Parser(toks =&gt;\n    toks match {\n        case Nil =&gt; Failed(s\"sat() is called with an empty token stream. ${err}\")\n        case (c :: cs) if p(c) =&gt; Ok((c, cs))\n        case (c :: cs) =&gt;\n            Failed(s\"sat() is called with a unsatisfied predicate with ${c}. ${err}\")\n    }\n)\n</code></pre> In the <code>sat</code> combinator, we pass in two arguments, the predicate <code>p</code> and an optional <code>err</code> error parameter, which is default to empty string.</p> <p>More importantly, we could define more generic and useful combinators</p> <pre><code>def choice[T, A](p: Parser[T, A])(q: Parser[T, A])(using\n    m: MonadError[ParserM[T], Error]\n): Parser[T, A] = m.handleErrorWith(p)(e =&gt; q)\n</code></pre> <p>The <code>choice</code> combinator takes two parsers <code>p</code> and <code>q</code>. It tries to run <code>p</code>. In case <code>p</code> fails, it backtracks (by restoring the original state) and runs <code>q</code>. </p> <p>Now we can make use of <code>choice</code> to define an <code>optional</code> combinator <pre><code>def optional[T, A](pa: Parser[T, A]): Parser[T, Either[Unit, A]] = {\n    val p1: Parser[T, Either[Unit, A]] = for (a &lt;- pa) yield (Right(a))\n    val p2: Parser[T, Either[Unit, A]] = Parser(toks =&gt; Ok((Left(()), toks)))\n    choice(p1)(p2)\n}\n</code></pre></p> <p><code>optional</code> takes a parser <code>pa</code> and tries to execute it with the current input. If it fails, it restores the original state and returns <code>Unit</code>.</p> <p>Let's try to write and a parser for the simple arithmetic expression </p> <p>Recall the nice property of a top-down parser is that the parser is correspondent to the top-down traversal of the production rules.</p> <p>Recall the grammar 4 of Math expression with left recursion. <pre><code>E::= T + E\nE::= T\nT::= T * F \nT::= F\nF::= i    \n</code></pre></p> <pre><code>def parseExp:Parser[LToken, Exp] = \n    choice(parsePlusExp)(parseTermExp)\n\ndef parsePlusExp:Parser[LToken, Exp] = for {\n    t &lt;- parseTerm\n    plus &lt;- parsePlusTok\n    e &lt;- parseExp\n} yield PlusExp(t, e)\n\ndef parseTermExp:Parser[LToken, Exp] = for {\n    t &lt;- parseTerm\n} yield TermExp(t)\n</code></pre> <p>Up to this point we are ok as production rules with <code>E</code> on the LHS are not left recursive. It gets tricky when paarsing <code>T</code> which contains left recursion. Recall the modified grammar of  <code>T</code> having left-recursion eliminated.</p> <pre><code>T  ::= FT'  \nT' ::= *FT'\nT' ::= epsilon\n</code></pre> <p>In terms of Scala enum type, we refer to them as <code>TermLE</code> and <code>TermLEP</code>. Hence the parser <code>parserTerm</code> has to be defined in terms of <code>parseTermLE</code>, then  convert the result of <code>TermLE</code> back to <code>Term</code></p> <pre><code>def parseTerm:Parser[LToken, Term] = for {\n    tle &lt;- parseTermLE\n} yield fromTermLE(tle)\n</code></pre> <p>Where <code>parseTermLE</code> can be implemented using parsec, </p> <pre><code>def parseTermLE:Parser[LToken, TermLE] = for {\n    f &lt;- parseFactor\n    tp &lt;- parseTermP \n} yield TermLE(f, tp)\n\ndef parseTermP:Parser[LToken, TermLEP] = for {\n    omt &lt;- optional(parseMultTermP)\n} yield { omt match {\n    case Left(_) =&gt; Eps\n    case Right(t) =&gt; t\n}}\n\ndef parseMultTermP:Parser[LToken, TermLEP] = for {\n    asterix &lt;- parseAsterixTok\n    f &lt;- parseFactor\n    tp &lt;- parseTermP\n} yield MultTermLEP(f, tp)\n\ndef parseFactor:Parser[LToken, Factor] = for {\n    i &lt;- parseIntTok\n    f &lt;- someOrFail(i)( itok =&gt; itok match {\n        case IntTok(v) =&gt; Some(Factor(v))\n        case _         =&gt; None\n    })(\"parseFactor() fail: expect to parse an integer token but it is not an integer.\")\n} yield f\n\ndef parsePlusTok:Parser[LToken, LToken] = sat ((x:LToken) =&gt; x match {\n    case PlusTok =&gt; true\n    case _       =&gt; false\n}, \"Expecting a + symbol\")\n\n\ndef parseAsterixTok:Parser[LToken, LToken] = sat ((x:LToken) =&gt; x match {\n    case AsterixTok =&gt; true\n    case _          =&gt; false\n}, \"Expecting a * symbol\")\n\ndef parseIntTok:Parser[LToken, LToken] = sat ((x:LToken) =&gt; x match {\n    case IntTok(v) =&gt; true\n    case _         =&gt; false\n}, \"Expecting an int token\")\n</code></pre> <p>Finally the <code>TermLE</code> to <code>Term</code> conversion is an inversed in order traversal, as the parse tree of <code>TermLE</code></p> <pre><code>    T\n   / \\\n  f   T'\n     /|\\\n    * f T'\n       /|\\\n      * f T'\n          |\n         eps\n</code></pre> <p>and the parse tree of <code>Term</code> is</p> <pre><code>        T\n       /|\\\n      T * f\n     /| \\\n    T * f \n    |\n    f\n</code></pre> <p>The implementation can be found as follows,</p> <pre><code>def fromTermLE(t:TermLE):Term = t match {\n    case TermLE(f, tep) =&gt; fromTermLEP(FactorTerm(f))(tep)\n} \ndef fromTermLEP(t1:Term)(tp1:TermLEP):Term = tp1 match {\n    case Eps =&gt; t1 \n    case MultTermLEP(f2, tp2) =&gt; {\n        val t2 = MultTerm(t1, f2)\n        fromTermLEP(t2)(tp2)\n    }\n}\n</code></pre> <p>And here is some test cases</p> <pre><code>test(\"test_parse\") {\n    // val s = \"1+2*3\"\n    val toks = List(IntTok(1), PlusTok, IntTok(2), AsterixTok, IntTok(3))\n    val result = BacktrackParsec.run(parseExp)(toks)\n    val expected = PlusExp(FactorTerm(Factor(1)),TermExp(MultTerm(FactorTerm(Factor(2)),Factor(3))))\n    result match {\n        case Ok((t, Nil)) =&gt;  assert(t == expected)\n        case _ =&gt; assert(false)\n    }\n}\n</code></pre>"},{"location":"syntax_analysis_parser_combinator/#parser-combinator-without-backtracking-in-spirit-of-ll1","title":"Parser Combinator without backtracking (In spirit of LL(1))","text":"<p>Let's extend our parser combinator to support <code>LL(1)</code> parsing without backtracking.</p> <p>We introduce the following algebraic datatype to label an (intermediate) parsing result  <pre><code>enum Progress[+A] {\n    case Consumed(value: A)\n    case Empty(value: A)\n}\n</code></pre></p> <p>The partial is is <code>Consumed</code> when there has been input tokens consumed, otherwise, <code>Empty</code>.</p> <p>We adjust the definition of the <code>Parser</code> case class as follows</p> <pre><code>case class Parser[T, A](p: List[T] =&gt; Progress[Result[(A, List[T])]]) {\n    def map[B](f: A =&gt; B): Parser[T, B] = this match {\n        case Parser(p) =&gt;\n            Parser(toks =&gt;\n                p(toks) match {\n                    case Empty(Failed(err))    =&gt; Empty(Failed(err))\n                    case Empty(Ok((a, toks1)))  =&gt; Empty(Ok((f(a), toks1)))\n                    case Consumed(Failed(err)) =&gt; Consumed(Failed(err))\n                    case Consumed(Ok((a, toks1))) =&gt;\n                        Consumed(Ok((f(a), toks1)))\n                }\n            )\n    }\n    def flatMap[B](f: A =&gt; Parser[T, B]): Parser[T, B] = this match {\n        case Parser(p) =&gt;\n            Parser(toks =&gt;\n                p(toks) match {\n                    case Consumed(v) =&gt; {\n                        lazy val cont = v match {\n                            case Failed(err) =&gt; Failed(err)\n                            case Ok((a, toks1)) =&gt;\n                                f(a) match {\n                                    case Parser(p2) =&gt;\n                                        p2(toks1) match {\n                                            case Consumed(x) =&gt; x\n                                            case Empty(x)    =&gt; x\n                                        }\n                                }\n                        }\n                        Consumed(cont)\n                    }\n                    case Empty(v) =&gt;\n                        v match {\n                            case Failed(err) =&gt; Empty(Failed(err))\n                            case Ok((a, toks1)) =&gt;\n                                f(a) match {\n                                    case Parser(p2) =&gt; p2(toks1)\n                                }\n                        }\n                }\n            )\n    }\n}\n\ndef run[T, A](\n    parser: Parser[T, A]\n)(toks: List[T]): Progress[Result[(A, List[T])]] = parser match {\n    case Parser(p) =&gt; p(toks)\n}\n</code></pre> <p>Let's look at the <code>flatMap</code> which is the Monadic <code>bind</code> eventually. It takes the first parser (<code>this</code>) and pattern-matches it against <code>Parser(p)</code>. In the output parser, we first apply <code>p</code> to the tokens <code>toks</code>. </p> <ul> <li>If the result's progress is <code>Empty(v)</code>, we check whether <code>v</code> is an error or <code>Ok</code>. When it is an error, it will be propogated, otherwise, we apply <code>f</code> to the output of <code>p(toks)</code> which is <code>a</code>. That will give us the second parser to continue with <code>Parser(p2)</code>. We then apply <code>p2</code> to <code>toks1</code> which should be the same as <code>toks</code>. </li> <li>If the result's progress is <code>Consumed(v)</code>, we note that some part of <code>toks</code> has been parsed. The parser's behaviour here should be similar to the previous case, except that <code>p2(toks1)</code> progress result will always be updated as <code>Consumed</code> regardless whether <code>p2</code> has consumed anything. The lazy declaration of the immutable variable <code>cont</code> is an optimization which allows us to return the progress information <code>Consumed</code> without actually executing <code>p2(toks1)</code> when its result is not needed.</li> </ul> <p>The defintion of the MonadError type class instance for Parser is as follows</p> <pre><code>type ParserM = [T] =&gt;&gt; [A] =&gt;&gt; Parser[T, A]\n\ngiven parsecMonadError[T]: MonadError[ParserM[T], Error] =\n    new MonadError[ParserM[T], Error] {\n        override def pure[A](a: A): Parser[T, A] =\n            Parser(cs =&gt; Empty(Ok((a, cs))))\n        override def bind[A, B](\n            fa: Parser[T, A]\n        )(f: A =&gt; Parser[T, B]): Parser[T, B] = fa.flatMap(f)\n\n        override def raiseError[A](e: Error): Parser[T, A] =\n            Parser(toks =&gt; Empty(Failed(e)))\n        override def handleErrorWith[A](\n            fa: Parser[T, A]\n        )(f: Error =&gt; Parser[T, A]): Parser[T, A] = fa match {\n            case Parser(p) =&gt;\n                Parser(toks =&gt;\n                    p(toks) match {\n                        case Empty(\n                                Failed(err)\n                            ) =&gt; // only backtrack when it is empty\n                            {\n                                f(err) match {\n                                    case Parser(p2) =&gt; p2(toks)\n                                }\n                            }\n                        case Empty(\n                                Ok(v)\n                            ) =&gt; // LL(1) parser will also attempt to look at f if fa does not consume anything\n                            {\n                                f(\"faked error\") match {\n                                    case Parser(p2) =&gt;\n                                        p2(toks) match {\n                                            case Empty(_) =&gt;\n                                                Empty(\n                                                    Ok(v)\n                                                ) // if f also fail, we report the error from fa\n                                            case consumed =&gt; consumed\n                                        }\n                                }\n                            }\n                        case Consumed(v) =&gt; Consumed(v)\n                    }\n                )\n        }\n\n    }\n</code></pre> <p>The interesting part lies in the <code>handleErrorWith</code> method, which the error recovery method <code>fa</code> is applied only when the progress of the parsing so far is <code>Empty</code>. In other words, given a choice of two parsers <code>choice(p1)(p2)</code>, it will not backtrack to <code>p2</code> if <code>p1</code> has consumed some token. This is in-sync with predictive parsing.</p> <p>All other combinators such as <code>choice</code>, <code>item</code>, <code>sat</code>, <code>optional</code> can be adjusted in the same fashion.</p> <p>We know that not all languages are in <code>LL(1)</code>.  It is undecidable to find out which <code>k</code> of <code>LL(k)</code> that a language is in. Thus, the above parser might not be very useful since it only works if the given language is in <code>LL(1)</code>. </p> <p>Thanks to the Monadic design, it is very easy to extend the parser to accept non <code>LL(1)</code> language by supporting backtracking on-demand. That is, the parser by default is not backtracking, however, it could if we want it to backtrack explicitly.</p> <p><pre><code>// explicit try and backtrack if fails\ndef attempt[T, A](p: Parser[T, A]): Parser[T, A] =\n    Parser(toks =&gt;\n        run(p)(toks) match {\n            case Consumed(Failed(err)) =&gt; Empty(Failed(err))\n            case otherwise =&gt; otherwise \n        }\n    )\n</code></pre> The <code>attempt</code> combinator takes a parser <code>p</code> and runs it. If its result is <code>Consumed</code> but <code>Failed</code>, it will reset the progress as <code>Empty</code>.  The full implementation of the on-demand backtracking parser combinator implementation is given in the stub project <code>Parsec.scala</code>.</p>"}]}